<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on vBünzli</title>
    <link>//localhost:1313/post/</link>
    <description>Recent content in Posts on vBünzli</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2022-2024, Y.Gerber &amp; Authors</copyright>
    <lastBuildDate>Mon, 09 Feb 2026 17:40:28 +0100</lastBuildDate><atom:link href="//localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data Services Manager - K8s </title>
      <link>//localhost:1313/post/dsm-9-k8s/</link>
      <pubDate>Mon, 09 Feb 2026 17:40:28 +0100</pubDate>
      
      <guid>//localhost:1313/post/dsm-9-k8s/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vmware-data-services-manager---k8s-operator&#34;&gt;VMware Data Services Manager - K8s Operator&lt;/h1&gt;
&lt;p&gt;VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DSM UI&lt;/li&gt;
&lt;li&gt;VCF Automation&lt;/li&gt;
&lt;li&gt;K8s Operator&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Services Manager - Update to 9</title>
      <link>//localhost:1313/post/dsm-9-upgrade/</link>
      <pubDate>Mon, 09 Feb 2026 17:40:21 +0100</pubDate>
      
      <guid>//localhost:1313/post/dsm-9-upgrade/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Vcf 9 Upgrade</title>
      <link>//localhost:1313/post/vcf-9-upgrade/</link>
      <pubDate>Mon, 09 Feb 2026 17:39:56 +0100</pubDate>
      
      <guid>//localhost:1313/post/vcf-9-upgrade/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu Package Troubleshooting</title>
      <link>//localhost:1313/post/tanzu-package-troubleshooting/</link>
      <pubDate>Mon, 06 Jan 2025 15:26:46 +0100</pubDate>
      
      <guid>//localhost:1313/post/tanzu-package-troubleshooting/</guid>
      <description>
        
          
            &lt;h1 id=&#34;cannot-delete-tanzu-package&#34;&gt;Cannot delete Tanzu Package&lt;/h1&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;error-messaage&#34;&gt;&lt;strong&gt;Error Messaage&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Error: &lt;em&gt;Getting service account: serviceaccounts &amp;quot;grafana-tanzu-system-dashboards-sa&amp;quot; not found&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get apps grafana -oyaml | grep finalizer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl patch app grafana -n &amp;lt; my namespace &amp;gt; -p &amp;#39;{&amp;#34;metadata&amp;#34;:{&amp;#34;finalizers&amp;#34;:[]}}&amp;#39; --type=merge
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl delete app grafana
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware Edge Compute Stack</title>
      <link>//localhost:1313/post/vmware-edge-compute-stack/</link>
      <pubDate>Mon, 30 Dec 2024 18:52:03 +0100</pubDate>
      
      <guid>//localhost:1313/post/vmware-edge-compute-stack/</guid>
      <description>
        
          
            &lt;p&gt;VMware Edge Cloud Compute Stack is a Solution for running workloads on the Edge - &amp;quot;Edge&amp;quot; in this case means running a modified Version of ESXi on really small compute footprint. Management is done from a central Appliane called VECO (VMware Edge Cloud Orchestrator).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ECS vs VCF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ECS has nothing todo with VMware Cloud Foundation Edge (VCF-Edge). VCF Edge is just a SKU. VCF has different Edge Reference Architectures. ECS is a different Product, generally meat for smaller compute needs.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Cis Hardening Vsphere</title>
      <link>//localhost:1313/post/cis-hardening-vsphere/</link>
      <pubDate>Mon, 16 Dec 2024 20:25:11 +0100</pubDate>
      
      <guid>//localhost:1313/post/cis-hardening-vsphere/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>2-Node vSAN Cluster</title>
      <link>//localhost:1313/post/vsan-2-node/</link>
      <pubDate>Tue, 10 Dec 2024 20:20:23 +0100</pubDate>
      
      <guid>//localhost:1313/post/vsan-2-node/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vsan-esa-2-node-cluster-guide-and-recommendations&#34;&gt;vSAN ESA 2-Node Cluster Guide and Recommendations&lt;/h1&gt;
&lt;p&gt;This Guide will help you in your Design Considerations and during the Deployment of a 2-Node vSAN Cluster.&lt;/p&gt;
&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions&lt;/h2&gt;
&lt;p&gt;The following assumptions where made when creating this blog post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have vSAN Ready Nodes or all Hardware is on the vSAN &lt;a href=&#34;https://compatibilityguide.broadcom.com/&#34;&gt;Hardware Compatibility List&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have VMware Cloud Foundation (VCF) Licenses or VVF with enough vSAN Add-on Licences&lt;/li&gt;
&lt;li&gt;Phyiscal Network Adapter for vSAN have a Bandwith of 25Gbits or above&lt;/li&gt;
&lt;li&gt;General Knowledge of vSAN and vSphere&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;design&#34;&gt;Design&lt;/h1&gt;
&lt;h2 id=&#34;constraints&#34;&gt;Constraints&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Only one host failure can be tolerated at a time (FTT1)&lt;/li&gt;
&lt;li&gt;Adding nodes beyond two would require reconfiguration to a standard vSAN cluster.&lt;/li&gt;
&lt;li&gt;Requires a witness host, which needs two IP addresses - one for management and one for vSAN traffic&lt;/li&gt;
&lt;li&gt;Managing multiple 2-node clusters can increase operational complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;general-considerations&#34;&gt;General Considerations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Do you have a dedicated vCenter for your 2-Node Cluster?&lt;/li&gt;
&lt;li&gt;Where do you run the vCenter VM? On the 2-Node Cluster itself or outside?&lt;/li&gt;
&lt;li&gt;Does the two physical Server run on the same physical site?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vsan-traffic-direct-attached-or-switched&#34;&gt;vSAN Traffic: direct attached or Switched&lt;/h3&gt;
&lt;p&gt;With a 2-Node Cluster you have the possibility to run vSAN (and vMotion) traffic over direct attached Cables. This is a great solution for RoBo or Site which do not offer 25Gbits Ports on Network Switches.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>RBAC on VKS</title>
      <link>//localhost:1313/post/k8s-rbac/</link>
      <pubDate>Sun, 08 Dec 2024 12:29:05 +0100</pubDate>
      
      <guid>//localhost:1313/post/k8s-rbac/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;This Blogpost explains RBAC on vSphere Kubernetes Service (VKS), formely known as Tanzu Kubernetes Grid Service (TKGS). It also shows how to give access via &lt;em&gt;kubectl&lt;/em&gt; with granular permissions, leveraging OIDC Auth with pinniped on VKS-Clusters.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;naming&#34;&gt;&lt;strong&gt;Naming&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;I use VKS-, TKGS- and K8s Clusters in this Blog Post. All have the same meaning - simply a Kubernetes Cluster :)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;rbac-on-vsphere-kubernetes-service&#34;&gt;RBAC on vSphere Kubernetes Service&lt;/h1&gt;
&lt;p&gt;Role-based Access Control on vSphere Kubernetes Cluster has a few Keypoint which should be though about. The main question you have to ask yourself: Which Level of access do I want to grant to which people?&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Harbor</title>
      <link>//localhost:1313/post/harbor/</link>
      <pubDate>Sat, 30 Nov 2024 00:47:13 +0100</pubDate>
      
      <guid>//localhost:1313/post/harbor/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Supervisor Services</title>
      <link>//localhost:1313/post/supervisor-services/</link>
      <pubDate>Sat, 30 Nov 2024 00:47:03 +0100</pubDate>
      
      <guid>//localhost:1313/post/supervisor-services/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Velero</title>
      <link>//localhost:1313/post/velero-standalone/</link>
      <pubDate>Sat, 30 Nov 2024 00:46:45 +0100</pubDate>
      
      <guid>//localhost:1313/post/velero-standalone/</guid>
      <description>
        
          
            &lt;h1 id=&#34;velero&#34;&gt;Velero&lt;/h1&gt;
&lt;p&gt;This blog post explains how to install velero on different configurations on vSphere with Tanzu / IaaS Control Plane.&lt;/p&gt;
&lt;h2 id=&#34;different-velero-installations-with-tgks&#34;&gt;Different Velero Installations with TGKS&lt;/h2&gt;
&lt;p&gt;Depending on your Supervisor Cluster Configuration, there are different ways to install Velero. Each approach offers different usage on velero.&lt;/p&gt;
&lt;h3 id=&#34;ways-to-install-velero&#34;&gt;Ways to Install Velero&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Velero Plugin for vSphere&lt;/li&gt;
&lt;li&gt;Velero &amp;quot;Standalone&amp;quot; via Helm Chart&lt;/li&gt;
&lt;li&gt;Velero &amp;quot;Standalone&amp;quot; via velero-cli&lt;/li&gt;
&lt;/ul&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere VM - conflicted PortID</title>
      <link>//localhost:1313/post/vsphere-vm-portid-cport/</link>
      <pubDate>Fri, 22 Nov 2024 07:41:26 +0100</pubDate>
      
      <guid>//localhost:1313/post/vsphere-vm-portid-cport/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vms-with-conflicted-portids&#34;&gt;VMs with conflicted PortIDs&lt;/h1&gt;
&lt;p&gt;In certain scenarios it can happend that the PortID (distributed port group) of a VM shows up with the prefix &amp;quot;-c&amp;quot; (eg c-245).
vCenter Server creates a conflict port if multiple virtual machine&#39;s point to the same port.
It does not automatically reconfigure the virtual machine to connect to a regular port if the conflict remains and the virtual machine stays connected to the conflict port.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere Lifecycle Manager and HPE OneView for vCenter - Troubleshooting</title>
      <link>//localhost:1313/post/vclm-hpe-oneview/</link>
      <pubDate>Wed, 20 Nov 2024 09:25:35 +0100</pubDate>
      
      <guid>//localhost:1313/post/vclm-hpe-oneview/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vsphere-lifecycle-manager-vlcm&#34;&gt;vSphere Lifecycle Manager (vLCM)&lt;/h1&gt;
&lt;p&gt;Some Guidance when troubleshooting Issues with vSphere Lifecycle Manager and HPE OneView for vCenter as a HSM (Hardware Support Manager).&lt;/p&gt;
&lt;h2 id=&#34;log-files&#34;&gt;Log Files&lt;/h2&gt;
&lt;p&gt;The General Log File for vLCM is here:&lt;/p&gt;
&lt;p&gt;Log File: /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log&lt;/p&gt;
&lt;p&gt;To see error from this log file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;var&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;updatemgr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vum&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;server&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vum&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;server&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grep&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;error&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;3rd Party Depots and their the vCenter Access to those depot is logged in the following file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;var&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;envoy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;envoy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;access&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grep&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;hpe&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;common-errors&#34;&gt;Common Errors&lt;/h2&gt;
&lt;h3 id=&#34;cannot-sync-software-depots&#34;&gt;Cannot sync software depots&lt;/h3&gt;
&lt;p&gt;To verify that the online depot registration was successful, navigate to &lt;em&gt;Menu &amp;gt; Lifecycle Manager &amp;gt; Settings &amp;gt; Administration &amp;gt; Patch Setup&lt;/em&gt;. The values in the Enabled and Connectivity Status columns should be &lt;em&gt;Yes&lt;/em&gt; and &lt;em&gt;Connected&lt;/em&gt; respectively. If the Connectivity Status is Not Connected, verify the proper settings for the vCenter proxy configuration and perform a manual sync of the updates.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>OpenShift on vSphere - Roles and Permissions</title>
      <link>//localhost:1313/post/openshift-on-vsphere/</link>
      <pubDate>Tue, 19 Nov 2024 23:20:09 +0100</pubDate>
      
      <guid>//localhost:1313/post/openshift-on-vsphere/</guid>
      <description>
        
          
            &lt;h1 id=&#34;deployment-mode-installed-provisioned-infrastructure&#34;&gt;Deployment Mode: Installed Provisioned Infrastructure&lt;/h1&gt;
&lt;p&gt;OpenShift Installer-Provisioned Infrastructure (IPI) is a deployment method for OpenShift Container Platform that provides a full-stack automated installation and setup process.&lt;/p&gt;
&lt;p&gt;The installer manages all aspects of the cluster deployment, including the underlying infrastructure and the operating system itself. IPI creates a bootstrap virtual machine on a provisioner node, which assists in deploying the OpenShift cluster.&lt;/p&gt;
&lt;p&gt;The most common way to deploy OpenShift is on vSphere. For the vSphere Integration Permissions are needed:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Esxi Tpm Key Backup</title>
      <link>//localhost:1313/post/esxi-tpm-key-backup/</link>
      <pubDate>Wed, 23 Oct 2024 12:28:07 +0200</pubDate>
      
      <guid>//localhost:1313/post/esxi-tpm-key-backup/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Insert Lead paragraph here.&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>ArgoCD Deployment &amp; Configuration</title>
      <link>//localhost:1313/post/argocd-configuration/</link>
      <pubDate>Fri, 13 Sep 2024 23:48:15 +0100</pubDate>
      
      <guid>//localhost:1313/post/argocd-configuration/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It&#39;s designed to make application deployment and lifecycle management automated, auditable, and easy to understand.&lt;/p&gt;
&lt;p&gt;Key features and concepts of ArgoCD include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GitOps Methodology: Argo CD uses Git repositories as the source of truth for defining the desired application state.&lt;/li&gt;
&lt;li&gt;Kubernetes-native: It&#39;s implemented as a Kubernetes controller that continuously monitors running applications and compares their current state against the desired state specified in Git.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blog post explains the deployment of ArgoCD in a Tanzu Kubernetes Grid Cluster. There is also the option to provision ArgoCD as a supervisor Service.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware vCenter Troubleshooting Guide</title>
      <link>//localhost:1313/post/vcenter-troubleshooting/</link>
      <pubDate>Fri, 23 Aug 2024 22:24:18 +0200</pubDate>
      
      <guid>//localhost:1313/post/vcenter-troubleshooting/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vcenter-troubleshooting-guide&#34;&gt;vCenter Troubleshooting Guide&lt;/h1&gt;
&lt;p&gt;The following Troubleshooting Guide applies to vCenter 8 and above.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;caution&#34;&gt;Caution!&lt;/h4&gt;
&lt;p&gt;Create a VM Snapshot of your vCenter before proceeding with any step!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;general-troubleshooting&#34;&gt;General Troubleshooting&lt;/h2&gt;
&lt;p&gt;Log files can be accesses trough ssh or bash. SSH login works with the root user or your SSO Admi User (commonly &lt;a href=&#34;mailto:administrator@vsphere.local&#34;&gt;administrator@vsphere.local&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SSH not working&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If SSH does not work (eg. due to a networki misconfiguration) you can access the vCenter via bash (VM Remote Console).&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware Cloud Foundation 5.2</title>
      <link>//localhost:1313/post/vcf52/</link>
      <pubDate>Tue, 25 Jun 2024 20:48:30 +0200</pubDate>
      
      <guid>//localhost:1313/post/vcf52/</guid>
      <description>
        
          
            &lt;p&gt;The new VCF 5.2 Update delivers quite a few interesting updates. This blog post covers expecially the updates and improvements for Tanzu.&lt;/p&gt;
&lt;p&gt;Part 1 covers new Feature within Tanzu on vSphere 8 Update 3.&lt;/p&gt;
&lt;h2 id=&#34;vsphere-with-tanzu-update---vsphere-iaas-control-plane&#34;&gt;vSphere with Tanzu Update -&amp;gt; vSphere IaaS Control Plane&lt;/h2&gt;
&lt;p&gt;vSphere with Tanzu is passé - the new naming is vSphere IaaS Control Plane.&lt;/p&gt;
&lt;p&gt;The naming already suggest - not only Kubernetes Cluster can be deployed, also VMs (this was possible a long time but with the new naming the focus is not only on k8s). Via the vm-operator you can deploy VMs alongside Tanzu Kubernetes Cluster. The IaaS Control Plane is really interesting as you can deploy VMs with Code. A YAML File will describe your VM. For that the VM Class now support much more configuration, it handled like a normal VM provisioned through vSphere UI.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>DevSecOps</title>
      <link>//localhost:1313/post/devsecops/</link>
      <pubDate>Tue, 25 Jun 2024 08:30:38 +0200</pubDate>
      
      <guid>//localhost:1313/post/devsecops/</guid>
      <description>
        
          
            &lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Due to the rise of containerized applications the new word DevOps or DevSecOps.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what is DevOps?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Basically the developer should also build &amp;amp; operate the infrastructure on which is code runs on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And DevSecOps?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We need Security, so adapt DevOps Practises also for Security. Devs should also be resonsible to securing their infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What are the goals?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The main Goal for DevOps Practises is to shorten time of the SDLC (Systems Develoopment Lifecyle). Basically be faster to ship code into production.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere REST API - Intro</title>
      <link>//localhost:1313/post/vsphere-api/</link>
      <pubDate>Thu, 20 Jun 2024 22:22:08 +0200</pubDate>
      
      <guid>//localhost:1313/post/vsphere-api/</guid>
      <description>
        
          
            &lt;h1 id=&#34;getting-started-with-the-vsphere-api&#34;&gt;Getting started with the vSphere API&lt;/h1&gt;
&lt;p&gt;The vSphere REST API, introduced in vSphere 6.5, represents a significant leap forward in managing and automating VMware environments. This modern, developer-friendly interface offers a more streamlined approach to interacting with vSphere compared to its predecessor, the SOAP-based vSphere Web Services API.&lt;/p&gt;
&lt;p&gt;Key features of the vSphere REST API include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RESTful architecture: It follows REST principles, using standard HTTP methods (GET, POST, PUT, DELETE) for operations.&lt;/li&gt;
&lt;li&gt;JSON-based: Requests and responses use JSON format, making it easier to parse and work with data.&lt;/li&gt;
&lt;li&gt;Simplified interaction: The API is designed to be more intuitive and easier to use than the older SOAP-based API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started with the vSphere REST API, you can use the built-in API Explorer in vCenter Server, accessible at https://&amp;lt; vcenter.example.com &amp;gt;/apiexplorer. This tool provides interactive documentation and allows you to test API calls directly from your browser.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>vCenter File Level Restore - Edit Configuration</title>
      <link>//localhost:1313/post/vcenter-file-level-restore/</link>
      <pubDate>Fri, 23 Feb 2024 12:18:49 +0200</pubDate>
      
      <guid>//localhost:1313/post/vcenter-file-level-restore/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;With vCenter File-level Backups you can restore your entire vCenter configuration. If your running vCenter has some misconfigurations (eg. a false proxy setting) and isn&#39;t working, you can use existing file-level backups and clear those misconfigurations before restoring.&lt;/p&gt;
&lt;h2 id=&#34;basic-process&#34;&gt;Basic Process&lt;/h2&gt;
&lt;p&gt;The Basic vCenter File-level Restore Process looks like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy a fresh vCenter via ISO (VCSA UI Installer)
&lt;ul&gt;
&lt;li&gt;Shutdown the defunct vCenter (IP Conflicts)&lt;/li&gt;
&lt;li&gt;Same Network Configuration&lt;/li&gt;
&lt;li&gt;Same build number&lt;/li&gt;
&lt;li&gt;Same Size (Tiny, Small, Medium Large)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;When Stage 1 is completed you can start the Stage 2 Process to Restore your vCenter Configuration&lt;/li&gt;
&lt;li&gt;In case of a misconfiguration (which has also been backed up) you have the option to edit all configurations files&lt;/li&gt;
&lt;li&gt;Restore vCenter with configurations edits&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;stage-1---deploying-a-fresh-blank-appliance&#34;&gt;Stage 1 - Deploying a fresh (blank) appliance&lt;/h1&gt;
&lt;p&gt;Before deploying a fresh vCenter, have a look at you VCSA Backup folder. In the root of each backupfolder there is a file &#39;backup-metadata.json&#39; - within there grab the details before deploying a fresh VCSA:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware Tanzu - Overview</title>
      <link>//localhost:1313/post/tanzu-overview/</link>
      <pubDate>Thu, 08 Feb 2024 20:43:03 +0200</pubDate>
      
      <guid>//localhost:1313/post/tanzu-overview/</guid>
      <description>
        
          
            &lt;h1 id=&#34;tanzu-portfolio-explained&#34;&gt;Tanzu Portfolio explained&lt;/h1&gt;
&lt;p&gt;Lately, we&#39;ve been having lots conversations internally and with customers about Tanzu.
There appears to be some confusion about Tanzu&#39;s portfolio and which Tanzu products are best suited for specific use cases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;update-november-2024&#34;&gt;&lt;strong&gt;Update November 2024&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;With the VMware aquisition by Broadcom, the are some changes in the Naming and Product Bundles.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The new naming for vSphere with Tanzu is IaaS Control Plane. The Kubernetes Runtime itself is part of VMware Cloud Foundation (VCF).
Add-on&#39;s, Management Tools are now part of the Tanzu Business Unit (BU). Tanzu Products include:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Reformatting enterprise storage drives to 512 bytes</title>
      <link>//localhost:1313/post/reformatting-drives/</link>
      <pubDate>Tue, 26 Dec 2023 15:28:45 +0200</pubDate>
      
      <guid>//localhost:1313/post/reformatting-drives/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently I got my hands on some old HPE 3PAR 1.92TB SAS SSDs (Samsung PM1633a). I though they would be perfect for my SDDC home lab - sadly they did not work as intended with my HPE DL360 server. The SmartArray P420i was recognizing the disks but was giving the error &amp;quot;This physical drive does not support RAID and is not exposed to OS. It cannot be used for configuration on this controller&amp;quot;. But hopefully there is a solution to the problem we are facing:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu Mission Control Self Managed</title>
      <link>//localhost:1313/post/tanzu-mission-control-self-managed/</link>
      <pubDate>Sat, 09 Dec 2023 13:57:09 +0100</pubDate>
      
      <guid>//localhost:1313/post/tanzu-mission-control-self-managed/</guid>
      <description>
        
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu with vSphere and Antrea: Cluster Creation</title>
      <link>//localhost:1313/post/tanzu-cluster-creation/</link>
      <pubDate>Fri, 08 Dec 2023 20:43:03 +0200</pubDate>
      
      <guid>//localhost:1313/post/tanzu-cluster-creation/</guid>
      <description>
        
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware Tanzu Kubernetes Grid -  getting started</title>
      <link>//localhost:1313/post/tkg-getting-started/</link>
      <pubDate>Wed, 29 Nov 2023 22:04:40 +0100</pubDate>
      
      <guid>//localhost:1313/post/tkg-getting-started/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;This blog post is about getting started with VMware Tanzu Kubernetes Grid (TKG). TKG is used to deploy Tanzu Kubernetes Cluster to various Cloud Providers including vSphere, AWS and Azure.
First, a management cluster (based on Docker and kind) is created from a Bootstrap Workstation, which itself then  provides its web interface and additional CLI tools to create a Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;TKG has no integration in vSphere UI. If you wan&#39;t to integrate Tanzu with vCenter/vSphere UI - use vSphere with Tanzu instead.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Cillium Gateway API - Getting Started</title>
      <link>//localhost:1313/post/cillium-getting-started/</link>
      <pubDate>Sat, 14 Oct 2023 18:18:26 +0200</pubDate>
      
      <guid>//localhost:1313/post/cillium-getting-started/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Cillium Gateway API&lt;/strong&gt;
The Cilium Gateway API is a Kubernetes API that provides a more powerful and flexible way to manage traffic routing than the traditional Ingress API of (vanilla) Kubernetes. It is a set of resources that model service networking in Kubernetes, and is designed to be role-oriented, portable, expressive, and extensible.
The Cilium Service Mesh Gateway API Controller requires the ability to create LoadBalancer Kubernetes services.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu Kubernetes Grid - Linux Workstation Setup</title>
      <link>//localhost:1313/post/tanzu-kubernetes-grid-workstation-setup/</link>
      <pubDate>Sun, 08 Oct 2023 20:43:03 +0200</pubDate>
      
      <guid>//localhost:1313/post/tanzu-kubernetes-grid-workstation-setup/</guid>
      <description>
        
          
            &lt;h2 id=&#34;tkg-bootstrap-machine-on-fedora&#34;&gt;TKG Bootstrap Machine on Fedora&lt;/h2&gt;
&lt;p&gt;Tanzu Kubernetes Grid needs a workstation to bootstrap your Kubernetes Cluster. This is a short guide for a Fedora based bootstrap Machine:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to run Docker&lt;/li&gt;
&lt;li&gt;Access to download Tanzu CLI (via Customer Connect Portal)&lt;/li&gt;
&lt;li&gt;dedicated admin user in the wheel-group&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Docker on modern RHEL systems can  be confusing because the latest versions of RHEL replace Docker with Podman (dnf install docker will install podman). I should work with podman, but we explicity are going to install Docker Container Engine.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>VMware vSAN Network Considerations</title>
      <link>//localhost:1313/post/vmware-vsan-network-requirements/</link>
      <pubDate>Wed, 23 Aug 2023 23:13:14 +0200</pubDate>
      
      <guid>//localhost:1313/post/vmware-vsan-network-requirements/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;VMware vSAN is a Hyperconverged Infrastructure Solution from VMware. It basically turns your servers direct attached disks into a storage cluster.&lt;/p&gt;
&lt;h2 id=&#34;minimum-bandwidth-requirements-vsan-esa&#34;&gt;Minimum Bandwidth Requirements vSAN ESA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A minimum bandwidth of 25 Gbps is required, 10Gbit works but is only supported for the AF0 Config.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For larger clusters or environments with high-capacity NVMe disks, even 25 Gbps may be insufficient. If you can do 100Gbps, the Performance gain will be massive.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere Monitoring with Grafana</title>
      <link>//localhost:1313/post/vsphere-monitoring/</link>
      <pubDate>Tue, 08 Aug 2023 20:43:03 +0200</pubDate>
      
      <guid>//localhost:1313/post/vsphere-monitoring/</guid>
      <description>
        
          
            &lt;h1 id=&#34;vsphere-monitoring-with-tig-telegraf-influxdb-grafana&#34;&gt;vSphere Monitoring with TIG (Telegraf, InfluxDB, Grafana)&lt;/h1&gt;
&lt;p&gt;This blog post describes a solution for monitoring  SDDC infrastructure using Telegraf, InfluxDB, and Grafana. This solution is based on Docker and displays graphs and metrics via Grafana. All metrics are described in the &lt;em&gt;&lt;strong&gt;telegraf.conf&lt;/strong&gt;&lt;/em&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;picture&gt;

    
      
        
        
        
        
        
        
    &lt;img
      loading=&#34;lazy&#34;
      decoding=&#34;async&#34;
      alt=&#34;Grafana - vSphere Overview&#34;
      
        class=&#34;image_figure image_external image_processed&#34;
        width=&#34;3338&#34;
        height=&#34;1334&#34;
        src=&#34;//localhost:1313/images/QbfN11Q_17788218198938191418.png&#34;
      
      
    /&gt;

    &lt;/picture&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;TL;DR:
&lt;a href=&#34;https://github.com/varmox/vsphere-monitoring.git&#34;&gt;https://github.com/varmox/vsphere-monitoring.git&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;prerequiries&#34;&gt;Prerequiries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RHEL based Linux&lt;/li&gt;
&lt;li&gt;Docker &amp;amp; Docker Compose (or Podman with Docker Compose. In this tutorial we are using docker &amp;amp; docker compose) installed&lt;/li&gt;
&lt;li&gt;3 internal IPs for telegraf, grafana and influxdb containers.&lt;/li&gt;
&lt;li&gt;Access to vSphere API (read-only is sufficient)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>State of Enterprise IT-Security</title>
      <link>//localhost:1313/post/state-of-enterprise-itsecurity/</link>
      <pubDate>Sun, 09 Jul 2023 15:28:45 +0200</pubDate>
      
      <guid>//localhost:1313/post/state-of-enterprise-itsecurity/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Why current Enterprise IT-Security is s shipwreck.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The state of enterprise IT-Security is pretty bad and I don&#39;t think the current strategy to better it is any good. And here&#39;s the why:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;status quo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But firstly, what are the security concerns of businesses regarding IT-Security?&lt;/p&gt;
&lt;p&gt;If we take the CIA triad for help:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ransomware that disrups availability and integrity&lt;/li&gt;
&lt;li&gt;Data Leaks that violences confidentiality&lt;/li&gt;
&lt;li&gt;malware/virus that disrups integrity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;.. and more&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu Kubernetes Cluster Example Deployments</title>
      <link>//localhost:1313/post/tanzu-kubernetes-cluster-examples/</link>
      <pubDate>Fri, 23 Jun 2023 23:13:14 +0200</pubDate>
      
      <guid>//localhost:1313/post/tanzu-kubernetes-cluster-examples/</guid>
      <description>
        
          
            &lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;This post gives a few examples of Tanzu Kubernetes Cluster Manifests and how to deploy them.&lt;/p&gt;
&lt;h3 id=&#34;prerequiries&#34;&gt;Prerequiries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Successfull Installation of vSphere with Tanzu Supervisor&lt;/li&gt;
&lt;li&gt;vSphere Namespace created&lt;/li&gt;
&lt;li&gt;Already existing VM Classes, Storages Policies and Tanzu Kubernetes Releases assigned to the Namespace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;login-via-kubectl&#34;&gt;Login via kubectl&lt;/h2&gt;
&lt;p&gt;You will find the IP to your Kubernetes API on the Namespace Option &amp;quot;Link to CLI Tools&amp;quot;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl vsphere login --server=10.40.80.20
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl config get-contexts
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl config use-context &amp;lt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;get-parameters-of-your-vsphere-namespace&#34;&gt;Get Parameters of your vSphere Namespace&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;VM Class&lt;/li&gt;
&lt;li&gt;Storagge Policy&lt;/li&gt;
&lt;li&gt;Tanzu Releases&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get vmclass
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get storageclass
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get tanzukubernetesrelease
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cluster-with-3-control-and-6-worker-nodes&#34;&gt;Cluster with 3 Control and 6 Worker Nodes&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tanzu&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;com&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v1alpha3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TanzuKubernetesCluster&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tkgs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cluster&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;n&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tkgs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cluster&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ns&lt;/span&gt;   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;topology&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;controlPlane&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;vmClass&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;best&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;effort&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;large&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;storageClass&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;workload&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;management&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;policy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;tkr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;n&#34;&gt;reference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;23.8&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vmware&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tkg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zshippable&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;nodePools&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;worker&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;vmClass&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;best&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;effort&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;large&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;storageClass&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;workload&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;management&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;policy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Save this file as a yaml file on your workstation.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Quicktip: Tanzu &amp; HA-Proxy</title>
      <link>//localhost:1313/post/tanzu-haproxy-tips/</link>
      <pubDate>Wed, 08 Mar 2023 20:43:03 +0200</pubDate>
      
      <guid>//localhost:1313/post/tanzu-haproxy-tips/</guid>
      <description>
        
          
            &lt;h1 id=&#34;quick-tip-vsphere-with-tanzu-an-ha-proxy&#34;&gt;Quick Tip: vSphere with Tanzu an HA-Proxy&lt;/h1&gt;
&lt;p&gt;This is a short article with tips and tricks I experienced when deploying vSphere with Tanzu and HA-Proxy as a Load Balancer.&lt;/p&gt;
&lt;h2 id=&#34;deploying-ha-proxy-ova&#34;&gt;Deploying Ha-Proxy OVA&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Network&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alwasys use three NICs when deploying in a production environment. It is very important that these  IP addresses do not overlap with the address ranges of your workloads and load balancers, or with the gateway itself.&lt;/p&gt;
&lt;p&gt;At first it can be somewhat confusing what networks are needed and which services will be deployed in each network.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>K8 Cheat Sheet</title>
      <link>//localhost:1313/post/kubernetes/k8-cheat-sheet/</link>
      <pubDate>Sun, 04 Dec 2022 23:16:50 +0100</pubDate>
      
      <guid>//localhost:1313/post/kubernetes/k8-cheat-sheet/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Kubernetes Cheat Sheet&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cheat Sheet for kubectl, kubeadm and general k8s related commands.&lt;/p&gt;
&lt;h3 id=&#34;kubectl-interacting-with-clusters&#34;&gt;kubectl Interacting with clusters&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config view&lt;/td&gt;
          &lt;td&gt;View Kubectl Configuration&lt;/td&gt;
          &lt;td&gt;Display the current Kubectl configuration&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config get-contexts&lt;/td&gt;
          &lt;td&gt;List available contexts&lt;/td&gt;
          &lt;td&gt;See the available Kubernetes clusters and their associated contexts&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config current-context&lt;/td&gt;
          &lt;td&gt;Display the current context&lt;/td&gt;
          &lt;td&gt;Show the active Kubernetes cluster and context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config use-context &amp;lt;context_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Switch to another context&lt;/td&gt;
          &lt;td&gt;Change the active context to the specified one&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config set-context &amp;lt;context_name&amp;gt; --namespace=&amp;lt;namespace_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Set default namespace for a context&lt;/td&gt;
          &lt;td&gt;Specify the default namespace for a specific context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config unset current-context&lt;/td&gt;
          &lt;td&gt;Unset the current context&lt;/td&gt;
          &lt;td&gt;Remove the association with the current context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config set-cluster &amp;lt;cluster_name&amp;gt; --server=&amp;lt;api_server_url&amp;gt;&lt;/td&gt;
          &lt;td&gt;Add a new cluster to the configuration&lt;/td&gt;
          &lt;td&gt;Define a new Kubernetes cluster with its API server URL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config set-credentials &amp;lt;user_name&amp;gt; --token=&amp;lt;access_token&amp;gt;&lt;/td&gt;
          &lt;td&gt;Add user credentials to the configuration&lt;/td&gt;
          &lt;td&gt;Specify user credentials, typically an access token&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config set-context &amp;lt;context_name&amp;gt; --cluster=&amp;lt;cluster_name&amp;gt; --user=&amp;lt;user_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Create a new context&lt;/td&gt;
          &lt;td&gt;Combine a cluster and user to create a context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config delete-context &amp;lt;context_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete a context&lt;/td&gt;
          &lt;td&gt;Remove a specific context from the configuration&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config delete-cluster &amp;lt;cluster_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete a cluster&lt;/td&gt;
          &lt;td&gt;Remove a cluster from the configuration&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config delete-user &amp;lt;user_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete user credentials&lt;/td&gt;
          &lt;td&gt;Remove user credentials from the configuration&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config use-context &amp;lt;context_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Switch to another context&lt;/td&gt;
          &lt;td&gt;Change the active context to the specified one&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config rename-context &amp;lt;old_name&amp;gt; &amp;lt;new_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Rename a context&lt;/td&gt;
          &lt;td&gt;Change the name of an existing context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl config set preferences.colors true&lt;/td&gt;
          &lt;td&gt;Enable colorized output&lt;/td&gt;
          &lt;td&gt;Enhance readability with colorized command output&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;general-troubleshooting&#34;&gt;General Troubleshooting&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get events&lt;/td&gt;
          &lt;td&gt;Check events for resources&lt;/td&gt;
          &lt;td&gt;View events for debugging and troubleshooting&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe node &amp;lt;node_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Run diagnostics on a specific node&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;node_name&amp;gt;&lt;/code&gt; with the actual node name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get componentstatuses&lt;/td&gt;
          &lt;td&gt;Check the health of cluster components&lt;/td&gt;
          &lt;td&gt;Verify the status of essential components in the cluster&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get pods --all-namespaces&lt;/td&gt;
          &lt;td&gt;List all pods in all namespaces&lt;/td&gt;
          &lt;td&gt;Useful for identifying pods in any namespace&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe pod &amp;lt;pod_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a specific pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl logs &amp;lt;pod_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Retrieve the logs from a specific pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl top nodes&lt;/td&gt;
          &lt;td&gt;Display resource usage statistics for nodes&lt;/td&gt;
          &lt;td&gt;View CPU and memory usage for all nodes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl top pods&lt;/td&gt;
          &lt;td&gt;Display resource usage statistics for pods&lt;/td&gt;
          &lt;td&gt;View CPU and memory usage for all pods&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;nodes&#34;&gt;Nodes&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes&lt;/td&gt;
          &lt;td&gt;List all nodes in the cluster&lt;/td&gt;
          &lt;td&gt;Display a list of nodes and their status&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe node &amp;lt;node_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a node&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;node_name&amp;gt;&lt;/code&gt; with the actual node name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes -o wide&lt;/td&gt;
          &lt;td&gt;Display additional details about nodes&lt;/td&gt;
          &lt;td&gt;View IP addresses and other information about nodes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes --show-labels&lt;/td&gt;
          &lt;td&gt;Show labels assigned to nodes&lt;/td&gt;
          &lt;td&gt;Display labels associated with each node&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes -o json&lt;/td&gt;
          &lt;td&gt;Display node information in JSON format&lt;/td&gt;
          &lt;td&gt;View detailed node information in JSON&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl cordon &amp;lt;node_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Mark a node as unschedulable&lt;/td&gt;
          &lt;td&gt;Prevent new pods from being scheduled on the specified node&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl uncordon &amp;lt;node_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Mark a node as schedulable&lt;/td&gt;
          &lt;td&gt;Allow new pods to be scheduled on the specified node&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl drain &amp;lt;node_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Safely evict all pods from a node&lt;/td&gt;
          &lt;td&gt;Evacuate a node for maintenance&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl top nodes&lt;/td&gt;
          &lt;td&gt;Display resource usage statistics for nodes&lt;/td&gt;
          &lt;td&gt;View CPU and memory usage for all nodes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes --sort-by=.status.capacity.cpu&lt;/td&gt;
          &lt;td&gt;Sort nodes by CPU capacity&lt;/td&gt;
          &lt;td&gt;Sort nodes based on CPU capacity in ascending order&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl label nodes &amp;lt;node_name&amp;gt; &amp;lt;label_key&amp;gt;=&amp;lt;label_value&amp;gt;&lt;/td&gt;
          &lt;td&gt;Label a node&lt;/td&gt;
          &lt;td&gt;Assign a label to a specific node&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl taint nodes &amp;lt;node_name&amp;gt; &amp;lt;taint_key&amp;gt;=&amp;lt;taint_value&amp;gt;:&lt;effect&gt;&lt;/td&gt;
          &lt;td&gt;Taint a node&lt;/td&gt;
          &lt;td&gt;Apply a taint to a node for node affinity or tolerations&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe nodes&lt;/td&gt;
          &lt;td&gt;Get detailed information about all nodes&lt;/td&gt;
          &lt;td&gt;View detailed information about all nodes in the cluster&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get componentstatuses&lt;/td&gt;
          &lt;td&gt;Check the health of cluster components&lt;/td&gt;
          &lt;td&gt;Verify the status of essential components in the cluster&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.phase&lt;/td&gt;
          &lt;td&gt;Display custom columns for nodes&lt;/td&gt;
          &lt;td&gt;Define custom output columns for nodes&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;pods&#34;&gt;Pods&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get pods&lt;/td&gt;
          &lt;td&gt;List all pods in the current namespace&lt;/td&gt;
          &lt;td&gt;Display a list of pods and their status&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get pods -n &lt;namespace&gt;&lt;/td&gt;
          &lt;td&gt;List pods in a specific namespace&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;namespace&amp;gt;&lt;/code&gt; with the desired namespace&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe pod &amp;lt;pod_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl logs &amp;lt;pod_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Retrieve the logs from a specific pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl exec -it &amp;lt;pod_name&amp;gt; -- &lt;command&gt;&lt;/td&gt;
          &lt;td&gt;Execute a command in a running pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name and &lt;code&gt;&amp;lt;command&amp;gt;&lt;/code&gt; with the desired command&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl port-forward &amp;lt;pod_name&amp;gt; &amp;lt;local_port&amp;gt;:&amp;lt;pod_port&amp;gt;&lt;/td&gt;
          &lt;td&gt;Forward a pod&#39;s port to the local machine&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;local_port&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;pod_port&amp;gt;&lt;/code&gt; with the actual values&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl delete pod &amp;lt;pod_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete a specific pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl delete pods --all&lt;/td&gt;
          &lt;td&gt;Delete all pods in the current namespace&lt;/td&gt;
          &lt;td&gt;Be cautious when using this command&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl apply -f &amp;lt;pod_manifest.yaml&amp;gt;&lt;/td&gt;
          &lt;td&gt;Create or update a pod using a manifest file&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_manifest.yaml&amp;gt;&lt;/code&gt; with the path to the pod&#39;s YAML manifest&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get pod &amp;lt;pod_name&amp;gt; -o yaml&lt;/td&gt;
          &lt;td&gt;Get the YAML definition of a pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl exec -it &amp;lt;pod_name&amp;gt; -- /bin/bash&lt;/td&gt;
          &lt;td&gt;Open a shell in a running pod for debugging&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe pod &amp;lt;pod_name&amp;gt; | grep Image&lt;/td&gt;
          &lt;td&gt;Get the container image version running in a pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get storageclass&lt;/td&gt;
          &lt;td&gt;List available storage classes&lt;/td&gt;
          &lt;td&gt;Check the available storage classes in the cluster&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe storageclass &amp;lt;storageclass_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a storage class&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;storageclass_name&amp;gt;&lt;/code&gt; with the actual storage class name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get persistentvolumes&lt;/td&gt;
          &lt;td&gt;Display information about persistent volumes&lt;/td&gt;
          &lt;td&gt;View details of persistent volumes in the cluster&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe persistentvolume &amp;lt;pv_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a persistent volume&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pv_name&amp;gt;&lt;/code&gt; with the actual persistent volume name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get persistentvolumeclaims&lt;/td&gt;
          &lt;td&gt;List persistent volume claims&lt;/td&gt;
          &lt;td&gt;View information about persistent volume claims in the current namespace&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe persistentvolumeclaim &amp;lt;pvc_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a persistent volume claim&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pvc_name&amp;gt;&lt;/code&gt; with the actual persistent volume claim name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get pv,pvc&lt;/td&gt;
          &lt;td&gt;List both persistent volumes and claims&lt;/td&gt;
          &lt;td&gt;Display a combined list of persistent volumes and claims&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl apply -f &amp;lt;storage_manifest.yaml&amp;gt;&lt;/td&gt;
          &lt;td&gt;Create or update storage using a manifest file&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;storage_manifest.yaml&amp;gt;&lt;/code&gt; with the path to the storage YAML manifest&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl delete persistentvolume &amp;lt;pv_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete a specific persistent volume&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pv_name&amp;gt;&lt;/code&gt; with the actual persistent volume name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl delete persistentvolumeclaims --all&lt;/td&gt;
          &lt;td&gt;Delete all persistent volume claims in the current namespace&lt;/td&gt;
          &lt;td&gt;Be cautious when using this command&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get storageclass -o yaml&lt;/td&gt;
          &lt;td&gt;Get the YAML definition of a storage class&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;storageclass_name&amp;gt;&lt;/code&gt; with the actual storage class name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get persistentvolume -o yaml&lt;/td&gt;
          &lt;td&gt;Get the YAML definition of a persistent volume&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pv_name&amp;gt;&lt;/code&gt; with the actual persistent volume name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get persistentvolumeclaim -o yaml&lt;/td&gt;
          &lt;td&gt;Get the YAML definition of a persistent volume claim&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pvc_name&amp;gt;&lt;/code&gt; with the actual persistent volume claim name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl exec -it &amp;lt;pod_name&amp;gt; -- df -h&lt;/td&gt;
          &lt;td&gt;Check storage usage inside a pod&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;pod_name&amp;gt;&lt;/code&gt; with the actual pod name and &lt;code&gt;df -h&lt;/code&gt; with the desired command&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe storageclass &amp;lt;storageclass_name&amp;gt; | grep Provisioner&lt;/td&gt;
          &lt;td&gt;Get the provisioner information for a storage class&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;storageclass_name&amp;gt;&lt;/code&gt; with the actual storage class name&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;services&#34;&gt;Services&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;kubectl command&lt;/th&gt;
          &lt;th&gt;description&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get services&lt;/td&gt;
          &lt;td&gt;List all services in the current namespace&lt;/td&gt;
          &lt;td&gt;Display a list of services and their types&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get services -o wide&lt;/td&gt;
          &lt;td&gt;Display additional details about services&lt;/td&gt;
          &lt;td&gt;View IP addresses and ports of services&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe service &amp;lt;service_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about a service&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;service_name&amp;gt;&lt;/code&gt; with the actual service name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get service &amp;lt;service_name&amp;gt; -o yaml&lt;/td&gt;
          &lt;td&gt;Get the YAML definition of a service&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;service_name&amp;gt;&lt;/code&gt; with the actual service name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl expose deployment &amp;lt;deployment_name&amp;gt; --port=&amp;lt;port_number&amp;gt; --target-port=&amp;lt;target_port&amp;gt; --name=&amp;lt;service_name&amp;gt; --type=&amp;lt;service_type&amp;gt;&lt;/td&gt;
          &lt;td&gt;Expose a deployment as a service&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;deployment_name&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;port_number&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;target_port&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;service_name&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;service_type&amp;gt;&lt;/code&gt; with the actual values&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl apply -f &amp;lt;service_manifest.yaml&amp;gt;&lt;/td&gt;
          &lt;td&gt;Create or update a service using a manifest file&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;service_manifest.yaml&amp;gt;&lt;/code&gt; with the path to the service YAML manifest&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl delete service &amp;lt;service_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Delete a specific service&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;service_name&amp;gt;&lt;/code&gt; with the actual service name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get services --sort-by=.spec.ports[0].nodePort&lt;/td&gt;
          &lt;td&gt;Sort services by NodePort&lt;/td&gt;
          &lt;td&gt;Sort services based on NodePort value in ascending order&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get services -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP,EXTERNAL-IP:.status.loadBalancer.ingress[0].ip,PORT(S).:.spec.ports[*].nodePort&lt;/td&gt;
          &lt;td&gt;Display custom columns for services&lt;/td&gt;
          &lt;td&gt;Define custom output columns for services&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get services -n &lt;namespace&gt;&lt;/td&gt;
          &lt;td&gt;List services in a specific namespace&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;namespace&amp;gt;&lt;/code&gt; with the desired namespace&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get endpoints &amp;lt;service_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Display endpoints for a service&lt;/td&gt;
          &lt;td&gt;View the IP addresses and ports of pods backing a service&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl describe ingress &amp;lt;ingress_name&amp;gt;&lt;/td&gt;
          &lt;td&gt;Get detailed information about an ingress&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;ingress_name&amp;gt;&lt;/code&gt; with the actual ingress name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;kubectl get svc &amp;lt;service_name&amp;gt; -o=jsonpath=&#39;{.spec.ports[0].nodePort}&#39;&lt;/td&gt;
          &lt;td&gt;Get the NodePort of a service using JSONPath&lt;/td&gt;
          &lt;td&gt;Replace &lt;code&gt;&amp;lt;service_name&amp;gt;&lt;/code&gt; with the actual service name&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;operations&#34;&gt;Operations&lt;/h3&gt;
&lt;p&gt;Run kubectl as non-root user &lt;br&gt;
&lt;code&gt;non-root@cp: ̃$ mkdir -p $HOME/.kube&lt;/code&gt; &lt;br&gt;
&lt;code&gt;non-root@cp: ̃$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt; &lt;br&gt;
&lt;code&gt;non-root@cp: ̃$ sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt; &lt;br&gt;
&lt;code&gt;non-root@cp: ̃$ less .kube/config&lt;/code&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Huawei VRP</title>
      <link>//localhost:1313/post/networking/huawei-vrp/</link>
      <pubDate>Sun, 04 Dec 2022 15:11:13 +0100</pubDate>
      
      <guid>//localhost:1313/post/networking/huawei-vrp/</guid>
      <description>
        
          
            &lt;p&gt;&lt;strong&gt;Huawei Versatile Routing Platform (VRP) Overview&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This post gives an overview about the network operating system for huawei network devices - VRP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;General&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Task&lt;/th&gt;
          &lt;th&gt;Command&lt;/th&gt;
          &lt;th&gt;Comment&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Enter System View&lt;/td&gt;
          &lt;td&gt;&amp;lt; Huawei &amp;gt; sys&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Enter Interface View&lt;/td&gt;
          &lt;td&gt;[R1] int GigabitEthernet0/0/1&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Enter Protocol View&lt;/td&gt;
          &lt;td&gt;[R1] ospf 1&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Display Running Config in current View&lt;/td&gt;
          &lt;td&gt;[R1- GigabitEthernet0/0/3] display this&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Display current configuration of the System&lt;/td&gt;
          &lt;td&gt;[R1] dis current-config&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;System Configuration&lt;/strong&gt;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
