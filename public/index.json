[{"body":"","link":"//localhost:1313/","section":"","tags":null,"title":""},{"body":"","link":"//localhost:1313/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Dsm 9 K8s"},{"body":"","link":"//localhost:1313/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"//localhost:1313/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"//localhost:1313/tags/tag_name1/","section":"tags","tags":null,"title":"Tag_name1"},{"body":"","link":"//localhost:1313/tags/tag_name2/","section":"tags","tags":null,"title":"Tag_name2"},{"body":"","link":"//localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"//localhost:1313/categories/technology/","section":"categories","tags":null,"title":"Technology"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-upgrade/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Dsm 9 Upgrade"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/vcf-9-upgrade/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Vcf 9 Upgrade"},{"body":"","link":"//localhost:1313/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"Cannot delete Tanzu Package Error Messaage Error: Getting service account: serviceaccounts \u0026quot;grafana-tanzu-system-dashboards-sa\u0026quot; not found\nResolution\nkubectl get apps grafana -oyaml | grep finalizer kubectl patch app grafana -n \u0026lt; my namespace \u0026gt; -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:[]}}\u0026#39; --type=merge kubectl delete app grafana ","link":"//localhost:1313/post/tanzu-package-troubleshooting/","section":"post","tags":["Tanzu Packages","Troubleshooting"],"title":"Tanzu Package Troubleshooting"},{"body":"","link":"//localhost:1313/tags/tanzu-packages/","section":"tags","tags":null,"title":"Tanzu Packages"},{"body":"","link":"//localhost:1313/tags/troubleshooting/","section":"tags","tags":null,"title":"Troubleshooting"},{"body":"","link":"//localhost:1313/categories/vmware/","section":"categories","tags":null,"title":"VMware"},{"body":"","link":"//localhost:1313/tags/vmware-ecs/","section":"tags","tags":null,"title":"VMware ECS"},{"body":"VMware Edge Cloud Compute Stack is a Solution for running workloads on the Edge - \u0026quot;Edge\u0026quot; in this case means running a modified Version of ESXi on really small compute footprint. Management is done from a central Appliane called VECO (VMware Edge Cloud Orchestrator).\nECS vs VCF\nECS has nothing todo with VMware Cloud Foundation Edge (VCF-Edge). VCF Edge is just a SKU. VCF has different Edge Reference Architectures. ECS is a different Product, generally meat for smaller compute needs.\nManagement - Edge Cloud Orchestrator Unlinke vSphere you won't deploy a vCenter to manage your ECS Hosts. In a classic vSphere Infrastructure the vCenter is Control- and Management Plane. In ECS your Management Plane is the VMware Edge Cloud Orchestrator (VECO). This Appliance runs either as a SaaS Service (on Google Cloud) or can be deployed on-premises.\nControl Plane Components are always residing on the Edge Site, so you do not need constant connectivity to VECO.\nCompute - ESC Hosts ECS Hosts (a modified Version of ESXi, but with full ESXi Capabilities) can be even consumer Hardware with very small energy consumption. Like Barebones, NUCs, etc. You do not need beefy enterprise servers.\nYou can even have USB to RJ45 Network Adapters with the VMware USB Network Driver Fling!\nJust follow the VMware VGC for supported Hardware.\nEach ESC Host has to be booted from a custom ISO, which can be downloaded from VECO. The ISO is a little bit over 2GB in Size.\nGitOps Approach The unique thing about ECS is you manage the ECS Hosts and the Workloads with a GitOps approach. The configuration of the ECS Hosts like Networking, vSwitch Config is defined via yaml and stored in a Git Repository. The ECS Hosts has FluxCD installed on will check within the GitRepo for new yaml Manifests. FluxCD has a pull based approach - the destination (the ECS Host) will pull the config (yaml Files) themselves. With this approach it isn't a big Problem if the Edge Site does not have constant connectivity.\nYou cannot modify anything manually, the ECS Hosts will only have the GitRepo as a Configuration Source.\nThe ESC Hosts must have Network Connectivity to the desired GitRepo you add within VECO.\nExample\n#Example host configuration adding a new vSwitch and portgroup and assigning them to vmnic1 apiVersion: esx.vmware.com/v1alpha1 kind: HostConfiguration metadata: name: vswitch-config namespace: esx-system spec: layertype: Incremental profile: | { \u0026#34;esx\u0026#34;: { \u0026#34;network_vss\u0026#34;: { \u0026#34;switches\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;vSwitch1\u0026#34;, \u0026#34;policy\u0026#34;: { \u0026#34;nic_teaming\u0026#34;: { \u0026#34;policy\u0026#34;: \u0026#34;LOADBALANCE_SRCID\u0026#34;, \u0026#34;notify_switches\u0026#34;: true, \u0026#34;rolling_order\u0026#34;: false, \u0026#34;link_criteria_beacon\u0026#34;: \u0026#34;IGNORE\u0026#34;, \u0026#34;active_nics\u0026#34;: [ \u0026#34;vmnic1\u0026#34; ], \u0026#34;standby_nics\u0026#34;: [] }, \u0026#34;security\u0026#34;: { \u0026#34;allow_promiscuous\u0026#34;: false, \u0026#34;mac_changes\u0026#34;: false, \u0026#34;forged_transmits\u0026#34;: false }, \u0026#34;traffic_shaping\u0026#34;: { \u0026#34;enabled\u0026#34;: false } }, \u0026#34;bridge\u0026#34;: { \u0026#34;link_discovery_protocol\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;CDP\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;LISTEN\u0026#34; }, \u0026#34;nics\u0026#34;: [ \u0026#34;vmnic1\u0026#34; ] }, \u0026#34;num_ports\u0026#34;: 1024, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;port_groups\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;VM Network 16\u0026#34;, \u0026#34;vlan_id\u0026#34;: 0 } ] } ] } } } Workloads Workloads on ECS can either be classical VMs or Kubernetes Workloads. For Kubernetes Workload a single Kubernetes Worker Node will be deployed on the ESC Host. ESC Kubernetes has nothing todo with Tanzu or vSphere Kubernetes Services. It's a much simpler approach with less footprint.\nVM Deployment are done via Virtual Machine Classes (same approach as VMClasses in vSphere IaaS Controle Plane). You can either deploy Linux or Windows Workloads.\nWindows Workloads are also deployed with yaml Manifest. You can even use sysprep or Powershell within the yaml. Thats quite a new way to deploy Windows!\nExample:\napiVersion: vmoperator.vmware.com/v1alpha1 kind: VirtualMachine metadata: name: windows-server namespace: default spec: className: best-effort-large imageName: windows-server-2022.ova powerState: poweredOn networkInterfaces: - network: workload-network vmMetadata: configMapName: windows-sysprep-config transport: ExtraConfig --- apiVersion: v1 kind: ConfigMap metadata: name: windows-sysprep-config namespace: default data: userdata: | #cloud-config hostname: win-server users: - name: administrator passwd: VMware1! runcmd: - powershell -Command \u0026#34;Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;))\u0026#34; - choco install -y googlechrome VMware Docs\nMore Example Manifests for ESC\nKubernetes To deploy a Kubernetes Worker Node on the ESC Host use the following example yaml File:\n# Example of how to change the default kubernetes worker node configuration to provide more memory or CPUs apiVersion: vmoperator.vmware.com/v1alpha1 kind: VirtualMachineClass metadata: name: best-effort-ec-small # No resource requests/reservations. # Memory overcommit possible. spec: # VM operator controller name for VirtualMachine class # is different so addition of controllerName attr with invalid # value makes vm operator ignore this VM CRD instance. # This is majorly done because vm operator doesn\u0026#39;t have all the # needed capabilities to deploy ec worker vm. controllerName: vmoperator.vmware.com/ecworker hardware: cpus: 4 memory: 8Gi --- apiVersion: v1 kind: Namespace metadata: name: ec-system --- apiVersion: vmoperator.vmware.com/v1alpha1 kind: VirtualMachine metadata: name: ec-worker namespace: ec-system spec: className: best-effort-ec-small imageName: ec-wrkr.ova powerState: poweredOff vmMetadata: transport: CloudInit Note\nAfter the worker virtual machine is created and booted, changes to desired state are not applied to it until the next Edge Compute Stack Host reboot.\nKubernetes Networking\nKubernetes Networking on the ESC Hosts is very simple. No need for Ingress Controllers. Workloads can be directly exposed with Nodeport.\nVMware Docs\nNetworking LAN Connectivity will rely on vSwitches, NSX isn't designed for ECS as the footprint is too big. But for the WAN Connectivty, well there is a really good solution: VeloCloud SD-WAN\nSD-WAN VeloCloud SD-WAN can either be deployed as a Virtual Appliance or Hardware. VeloCloud allows for SD-WAN Connectivity, L3+L4+L7 Firewall and VPN Services as well as a lot of other features. VeloCloud SD-WAN deploys so called \u0026quot;Edges\u0026quot;, the Appliance that will handle the traffic (Data Plane). The Management will be done from the SD-WAN Orchestrator Appliance, either running onpremises or as a SaaS (running on Google Cloud).\nClusters As of now two topologies are supported in a Edge Site:\nOne Node Two Node I heard some rumors that three Nodes ESC Setup (event with vSAN) is comming ;)\nAdditional Ressources ESC Examples ESC Admin Guide Getting started ","link":"//localhost:1313/post/vmware-edge-compute-stack/","section":"post","tags":["VMware ECS","VMware VECO"],"title":"VMware Edge Compute Stack"},{"body":"","link":"//localhost:1313/tags/vmware-veco/","section":"tags","tags":null,"title":"VMware VECO"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/cis-hardening-vsphere/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Cis Hardening Vsphere"},{"body":"vSAN ESA 2-Node Cluster Guide and Recommendations This Guide will help you in your Design Considerations and during the Deployment of a 2-Node vSAN Cluster.\nAssumptions The following assumptions where made when creating this blog post.\nYou have vSAN Ready Nodes or all Hardware is on the vSAN Hardware Compatibility List. You have VMware Cloud Foundation (VCF) Licenses or VVF with enough vSAN Add-on Licences Phyiscal Network Adapter for vSAN have a Bandwith of 25Gbits or above General Knowledge of vSAN and vSphere Design Constraints Only one host failure can be tolerated at a time (FTT1) Adding nodes beyond two would require reconfiguration to a standard vSAN cluster. Requires a witness host, which needs two IP addresses - one for management and one for vSAN traffic Managing multiple 2-node clusters can increase operational complexity General Considerations Do you have a dedicated vCenter for your 2-Node Cluster? Where do you run the vCenter VM? On the 2-Node Cluster itself or outside? Does the two physical Server run on the same physical site? vSAN Traffic: direct attached or Switched With a 2-Node Cluster you have the possibility to run vSAN (and vMotion) traffic over direct attached Cables. This is a great solution for RoBo or Site which do not offer 25Gbits Ports on Network Switches.\nJust consider the following:\nExpanding your vSAN 2-Node Cluster to a 3-Node Cluster can be a little bit tricky. If your setup vMotion vmk on the direct attached links, plan you migration path. If you want to migrate from other vSphere Clusters, additional vMotion vmks should be created (which have a L3 Uplink to your network). vSAN Witness If you go directed attached you need a addtional vmk that handels the traffic to the vSAN Witness Appliance.\nvSAN Witness Appliance should NOT run inside the 2-Node vSAN Cluster Have a L3 or L2 Network for vSAN Witness traffic to your vSphere Cluster which hosts the vSAN Witness Appliance. If you have a dedicated vCenter for your 2-Node vSAN Cluster: the Witness Appliance is added to a seperate Cluster (Witness Cluster) to the 2-Node vSAN vCenter. Network: Physical and vmkernel Adapter Example The following graphics is an example of a 2-Node vSAN Cluster:\nPhysical NICs VDS Switches VMkernel Interfaces +----------+ +-----------+ +----------------------+ | vmnic0 |-----| FRONTEND |---| vmk0 (Management) | +----------+ /| VDS | | vmk1 (vSAN Witness) | | vmnic2 |---/ +-----------+ +----------------------+ +----------+ +----------+ +-----------+ +----------------------+ | vmnic1 |-----| BACKEND |-----| vmk2 (vSAN Data) | +----------+ /| VDS |\\ | vmk3 (vMotion) | | vmnic3 |---/ +-----------+ \\ +----------------------+ +----------+ Per Hosts we have four physical Network Adapter:\n2x10Gbits Uplinks for Frontend Traffic (Workload, vSAN Witness) - those could also be 1Gbit links, depending on the Workloads 2x25Gbits directed attached Links for vSAN and vMotion Traffic Distributed Switch and Distributed Portgroup: FRONTEND VDS\nPortgroup vmk Service MTU Teaming and Failover vpg_management Management 1500 Route based on physical NIC load vpg_vsan-witness vSAN Witness 1500 Route based on physical NIC load BACKEND VDS\nPortgroup vmk Service MTU Teaming and Failover vpg_vsan-data vSAN 9000 Route based on physical NIC load vpg_vmotion vMotion 9000 Route based on physical NIC load For IP and VLAN assigned I suggest you reserve the VLAN + IP-Ranges within your companies IPAM, a) IP Adress conflict happens b) you have it documented.\nMaintenance Scenario When you want to Update your ESXi Hosts there are a few specialities in 2-Node vSAN Cluster Setup:\nFull data migration is not possible when entering maintenance mode. You must use the \u0026quot;Ensure accessibility\u0026quot; option when putting a host into maintenance mode When using Lifecycle Manager (vLCM) with Cluster Images the Remedation Process will not put Hosts into Maintenance Mode. Also Maintenance Mode will not evacuate any Workloads. Why is that?\nWell a 2-Node vSAN Cluster will only support the VM Storage Policy of Mirror/RAID1. An DRS want to ensure that the virtual object will be in sync. Iif a Hosts is down in a 2-Node vSAN Cluster, your virtual Objects will always be affected.\nTo perform maintenance on a 2-node vSAN cluster:\nManually vMotion VMs to the other host. Put the host into maintenance mode using \u0026quot;Ensure accessibility\u0026quot; option. Perform the required maintenance Performance Performance Benchmarking should be done with HCIBench.\nExample\nSetup:\n2x HPE DL365 Gen11 25Gbit DAC between the Hosts 7x7,68Tb Samsung PM9A3 SSD ","link":"//localhost:1313/post/vsan-2-node/","section":"post","tags":["vSAN","vSphere"],"title":"2-Node vSAN Cluster"},{"body":"","link":"//localhost:1313/tags/vsan/","section":"tags","tags":null,"title":"VSAN"},{"body":"","link":"//localhost:1313/tags/vsphere/","section":"tags","tags":null,"title":"VSphere"},{"body":"Intro This Blogpost explains RBAC on vSphere Kubernetes Service (VKS), formely known as Tanzu Kubernetes Grid Service (TKGS). It also shows how to give access via kubectl with granular permissions, leveraging OIDC Auth with pinniped on VKS-Clusters.\nNaming I use VKS-, TKGS- and K8s Clusters in this Blog Post. All have the same meaning - simply a Kubernetes Cluster :)\nRBAC on vSphere Kubernetes Service Role-based Access Control on vSphere Kubernetes Cluster has a few Keypoint which should be though about. The main question you have to ask yourself: Which Level of access do I want to grant to which people?\nOption 1: vSphere Namespace Permissions With vSphere Namespace Permissions (Select Permissions \u0026gt; Add Permissions) you grant access directly on the vSphere Namespace via vCenter.\nOption Description Can View Can read TKG cluster objects in the vSphere Namespace. No permissions mapped to Kubernetes roles. See Role Permissions and Bindings. Can Edit Can create, read, update, and delete TKG cluster objects in the vSphere Namespace. Can operate TKG clusters provisioned in the vSphere Namespace as the Kubernetes cluster-admin. See Role Permissions and Bindings. Owner Same permissions as Can Edit, with the additional permission to create and manage vSphere Namespaces using kubectl. Only available with vCenter SSO. See Role Permissions and Bindings. This means - depending on the Permission you give:\nUser has Access to all Ressources within that vSphere Namespace: all TKGS/VKS Guest Clusers (kubectl) all VMs wihtin that Namespace Create new TKGS/VKS Clusters Create new VMs But I only want to give \u0026quot;some\u0026quot; permissions to the Kubernetes Cluster for my devs - now Kubernetes RBAC Concept comes into play.\nOption 2: K8s RBAC With Kubernetes RBAC you have way more granular control over the permission:\nCore Components of Kubernetes RBAC\nRoles: Define a set of permissions for accessing Kubernetes resources within a single namespace. ClusterRoles: Similar to Roles but cluster-scoped, allowing permissions across all namespaces and for cluster-wide resources. RoleBindings: Grant the permissions defined in a Role to users or service accounts within a specific namespace. ClusterRoleBindings: Cluster-scoped version of RoleBindings, granting permissions defined in a ClusterRole across the entire cluster. +-------------+ | User | +------+------+ | v +-------------------+ | Authentication | +--------+----------+ | v +-------------------+ | Authorization | +--------+----------+ | v +-------------+-------------+ | | v v +-------------+ +----------------+ | Roles | | ClusterRoles | +------+------+ +--------+-------+ | | | +-------------+ | | v v +-------------+ +-----------------+ | RoleBinding | | ClusterRoleBinding | +------+------+ +-----------------+ | | | +--------------+ | | v v +----+----+----+ | Resources | | (Namespaced) | +-------------+ 1.) The process starts with a user attempting to access the cluster.\n2.) Authentication verifies the user's identity.\n3.) Authorization (RBAC) determines what actions the user can perform.\n4.) Roles and ClusterRoles define permissions for resources.\n5.) RoleBindings and ClusterRoleBindings associate users with roles.\n6.) Finally, access is granted or denied to the requested resources based on the RBAC configuration.\nWhen to choose what Choosing between vSphere Namespace Permissions and K8s RBAC can be simplified to the following:\nFull Access to the all VKS/TKGS Cluster within a vSphere Namespace -\u0026gt; vSphere Namespace Permissions Granular Access to VKS/TKGS Cluster or even just K8s Namespace -\u0026gt; K8s RBAC K8s RBAC on VKS with pinniped Scenario: Developer Access to a K8s Namespace\nScenario Intro:\nYour companies Dev-Team needs kubectl access to a K8s Namespace on a shared VKS/TKGS Cluster. You (Infra Admin) have vSphere Namespace Permissions of \u0026quot;Edit\u0026quot; or \u0026quot;Owner\u0026quot;, also admin rights to the vSphere Supervisor You have a OIDC Provider (GitLab, WorkspaceONE Access, etc) already in place We will configure the infrastructure that the following is possible:\nDevs Team will have a kubeconfig File, which they can use to logon via kubectl GitLab will act as a SSO Provider Role and RoleBindings are applied to the VKS-Cluster to grant permissions to the K8s-Namespace. +------------------+ | User | +--------+---------+ | v +------------------+ +--------------------------------------------------------+ | Identity Provider|\u0026lt;--\u0026gt;|Pinniped Supervisor - running on the vSphere Supervisor | +------------------+ +--------------------------------------------------------+ ^ | +------+------+ | Pinniped | | Concierge | +------+------+ | v +------------------+ +------------------+ | vSphere | | Tanzu Kubernetes | | Namespace | | Cluster | +------------------+ +------------------+ Configure SSO with vSphere Supervisor First we will configure the vSphere Supervisor to use GitLab as a Identity Provider. For that we need to create a OIDC Application in GitLab:\nGo to your GitLab Admin Panel (eg. https://gitlab.yourdomain.tld/admin/applications ) Create a Application Example: Name: my-supervisor.yourdomain.tld Redirect URI: https://supervisor-fqdn/wcp/pinniped/callback Scopes: read_user openid profile email Futher Information: GitLab Docs\nThen configure GitLab as a Identity Provider in the vSphere Supervisor:\nExample:\nProvider Name: gitlab-oidc Issuer URL: https://gitlab.mydomain.local Username Claim: nickname Groups Claim: groups Client ID: the ID from your GitLab Application Client Secret: the Secret from your GitLab Application Additional Scopes: openid Certififacte Authority Data: PEM Formatted Cert The configuration of a Idendity Provider will install pinniped supervisor on the vSphere Namespace. Pinniped is used for OIDC.\nRole and RoleBinding GitLab Configuration Be careful with the \u0026quot;name: my-gitlab-group\u0026quot; within your Kubernetes Role. It really depends on how the GitLab Group was created. I \u0026gt; personally had problems with nested groups or groups created on projects. I always created the GitLab Groups/Membership with a Admin, didn't use the Group from a GitLab project.\nFurther we need a Role and a RoleBinding. Apply those on the actual Kubernetes Cluster.\nRoleBinding\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: rbac-default-developer-rolebinding namespace: NAMESPACE_NAME roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbac-default-developer-role subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: my-gitlab-group Role\nAdjust the permissions as needed\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: rbac-default-developer-role namespace: my-k8s-namespace rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;apps\u0026#34;, \u0026#34;batch\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;, \u0026#34;statefulsets\u0026#34;, \u0026#34;jobs\u0026#34;, \u0026#34;cronjobs\u0026#34;, \u0026#34;pods/exec\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;, \u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;networking.k8s.io\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;cluster.x-k8s.io\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;namespaces\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] Create the kubeconfig File Permissions Use your Account that has \u0026quot;Edit\u0026quot; or \u0026quot;Owner\u0026quot; Permissions on the vSphere Namespace, where the VKS-Cluster is residing in.\nTo create a kubeconfig file run the following commands:\ntanzu context create my-kubernetes-cluster --endpoint https://SupervisorVIP tanzu cluster kubeconfig get my-kubernetes-cluster -n my-vSphere-Namespace --export-file my-kubeconfig Note: The kubeconfig File does not store any personalized credentials, so can safely be shared with your devs. The kubeconfig also isn't specific to a K8s-Namespace. The kubeconfig is specific to a Kubernetes Cluster (kubeapi-server IP).\nYou could also use the Local Consumption Interface to download the kubeconfig:\nTest the Access Either let the user test or you can use the following command with your administrator account on the k8s cluster:\nkubectl auth can-i get pods -n namespace --as=user From a Dev's point of view, do the following:\nInstall Tanzu CLI and the pinniped auth Plugin:\ntanzu plugin install pinniped-auth Run kubectl with a the created kubeconfig file.\nkubectl --kubeconfig ./my-kubeconfig get ns Now you will be given a link (to the pinniped Service running on the vSphere Supervisor). Copy this link into your browser, GitLab will open (if not already logged in) and authenticate you. Afterwards you will be presented with a Token. Copy this token back in to your terminal to get access to your kubernetes cluster or namespace.\n$ kubectl --kubeconfig ./my-kubeconfig get pods Optionally, paste your authorization code: G2TcS145Q4e6A1YKf743n3BJlfQAQ_UdjXy38TtEEIo.ju4QV3PTsUvOigVUtQllZ7AJFU0YnjuLHTRVoNxvdZc âœ” successfully logged in to management cluster using the kubeconfig Checking for required plugins... All required plugins are already installed and up-to-date NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-7tzxb 1/1 Running 0 3d nginx-deployment-66b6c48dd5-8p4vk 1/1 Running 0 3d nginx-deployment-66b6c48dd5-f8t5r 1/1 Running 0 3d Optional: Overwrite or create the default kubeconfig file:\ncp my-kubeconfig ~/.kube/config Et voila, now your Developer has access via kubectl.\nScenario: Developer Access to a K8s Cluster, but not will full permission\nBasically the same as above, but now you will use a K8s ClusterRole and a ClusterRoleBinding instead of a Role \u0026amp; RoleBinding.\nClusterRole\nAdjust the permissions as needed\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: rbac-default-cluster-operator-clusterrole aggregationRule: clusterRoleSelectors: - matchLabels: rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;namespaces\u0026#34;, \u0026#34;configmaps\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;secrets\u0026#34;, \u0026#34;persistentvolumeclaims\u0026#34;, \u0026#34;persistentvolumes\u0026#34;, \u0026#34;services\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;, \u0026#34;statefulsets\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;networking.k8s.io\u0026#34;] resources: [\u0026#34;ingressclasses\u0026#34;, \u0026#34;ingresses\u0026#34;,\u0026#34;networkpolicies\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: rbac-default-cluster-operator-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: rbac-default-cluster-operator-clusterrole subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: my-gitlab-group Further Thoughts GitOps your K8s RBAC!\nDefine a default ClusterRole (for K8s-Cluster-Access) Define a default Role (for K8s-Namespace Access) Use ArgoCD or FluxCD to roll-out your RBAC Config *Token Lifetime\nDepending on your IdP, the Token TTL defines how many times the Dev have to re-authenticate. GitLab isn't really configurabe as a \u0026quot;real\u0026quot; IdP. Look at WorkspaceONE Access, Zitadel for further configurations.\n","link":"//localhost:1313/post/k8s-rbac/","section":"post","tags":["vSphere Kubernetes Service","Tanzu Kubernetes","vSphere with Tanzu","Tanzu Kubernetes Grid"],"title":"RBAC on VKS"},{"body":"","link":"//localhost:1313/tags/tanzu-kubernetes/","section":"tags","tags":null,"title":"Tanzu Kubernetes"},{"body":"","link":"//localhost:1313/tags/tanzu-kubernetes-grid/","section":"tags","tags":null,"title":"Tanzu Kubernetes Grid"},{"body":"","link":"//localhost:1313/tags/vsphere-kubernetes-service/","section":"tags","tags":null,"title":"VSphere Kubernetes Service"},{"body":"","link":"//localhost:1313/tags/vsphere-with-tanzu/","section":"tags","tags":null,"title":"VSphere With Tanzu"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/harbor/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Harbor"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/supervisor-services/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Supervisor Services"},{"body":"Velero This blog post explains how to install velero on different configurations on vSphere with Tanzu / IaaS Control Plane.\nDifferent Velero Installations with TGKS Depending on your Supervisor Cluster Configuration, there are different ways to install Velero. Each approach offers different usage on velero.\nWays to Install Velero Velero Plugin for vSphere Velero \u0026quot;Standalone\u0026quot; via Helm Chart Velero \u0026quot;Standalone\u0026quot; via velero-cli ","link":"//localhost:1313/post/velero-standalone/","section":"post","tags":["vSphere Kubernetes Service","Velero"],"title":"Velero"},{"body":"","link":"//localhost:1313/tags/velero/","section":"tags","tags":null,"title":"Velero"},{"body":"","link":"//localhost:1313/tags/cisco-aci/","section":"tags","tags":null,"title":"Cisco ACI"},{"body":"","link":"//localhost:1313/tags/vsphere-networking/","section":"tags","tags":null,"title":"VSphere Networking"},{"body":"VMs with conflicted PortIDs In certain scenarios it can happend that the PortID (distributed port group) of a VM shows up with the prefix \u0026quot;-c\u0026quot; (eg c-245). vCenter Server creates a conflict port if multiple virtual machine's point to the same port. It does not automatically reconfigure the virtual machine to connect to a regular port if the conflict remains and the virtual machine stays connected to the conflict port.\nSee: https://knowledge.broadcom.com/external/article/318950/vnetwork-distributed-switch-contains-dvp.html\nThis issue ussually happens when the distributed vSwitch is out-of sync. I've seen this mostly on Cisco ACI VMM Setups. I strongly do not recommend a Cisco ACI VMM Integration with vSphere.\nAnyway, the issue is there, we need to fix it. The fix is quite easy, disconnect the Network Adapter from the VM, save the VM config, then reconnect again. For this manual process I created some scripts:\nPowerCLI Script - Find VMs with conflicted PortIDs Newest Version here\n# Load the PowerCLI SnapIn and set the configuration Add-PSSnapin VMware.VimAutomation.Core -ea \u0026#34;SilentlyContinue\u0026#34; Set-PowerCLIConfiguration -InvalidCertificateAction Ignore -Confirm:$false | Out-Null # Get the vCenter Server address, username and password as PSCredential $vCenterServer = Read-Host \u0026#34;Enter vCenter Server host name (DNS with FQDN or IP address)\u0026#34; $vCenterUser = Read-Host \u0026#34;Enter your user name (DOMAIN\\User or user@domain.com)\u0026#34; $vCenterUserPassword = Read-Host \u0026#34;Enter your password (this will be converted to a secure string)\u0026#34; -AsSecureString:$true $Credentials = New-Object System.Management.Automation.PSCredential -ArgumentList $vCenterUser,$vCenterUserPassword # Connect to the vCenter Server with collected credentials Connect-VIServer -Server $vCenterServer -Credential $Credentials | Out-Null Write-Host \u0026#34;Connected to your vCenter server $vCenterServer\u0026#34; -ForegroundColor Green Add-PSSnapin VMware.VimAutomation.Core -ea \u0026#34;SilentlyContinue\u0026#34; Set-PowerCLIConfiguration -InvalidCertificateAction Ignore -Confirm:$false | Out-Null $vms = Get-VM $vmInfo = @() foreach ($vm in $vms) { $networkAdapters = Get-NetworkAdapter -VM $vm foreach ($adapter in $networkAdapters) { $portID = $adapter.ExtensionData.Backing.Port.PortKey # Only process if the portID starts with \u0026#34;c-\u0026#34; if ($portID -and $portID.StartsWith(\u0026#34;c-\u0026#34;)) { # Create a custom object with VM and port information $vmData = [PSCustomObject]@{ VMName = $vm.Name PowerState = $vm.PowerState NetworkName = $adapter.NetworkName PortID = $portID } # Add the object to the array $vmInfo += $vmData } } } $vmInfo | Export-Csv -Path \u0026#34;C:\\source\\VMPortInfo.csv\u0026#34; -NoTypeInformation PowerCLI Script - Find VMs with conflicted PortIDs and fix it This script then detects VMs with \u0026quot;c-\u0026quot; PortID and does the following:\nlist you all VMs will ask, if it should fix the Issue -\u0026gt; disconnects vNIC, waits 5sec, and reconnects the adapter Option to Ping the VMs afterwards (based on the IPs it reads out from VMware Tools) Newest Version here\n# Load the PowerCLI SnapIn and set the configuration Add-PSSnapin VMware.VimAutomation.Core -ea \u0026#34;SilentlyContinue\u0026#34; Set-PowerCLIConfiguration -InvalidCertificateAction Ignore -Confirm:$false | Out-Null # Get the vCenter Server address, username and password as PSCredential $vCenterServer = Read-Host \u0026#34;Enter vCenter Server host name (DNS with FQDN or IP address)\u0026#34; $vCenterUser = Read-Host \u0026#34;Enter your user name (DOMAIN\\User or user@domain.com)\u0026#34; $vCenterUserPassword = Read-Host \u0026#34;Enter your password (this will be converted to a secure string)\u0026#34; -AsSecureString:$true $Credentials = New-Object System.Management.Automation.PSCredential -ArgumentList $vCenterUser,$vCenterUserPassword # Connect to the vCenter Server with collected credentials Connect-VIServer -Server $vCenterServer -Credential $Credentials | Out-Null Write-Host \u0026#34;Connected to your vCenter server $vCenterServer\u0026#34; -ForegroundColor Green # Get all VMs with portID starting with \u0026#34;c-\u0026#34; $problematicVMs = Get-VM | Get-NetworkAdapter | Where-Object { $_.ExtensionData.Backing.Port.PortKey -like \u0026#34;c-*\u0026#34; } | Select-Object -ExpandProperty Parent -Unique # Print out the VMs with problematic portIDs Write-Host \u0026#34;VMs with portID \u0026#39;c-\u0026#39;:\u0026#34; $problematicVMs | ForEach-Object { Write-Host $_.Name } # Ask for confirmation to fix the issues $confirmation = Read-Host \u0026#34;Do you want to fix those issues for the following VMs - resulting in 5sec Network Connectivity loss? (yes/no)\u0026#34; if ($confirmation -eq \u0026#34;yes\u0026#34;) { foreach ($vm in $problematicVMs) { Write-Host \u0026#34;Processing $($vm.Name)...\u0026#34; # Disconnect the network adapter Get-NetworkAdapter -VM $vm | Where-Object { $_.ExtensionData.Backing.Port.PortKey -like \u0026#34;c-*\u0026#34; } | Set-NetworkAdapter -Connected $false -Confirm:$false # Wait for 5 seconds Start-Sleep -Seconds 5 # Reconnect the network adapter Get-NetworkAdapter -VM $vm | Where-Object { $_.ExtensionData.Backing.Port.PortKey -like \u0026#34;c-*\u0026#34; } | Set-NetworkAdapter -Connected $true -Confirm:$false Write-Host \u0026#34;Fixed network adapter for $($vm.Name)\u0026#34; } # List all affected VMs Write-Host \u0026#34;Affected VMs:\u0026#34; $problematicVMs | ForEach-Object { Write-Host $_.Name } # Option to read out IP and ping VMs $pingOption = Read-Host \u0026#34;Do you want to read out IPs and ping the affected VMs? (yes/no)\u0026#34; if ($pingOption -eq \u0026#34;yes\u0026#34;) { foreach ($vm in $problematicVMs) { $ip = $vm.Guest.IPAddress[0] if ($ip) { Write-Host \u0026#34;$($vm.Name) IP: $ip\u0026#34; $pingResult = Test-Connection -ComputerName $ip -Count 1 -Quiet if ($pingResult) { Write-Host \u0026#34;Ping successful for $($vm.Name)\u0026#34; } else { Write-Host \u0026#34;Ping failed for $($vm.Name)\u0026#34; } } else { Write-Host \u0026#34;Unable to retrieve IP for $($vm.Name)\u0026#34; } } } } else { Write-Host \u0026#34;Operation cancelled. No changes were made.\u0026#34; } Hope this helps fixing the Issue.\n","link":"//localhost:1313/post/vsphere-vm-portid-cport/","section":"post","tags":["vSphere Networking","Cisco ACI"],"title":"vSphere VM - conflicted PortID"},{"body":"","link":"//localhost:1313/tags/hpe-oneview/","section":"tags","tags":null,"title":"HPE OneView"},{"body":"vSphere Lifecycle Manager (vLCM) Some Guidance when troubleshooting Issues with vSphere Lifecycle Manager and HPE OneView for vCenter as a HSM (Hardware Support Manager).\nLog Files The General Log File for vLCM is here:\nLog File: /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log\nTo see error from this log file\ncat /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log | grep \u0026#34;error\u0026#34; 3rd Party Depots and their the vCenter Access to those depot is logged in the following file:\ncat /var/log/vmware/envoy/envoy-access.log | grep \u0026#34;hpe\u0026#34; Common Errors Cannot sync software depots To verify that the online depot registration was successful, navigate to Menu \u0026gt; Lifecycle Manager \u0026gt; Settings \u0026gt; Administration \u0026gt; Patch Setup. The values in the Enabled and Connectivity Status columns should be Yes and Connected respectively. If the Connectivity Status is Not Connected, verify the proper settings for the vCenter proxy configuration and perform a manual sync of the updates.\nAlso get the log file:\ncat /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log If you see this error \u0026quot;A depot is inaccessible or has invalid contents. Make sure an official depot source is used and verify connection to the depot\u0026quot;\n--\u0026gt; \u0026#34;error_type\u0026#34;: \u0026#34;ERROR\u0026#34;, --\u0026gt; \u0026#34;messages\u0026#34;: [ --\u0026gt; { --\u0026gt; \u0026#34;args\u0026#34;: [], --\u0026gt; \u0026#34;default_message\u0026#34;: \u0026#34;A depot is inaccessible or has invalid contents. Make sure an official depot source is used and verify connection to the depot.\u0026#34;, --\u0026gt; \u0026#34;id\u0026#34;: \u0026#34;com.vmware.vcIntegrity.lifecycle.depotContent.ValidationError\u0026#34; --\u0026gt; } --\u0026gt; ] --\u0026gt; }\u0026#34; --\u0026gt; } Check if you can access VMware Online Depots, from vCenter run:\ncurl -vvv https://hostupdate.vmware.com You should see a DigiCert Certificate printed:\n* issuer: C=US; O=DigiCert Inc; CN=DigiCert TLS RSA SHA256 2020 CA1 If not, check with your Firewall Team if they do TLS Intercepts.\nCheck also the connection to the OneView Depot\ncurl -vvv https://oneviewforvcenter.domain.example curl -vvv https://oneviewforvcenter.domain.example:3512 Also check your connected depots. Maybe there is a old depot still configured.\nRunning behind a HTTP(S) Forward Proxy If your Infrastructure needs a forward proxy to access the internet, the following must be done at the vCenter Level.\nvCenter will connect to the Internet (hostupdate.vmware.com) for ESXi updates vCenter wil connect to the HPE OneView for vCenter Depots for all Firmware stuff (SPP) So we need to set the HPE OneView in the NO_Proxy settings of the vCenter:\nvi /etc/sysconfig/proxy # Example: NO_PROXY=\u0026#34;internal.domain, internal-subnet , localhost\u0026#34; NO_PROXY=\u0026#34;localhost, 127.0.0.1, oneviewforvcenter.domain.example, IP of the HPE-OneView\u0026#34; HPE OneView for vCenter with named Certificates Check\ncat /var/log/vmware/envoy/envoy-access.log | grep \u0026#34;hpe\u0026#34; If you see some SSL Errors like:\nfailed: cURL Error: SSL peer certificate or SSH remote key was not OK, SSL certificate problem: certificate has expired Verify if the required certs for HPE OneView for vCenter are valid.\nFurther Troubleshooting Information\n","link":"//localhost:1313/post/vclm-hpe-oneview/","section":"post","tags":["vSphere","HPE OneView"],"title":"vSphere Lifecycle Manager and HPE OneView for vCenter - Troubleshooting"},{"body":"","link":"//localhost:1313/tags/openshift/","section":"tags","tags":null,"title":"OpenShift"},{"body":"Deployment Mode: Installed Provisioned Infrastructure OpenShift Installer-Provisioned Infrastructure (IPI) is a deployment method for OpenShift Container Platform that provides a full-stack automated installation and setup process.\nThe installer manages all aspects of the cluster deployment, including the underlying infrastructure and the operating system itself. IPI creates a bootstrap virtual machine on a provisioner node, which assists in deploying the OpenShift cluster.\nThe most common way to deploy OpenShift is on vSphere. For the vSphere Integration Permissions are needed:\nCreate vSphere Roles and Permissions I created a PowerShell Script to create all the necessary vSphere Roles and Permissions:\nFull script here\nThe script will generate the Roles, assigning the vSphere Roles to the required vSphere Objects is still needed, depending on your setup.\nAlways needed:\nAssing the 'OpenShift_vCenter Role' on the vCenter Object. Assign the 'OpenShift_Datastore Role' to the Datastore OpenShift will run on. Assign the 'OpenShift_PortGroup Role' to the Portgroup OpenShift will run on. Assign the 'OpenShift_VMFolder Role' where your OpenShift VMs will reside Optional:\nAssign the 'OpenShift_Cluster Role' to your vSphere Cluster. If VMs will be created in the cluster root Assign the 'OpenShift_ResourcePool Role' to your vSphere Ressource Pool. If an existing resource pool is provided Assign the 'OpenShift_Datacenter Role' to your Datacenter. If the installation program creates the virtual machine folder Additional Information OpenShift on vSphere Requirements PowerCLI Script for vSphere Role Creation ","link":"//localhost:1313/post/openshift-on-vsphere/","section":"post","tags":["vSphere","OpenShift"],"title":"OpenShift on vSphere - Roles and Permissions"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/esxi-tpm-key-backup/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Esxi Tpm Key Backup"},{"body":"Intro ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It's designed to make application deployment and lifecycle management automated, auditable, and easy to understand.\nKey features and concepts of ArgoCD include:\nGitOps Methodology: Argo CD uses Git repositories as the source of truth for defining the desired application state. Kubernetes-native: It's implemented as a Kubernetes controller that continuously monitors running applications and compares their current state against the desired state specified in Git. This blog post explains the deployment of ArgoCD in a Tanzu Kubernetes Grid Cluster. There is also the option to provision ArgoCD as a supervisor Service.\nOverview ArgoCD typically uses a centralized deployment model with one instance managing multiple Kubernetes clusters. ArgoCD uses a push-based architecture where workloads are pushed from a centralized cluster to remote clusters. Best Practise is to deploy ArgoCD on a dedicated Infrastructure/Management Kubernetes Cluster.\nArgoCD Deployment on TKGS Guest Cluster Create a namespace for ArgoCD\nkubectl create namespace argocd *** if you are in a restricted environment, where you can pull images from the internet ***\nSet up a local container registry within your air-gapped environment (for example Harbor) Mirror all required ArgoCD images to your local registry. docker pull quay.io/argoproj/argocd:v2.13.0 docker pull quay.io/argoproj/argocd-repo-server:v2.13.0 docker pull quay.io/argoproj/argocd-applicationset-controller:v2.13.0 Tag images for your local registry docker tag quay.io/argoproj/argocd:v2.13.0 local-registry.example.com/argoproj/argocd:v2.13.0 docker tag quay.io/argoproj/argocd-repo-server:v2.13.0 local-registry.example.com/argoproj/argocd-repo-server:v2.13.0 docker tag quay.io/argoproj/argocd-applicationset-controller:v2.13.0 local-registry.example.com/argoproj/argocd-applicationset-controller:v2.13.0 Push images to your local registry\ndocker push local-registry.example.com/argoproj/argocd:v2.13.0 docker push local-registry.example.com/argoproj/argocd-repo-server:v2.13.0 docker push local-registry.example.com/argoproj/argocd-applicationset-controller:v2.13.0 Download the ArgoCD installation manifests and modify them to use your local registry:\ncurl -o argocd-install.yaml https://raw.githubusercontent.com/argoproj/argo-cd/v2.13.0/manifests/install.yaml # Update image references in the YAML file sed -i \u0026#39;s|quay.io/argoproj/argocd|local-registry.example.com/argoproj/argocd|g\u0026#39; argocd-install.yaml Apply the Argo CD installation manifest\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Expose ArgoCD via nginx Ingress (Example):\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-server-ingress namespace: argocd annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTPS\u0026#34; spec: rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: name: https tls: - hosts: - argocd.example.com secretName: argocd-secret If you have AKO installed:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-server-ingress namespace: argocd annotations: kubernetes.io/ingress.class: avi ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; ako.vmware.com/ssl-passthrough: \u0026#34;true\u0026#34; spec: rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 443 tls: - hosts: - argocd.example.com secretName: argocd-secret Or you could use a standard Layer 4 LoadBalancer:\nkubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; To get the IP of your LoadBalancer (to create DNS Records):\nexport ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) Get the Initial Admin Password:\nexport ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d)``` Now go ahead on Login to your ArgoCD UI for the first time.\nArgoCD Configuration - TLS Certificates To use your own Certificates for the ArgoCD UI:\nkubectl create -n argocd secret tls argocd-server-tls \\ --cert=/path/to/cert.pem \\ --key=/path/to/key.pem Custom Root CA Edit the argocd-cm ConfigMap:\nkubectl edit configmap argocd-cm -n argocd Add the Root CA certificate to the ConfigMap under the tls.certs key. The format should be:\ndata: tls.certs: | -----BEGIN CERTIFICATE----- \u0026lt;Your Root CA certificate content here\u0026gt; -----END CERTIFICATE----- After updating the ConfigMap, you need to restart the Argo CD server pod for the changes to take effect:\nkubectl rollout restart deployment argocd-server -n argocd ArgoCD Configuration - SSO ArgoCD can use OpenID Connect (OIDC) Providers for SSO-Logins. In this example we use GitLab.\nConfigure GitLab:\nRegister a new application in GitLab:\nGo to Settings \u0026gt; Applications \u0026gt; New Application Set the Redirect URI to: https://argocd-domain.com/auth/callback Enable the scopes: API and read_user Save and note down the Application ID and Secret kubectl edit configmap argocd-cm -n argocd data: url: https://argocd.example.com dex.config: | connectors: - type: gitlab id: gitlab name: GitLab config: baseURL: https://gitlab.com clientID: $GITLAB_APPLICATION_ID clientSecret: $GITLAB_CLIENT_SECRET redirectURI: https://argocd.example.com/api/dex/callback users.anonymous.enabled: \u0026#34;false\u0026#34; Restart the deployment:\nkubectl rollout restart deployment argocd-server -n argocd Now a new Login Button appears on the ArgoCD UI.\nArgoCD Configuration - Forward Proxy If you run ArgoCD behind a Forward Proxy and want to use external Git-Repositories, adjust your deployment the following:\nkubectl edit configmap argocd-cm -n argocd data: HTTP_PROXY: \u0026#34;http://your-proxy-server:port\u0026#34; HTTPS_PROXY: \u0026#34;http://your-proxy-server:port\u0026#34; NO_PROXY: | argocd-repo-server, argocd-application-controller, argocd-applicationset-controller, argocd-metrics, argocd-server, argocd-server-metrics, argocd-redis, argocd-redis-ha-haproxy, argocd-dex-server, localhost, 127.0.0.1, kubernetes.default.svc, .svc.cluster.local, 10.0.0.0/8 - use your own internal CIDR here! ArgoCD Complete Config: Your whole ConfigMap (argocd-cm) should now look like:\napiVersion: v1 kind: ConfigMap metadata: name: argocd-cm namespace: argocd labels: app.kubernetes.io/name: argocd-cm app.kubernetes.io/part-of: argocd data: url: https://argocd.example.com dex.config: | connectors: - type: gitlab id: gitlab name: GitLab config: baseURL: https://gitlab.com clientID: $GITLAB_APPLICATION_ID clientSecret: $GITLAB_CLIENT_SECRET redirectURI: https://argocd.example.com/api/dex/callback users.anonymous.enabled: \u0026#34;false\u0026#34; HTTP_PROXY: \u0026#34;http://your-proxy-server:port\u0026#34; HTTPS_PROXY: \u0026#34;http://your-proxy-server:port\u0026#34; NO_PROXY: | argocd-repo-server, argocd-application-controller, argocd-applicationset-controller, argocd-metrics, argocd-server, argocd-server-metrics, argocd-redis, argocd-redis-ha-haproxy, argocd-dex-server, localhost, 127.0.0.1, kubernetes.default.svc, .svc.cluster.local, 10.0.0.0/8 tls.certs: | -----BEGIN CERTIFICATE----- \u0026lt;Your Root CA certificate content here\u0026gt; -----END CERTIFICATE----- ","link":"//localhost:1313/post/argocd-configuration/","section":"post","tags":["Kubernetes","CI/CD","VMware Tanzu","VMware Tanzu Kubernetes Grid"],"title":"ArgoCD Deployment \u0026 Configuration"},{"body":"","link":"//localhost:1313/tags/ci/cd/","section":"tags","tags":null,"title":"CI/CD"},{"body":"","link":"//localhost:1313/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"","link":"//localhost:1313/tags/vmware-tanzu/","section":"tags","tags":null,"title":"VMware Tanzu"},{"body":"","link":"//localhost:1313/tags/vmware-tanzu-kubernetes-grid/","section":"tags","tags":null,"title":"VMware Tanzu Kubernetes Grid"},{"body":"","link":"//localhost:1313/tags/vcenter/","section":"tags","tags":null,"title":"VCenter"},{"body":"","link":"//localhost:1313/tags/vcf/","section":"tags","tags":null,"title":"VCF"},{"body":"vCenter Troubleshooting Guide The following Troubleshooting Guide applies to vCenter 8 and above.\nCaution! Create a VM Snapshot of your vCenter before proceeding with any step!\nGeneral Troubleshooting Log files can be accesses trough ssh or bash. SSH login works with the root user or your SSO Admi User (commonly administrator@vsphere.local)\nSSH not working\nIf SSH does not work (eg. due to a networki misconfiguration) you can access the vCenter via bash (VM Remote Console).\nAt the VM Remote Console press F2 to access your vCenter or press ALT + F1 to access the bash shell.\nServices If you have trouble starting services and the GUI is not working, use service-control with ssh to get all the running services:\nservice-control --status To stop all services\nservice-control --stop --all To start all services:\nservice-control --start --all Log Files All vCenter Log Files are stored under:\n/var/log/vmware/ /var/log/vmware/\u0026lt;service_name\u0026gt; vCenter logs are grouped by component and purpose:\nvpxd.log - Main vCenter Server log for client connections, tasks, and host communication5 vpxd-profiler.log - Profiled metrics for vCenter operations eam.log - ESX Agent Manager logs sms.log - Storage Monitoring Service logs ls.log - Licensing Services logs Some key log files and directories include:\n/var/log/vmware/vpxd/vpxd.log - The main vCenter Server log /var/log/vmware/vsphere-ui/ - vSphere UI logs /var/log/vmware/eam/eam.log - ESX Agent Manager log /var/log/vmware/eam/applmgmt.log - Appliance Management Service Additional Important Logs\n/var/log/vmware/vpostgres/ - VMware Postgres service logs /var/log/vmware/vcha/ - vCenter High Availability service logs /var/log/vmware/rhttpproxy/ - VMware HTTP Reverse Proxy service logs /var/log/vmware/content-library/ - VMware Content Library Service logs /var/log/vmware/applmgmt/backup.log - VCSA Backup Log GUI\nvCenter Server logs can be viewed from:\nHome \u0026gt; Administration \u0026gt; System Logs. Ways to access your vcenter GUI (VAMI)\n'https:// vcenterfqdn :5480'\nSSH\nLogin via SSH (root user or SSO User)\nDCUI\nOpen the VM Remote Console of the vCenter VM (via ESXi) and Press F2. After a successfull login you will be able to edit the config via DCUI.\nDCUI can also be opened from ssh.\nBash shell\nTo Access the Bash Shell: Open the VM Remote Console of the vCenter VM (via ESXi) and Press ALT+F1.\nIf you need to enable bash shell, do the following first:\nOpen the VM Remote Console of the vCenter VM (via ESXi) and Press F2. After a successfull login go to troubleshooting options and enable bash or ssh Bash shell (Emergency Boot)\nLook at here\nNetworking Manual Network Config Run the following command to change Networking Settings from BASH\n/opt/vmware/share/vami/vami_config_net Alternatively edit the configuration files directly, to change network settings like IP address and netmask\n/etc/systemd/network/10-eth0.network Example:\n[Match] Name=eth0 [Network] Address=192.168.1.100/24 Gateway=192.168.1.1 DNS=8.8.8.8 DNS=8.8.4.4 [Route] Destination=10.0.0.0/24 Gateway=192.168.1.254 to modify routing information\n/etc/sysconfig/network/routes Example\ndefault 192.168.1.1 10.0.0.0/24 192.168.1.254 172.16.0.0/16 192.168.1.253 Each line follows this format: ' '\nor use the builtin DCUI from the bash shell\ndcui DNS Manual DNS Config\nvi etc/systemd/resolved.conf [Resolve] DNS=10.10.10.125 FallbackDNS=10.10.10.126 Domains=yourdomain.local or use the builtin DCUI from the bash shell\ndcui Flush DNS\nsystemctlÂ restartÂ systemdâ€“resolved.service systemctl restart dnsmasq HTTP Proxy Log File: /var/log/vmware/rhttpproxy/\nTo set a HTTP Proxy run:\n/opt/vmware/share/vami/vami_config_net It is also possible to edit the file directly:\nvi /etc/sysconfig/proxy Example:\nPROXY_ENABLED=\u0026#34;yes\u0026#34; HTTP_PROXY=\u0026#34;http://proxy.example.com:8080\u0026#34; HTTPS_PROXY=\u0026#34;http://proxy.example.com:8080\u0026#34; NO_PROXY=\u0026#34;localhost, 127.0.0.1, .*.example.com, 192.168.0.0/24, 192.168.1.55\u0026#34; Certificate Management Log File: /var/log/vmware/vmcad/certificate-manager.log\nRun this command to check certificate expiration dates:\nfor store in $(/usr/lib/vmware-vmafd/bin/vecs-cli store list | grep -v TRUSTED_ROOT_CRLS); do echo \u0026#34;[*] Store :\u0026#34; $store /usr/lib/vmware-vmafd/bin/vecs-cli entry list --store $store --text | grep -ie \u0026#34;Alias\u0026#34; -ie \u0026#34;Not After\u0026#34; done Renew all vCenter Machine Certificates Generally this should be done of all VMCA Certificates are self-signed and are expired. Later you can change the certificates to named ones from your Enterprise PKI.\nRun vCenter Certificate Manager:\n/usr/lib/vmware-vmca/bin/certificate-manager Choose your desired option. Most commonly option 4 or 8 are used.\nThis step automatically restarts the vCenter Server services. Additionally, the Name, Hostname, and VMCA values should match the Primary Network Identifier (PNID). The PNID should always match the Hostname.\nTo get the vCenter PNID:\n/usr/lib/vmware-vmafd/bin/vmafd-cli get-pnid --server-name localhost VMware Security Token Service (STS) certificate https://knowledge.broadcom.com/external/article?legacyId=79248\nFix trust issues - lsdoctor Use the lsdoctor tool from VMware: https://knowledge.broadcom.com/external/article?legacyId=80469\nUpload and unzip lsdoctor on vCenter\nCheck for issues\npython lsdoctor.py -l to fix trust mismatches\npython lsdoctor.py -t vSphere HA Log File: /var/log/fdm.log\nvSphere Lifecycle Manager (vLCM) Log File: /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log\ncat /var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log | grep \u0026#34;error\u0026#34; Error cannot sync depot\nA usual error is that vCenter is unable to reach vmwaredepot. Check DNS, Firewall and HTTP Proxy Settings. The log file indicated it the following:\n--\u0026gt; \u0026#34;error_type\u0026#34;: \u0026#34;ERROR\u0026#34;, --\u0026gt; \u0026#34;messages\u0026#34;: [ --\u0026gt; { --\u0026gt; \u0026#34;args\u0026#34;: [], --\u0026gt; \u0026#34;default_message\u0026#34;: \u0026#34;A depot is inaccessible or has invalid contents. Make sure an official depot source is used and verify connection to the depot.\u0026#34;, --\u0026gt; \u0026#34;id\u0026#34;: \u0026#34;com.vmware.vcIntegrity.lifecycle.depotContent.ValidationError\u0026#34; --\u0026gt; } --\u0026gt; ] --\u0026gt; }\u0026#34; --\u0026gt; } Check if you can access VMware Online Depots, from vCenter run:\ncurl -vvv https://hostupdate.vmware.com You should see a DigiCert Certificate printed:\n* issuer: C=US; O=DigiCert Inc; CN=DigiCert TLS RSA SHA256 2020 CA1 If not, check with your Firewall Team if they do TLS Intercepts.\nAlso check your connected depots. Maybe there is a old depot still configured (like a old HPE OneView Instance).\nHPE OneView for vCenter\nCheck\ncat /var/log/vmware/envoy/envoy-access.log | grep \u0026#34;hpe\u0026#34; If you see some SSL Errors like:\nfailed: cURL Error: SSL peer certificate or SSH remote key was not OK, SSL certificate problem: certificate has expired Verify if the required certs for HPE OneView for vCenter are valid.\nFurther Troubleshooting Information\nBackup and Restore Backup Log files:\n/var/log/vmware/applmgmt/backup.log /var/log/vmware/applmgmt/backupscheduler.log /var/log/vmware/applmgmt/backupschedulercron.log vCenter Appliance File-Restore See: VMware Docs\nIf during a File Level Restore something went run, check your logs at:\n/var/log/vmware/applmgmt/restore.log\nRestore fails - unable to ssh into machine If your vCenter Restore has failed and you aren't able to ssh into the machine. Try all the ways to access your vCenter as described in here\nvCenter and ESXi Disconnect ESXi if vCener no longer exists\ncmsso-util unregister --node-pnid vcenter.domain.com --usernameÂ [administrator@vsphere.local](mailto:administrator@vsphere.local)Â --passwd pw Forgot Root Password Simple Method: Login with SSO User If you forgot your root password or it is expired you can still access the vCenter via GUI or SSH with the SSO Admin User (usually administrator@vsphere.local)\nsudo passwd root Advanced root password restore Emergency boot the vCenter described below unlock the root account /usr/sbin/faillock --user root --reset After that you can set a new password\npasswd Then unmount the filesystem and reboot\numount / reboot -f Emergency Boot Edit GRUB Bootloader If you cannot SSH into or open DCUI/Bash Shell (via VM Remote Console) to your vCenter you can access the vCenter without a password the following:\nAccess VM Remote Console of the vCenter VM Reboot the VM press 'e' for emergency mode GRUB Bootloader will appear, edit the boot loader the following: Append these entries to the end of the line of '...consoleblank=0' with: - 'rw init=/bin/bash' Proceed to boot the vCenter VM with pressing F10 After booting you should be presented with a shell run: mount -o remount,rw / After that you can edit configuartion files as needed (in /etc/sysconfig for example)\nMake sure you unmount the filesystem and reboot after you have done your configuration\numount / reboot -f ","link":"//localhost:1313/post/vcenter-troubleshooting/","section":"post","tags":["vCenter","VCF"],"title":"VMware vCenter Troubleshooting Guide"},{"body":"","link":"//localhost:1313/tags/vmware/","section":"tags","tags":null,"title":"VMware"},{"body":"","link":"//localhost:1313/tags/vmware-cloud-foundation/","section":"tags","tags":null,"title":"VMware Cloud Foundation"},{"body":"The new VCF 5.2 Update delivers quite a few interesting updates. This blog post covers expecially the updates and improvements for Tanzu.\nPart 1 covers new Feature within Tanzu on vSphere 8 Update 3.\nvSphere with Tanzu Update -\u0026gt; vSphere IaaS Control Plane vSphere with Tanzu is passÃ© - the new naming is vSphere IaaS Control Plane.\nThe naming already suggest - not only Kubernetes Cluster can be deployed, also VMs (this was possible a long time but with the new naming the focus is not only on k8s). Via the vm-operator you can deploy VMs alongside Tanzu Kubernetes Cluster. The IaaS Control Plane is really interesting as you can deploy VMs with Code. A YAML File will describe your VM. For that the VM Class now support much more configuration, it handled like a normal VM provisioned through vSphere UI.\nIt is even possible to deploy Windows Workloads via VM-Operator with sysprep! (blog to follow)\nDeployment of Windows Server within a ArgoCD Pipeline is possible (if you would want that ;))\nNow also the Backup via vSphere Storage APIs for Data Protection (VADP) for VMs deployed via vm-operator are supportet (as \u0026quot;normal\u0026quot; VMs) - within Veeam you can just backup the whole vSphere Namespace (with is basically a advanced vSphere Ressource Pool)\nLCI - Local consumption Interface With the new Local Consumption Interface within vSphere UI you can deploy Tanzu Kubernetes Clusters or VMs - both via IaaS Control Plane with a GUI!\nSurely some of you already knew the Cloud Consumption Interface (CCI) within Aria - Local Cloud Consumption Interface is a built in UI within vSphere UI (no need to deploy Aria)\nLCI Installation LCI is a supervisor Service. Supervisor Services are deployed directly to the Supervisor instead of deploying to a TKGS Cluster.\nhttps://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-with-tanzu-services-workloads/GUID-4843E6C6-747E-43B1-AC55-8F02299CC10E.html https://vsphere-tmm.github.io/Supervisor-Services/#consumption-interface Download Link of LCI YAML: https://vmwaresaas.jfrog.io/ui/native/supervisor-services/cci-supervisor-service/v1.0.0/cci-supervisor-service.yml\nvSAN streched Cluster support for Tanzu Kubernetes Grid Service (TKGs) With vSphere 8 Update 3 TKGS now supports vSAN streched Clusters. Interesting is that Kubernetes Control Plane Nodes and the Supervisor Control Plane Nodes should be kept at one site within a vSAN streched Clusters. This is due to etcd. etcd requires more than half of the replicas to be available at any time.\nThe 3 Supervisor Control Plane VMs should be placed in the same site All the Control Plane VMs of any given TKGs cluster should be placed in the same site Worker Nodes can be streched acroos two sites Tanzu Kubernetes Cluster Autoscaling As the name already suggest: Autoscaling of Tanzu Kubernetes Workers Nodes (VMs)! Only requirements is to have TKR Relase 1.25\nLinks [1] https://blogs.vmware.com/cloud-foundation/2024/06/25/vmware-cloud-foundation-launch/\n[2] https://core.vmware.com/resource/whats-new-vsphere-update-3-vsphere-iaas-control-plane\n[3] https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vmware-vsphere-with-tanzu-80-release-notes/index.html\n","link":"//localhost:1313/post/vcf52/","section":"post","tags":["VMware","VMware Cloud Foundation","VMware Tanzu"],"title":"VMware Cloud Foundation 5.2"},{"body":"Introduction Due to the rise of containerized applications the new word DevOps or DevSecOps.\nSo what is DevOps?\nBasically the developer should also build \u0026amp; operate the infrastructure on which is code runs on.\nAnd DevSecOps?\nWe need Security, so adapt DevOps Practises also for Security. Devs should also be resonsible to securing their infrastructure.\nWhat are the goals?\nThe main Goal for DevOps Practises is to shorten time of the SDLC (Systems Develoopment Lifecyle). Basically be faster to ship code into production.\nProblems DevSecOps is not a person nor a role DevSecOps covers a whole lot of topics. Tooo much to handle for a single person. DevOps are practises.\nI've noticed a theme with security teams: they take a while to adapt. In the days before the Internet, security was not a priority because it was never a serious concern. These teams were established only when internet adoption began to spread, and they were incorporated into the early waterfall processes because it was how infrastructure teams and development teams collaborated.\nDevelopers aren't Security or Infrastructure Experts One of\n","link":"//localhost:1313/post/devsecops/","section":"post","tags":["DevSecOps"],"title":"DevSecOps"},{"body":"","link":"//localhost:1313/tags/devsecops/","section":"tags","tags":null,"title":"DevSecOps"},{"body":"","link":"//localhost:1313/categories/my-2-cents/","section":"categories","tags":null,"title":"My 2 Cents"},{"body":"","link":"//localhost:1313/tags/automation/","section":"tags","tags":null,"title":"Automation"},{"body":"Getting started with the vSphere API The vSphere REST API, introduced in vSphere 6.5, represents a significant leap forward in managing and automating VMware environments. This modern, developer-friendly interface offers a more streamlined approach to interacting with vSphere compared to its predecessor, the SOAP-based vSphere Web Services API.\nKey features of the vSphere REST API include:\nRESTful architecture: It follows REST principles, using standard HTTP methods (GET, POST, PUT, DELETE) for operations. JSON-based: Requests and responses use JSON format, making it easier to parse and work with data. Simplified interaction: The API is designed to be more intuitive and easier to use than the older SOAP-based API. To get started with the vSphere REST API, you can use the built-in API Explorer in vCenter Server, accessible at https://\u0026lt; vcenter.example.com \u0026gt;/apiexplorer. This tool provides interactive documentation and allows you to test API calls directly from your browser.\nEndpoints The vSphere API provides several key endpoints for interacting with different aspects of the vSphere environment:\nAPI Endpoint vSphere Automation API (REST) https://\u0026lt; vcenter.example.com \u0026gt;/api VIM JSON API (New in vSphere 8.0 Update 1) http://\u0026lt; vcenter.example.com \u0026gt;/sdk/vim25/8.0.1.0 vSphere Web Services API (SOAP) - deprecated https://\u0026lt; vcenter.example.com \u0026gt;/sdk vCenter Server Appliance Management API https://\u0026lt; vcenter.example.com \u0026gt;:5480/rest API Explorer: https://\u0026lt; vcenter.example.com \u0026gt;/apiexplorer vSphere Developer Center Within your vCenter there a some tools to help you interacting with the vSphere API provided by your vCenter. So you only use API Call that are actually available for your infrastructure.\nCode Capture With Code Capature you can capture a manual configuration and vSphere will give you the code in various languages for that configuration.\nActivate Code Capture\nFrom the burger menu, click Developer Center and go to the Code Capture tab. Now a Red Recording Buttons appears on the top right. To start a recording, navigate to your desired pane and click the red record button in the top pane. To start recording immediately, click Start Recording. While a recording is in progress, the red record button in the top pane blinks.\nWhile the recording is in progress, do your things in the GUI (like edit a VM, add a second Disk). After your manual configuration has been made, stop the recording. Now you will be presented with a sample Code.\nAPI Explorer To explore the vSphere APIs, you can access the API Explorer directly in your vCenter. From the burger menu, click Developer Center and select the API Explorer tab.\nOr Directly via: https://\u0026lt; vcenter.example.com \u0026gt;/apiexplorer\nAnsible Ansible can interact with vSphere via Module (mostly the Community.Vmware' module is used). But not all Automation Tasks can be done via this module. Sometimes direct API Calls via the 'uri' module have to be made for certain tasks.\nExample Ansible vSphere REST API Call - name: Basic vSphere API REST Call hosts: localhost gather_facts: no vars: vcenter_hostname: \u0026#34;vcenter.example.com\u0026#34; vcenter_username: \u0026#34;administrator@vsphere.local\u0026#34; vcenter_password: \u0026#34;your_password_here\u0026#34; tasks: - name: Get vCenter session token uri: url: \u0026#34;https://{{ vcenter_hostname }}/rest/com/vmware/cis/session\u0026#34; method: POST validate_certs: no force_basic_auth: yes user: \u0026#34;{{ vcenter_username }}\u0026#34; password: \u0026#34;{{ vcenter_password }}\u0026#34; register: login_result vSphere managedObject ID The Managed Object ID (MOID), also known as the Managed Object Reference ID (MORef ID), is a VMware internal identifier that is generated by vSphere when new objects like VMs are created, or when ESXi hosts are added to vCenter. MOIDs are used to uniquely identify VMware components and are used by all VMware solutions to reference objects within vCenter.\nMOIDs typically consist of a prefix indicating the object type, followed by a number. For example, \u0026quot;vm-1024\u0026quot; for a virtual machine or \u0026quot;datacenter-1001\u0026quot; for a datacenter\nA lot of times you will need to interact with the MOIDs. You can browse them in your vCenter:\nhttps://\u0026lt; vcenter.example.com \u0026gt;/mob To get those MOIDs here is an example:\n--- - name: Gather ESXi Host Info using vSphere API hosts: localhost gather_facts: no vars: vcenter_hostname: \u0026#34;vcenter.example.com\u0026#34; vcenter_username: \u0026#34;administrator@vsphere.local\u0026#34; vcenter_password: \u0026#34;your_password_here\u0026#34; cluster_name: \u0026#34;your_cluster_name\u0026#34; tasks: - name: Get vCenter session token uri: url: \u0026#34;https://{{ vcenter_hostname }}/rest/com/vmware/cis/session\u0026#34; method: POST validate_certs: no force_basic_auth: yes user: \u0026#34;{{ vcenter_username }}\u0026#34; password: \u0026#34;{{ vcenter_password }}\u0026#34; register: login_result - name: Get cluster MoID uri: url: \u0026#34;https://{{ vcenter_hostname }}/rest/vcenter/cluster\u0026#34; method: GET validate_certs: no headers: vmware-api-session-id: \u0026#34;{{ login_result.json.value }}\u0026#34; register: clusters_result - name: Set cluster MoID set_fact: cluster_moid: \u0026#34;{{ clusters_result.json.value | selectattr(\u0026#39;name\u0026#39;, \u0026#39;equalto\u0026#39;, cluster_name) | map(attribute=\u0026#39;cluster\u0026#39;) | first }}\u0026#34; - name: Get ESXi hosts in cluster uri: url: \u0026#34;https://{{ vcenter_hostname }}/rest/vcenter/host?filter.clusters={{ cluster_moid }}\u0026#34; method: GET validate_certs: no headers: vmware-api-session-id: \u0026#34;{{ login_result.json.value }}\u0026#34; register: hosts_result - name: Display ESXi host information debug: msg: \u0026#34;Host Name: {{ item.name }}, Host MoID: {{ item.host }}\u0026#34; loop: \u0026#34;{{ hosts_result.json.value }}\u0026#34; - name: Get detailed host info uri: url: \u0026#34;https://{{ vcenter_hostname }}/rest/vcenter/host/{{ item.host }}\u0026#34; method: GET validate_certs: no headers: vmware-api-session-id: \u0026#34;{{ login_result.json.value }}\u0026#34; register: host_details loop: \u0026#34;{{ hosts_result.json.value }}\u0026#34; - name: Display detailed host information debug: msg: \u0026#34;Host: {{ item.json.value.name }}, Connection State: {{ item.json.value.connection_state }}, Power State: {{ item.json.value.power_state }}\u0026#34; loop: \u0026#34;{{ host_details.results }}\u0026#34; This approach using the uri module gives you more direct control over the API calls and allows you to customize the requests and responses as needed. It's particularly useful when you need to perform actions that aren't covered by existing Ansible modules or when you want to work directly with the vSphere REST API.\nPowershell If PowerCLI does not have the function you are looking for, you could also you native Powershell to interact with the vSphere API.\nThis script does the following:\nSets up the vCenter server details. Adds a function to ignore SSL certificate errors (remove this in production environments). Authenticates with the vCenter server to get a session token. Uses the session token to make an API call to retrieve a list of VMs. Displays the results in a table format. # vCenter server details $vcServer = \u0026#34;vcenter.example.com\u0026#34; $vcUser = \u0026#34;administrator@vsphere.local\u0026#34; $vcPass = \u0026#34;YourPassword\u0026#34; # Ignore SSL certificate errors (remove in production) if (-not ([System.Management.Automation.PSTypeName]\u0026#39;ServerCertificateValidationCallback\u0026#39;).Type) { add-type @\u0026#34; using System.Net; using System.Security.Cryptography.X509Certificates; public class ServerCertificateValidationCallback { public static void Ignore() { ServicePointManager.ServerCertificateValidationCallback += delegate ( Object obj, X509Certificate certificate, X509Chain chain, SslPolicyErrors errors ) { return true; }; } } \u0026#34;@ } [ServerCertificateValidationCallback]::Ignore() # Authenticate and get session token $auth = [System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($vcUser + \u0026#39;:\u0026#39; + $vcPass)) $headers = @{ \u0026#39;Authorization\u0026#39; = \u0026#34;Basic $auth\u0026#34; } $sessionUrl = \u0026#34;https://$vcServer/rest/com/vmware/cis/session\u0026#34; $response = Invoke-RestMethod -Uri $sessionUrl -Method Post -Headers $headers $sessionId = $response.value # Set up headers for subsequent requests $headers = @{ \u0026#39;vmware-api-session-id\u0026#39; = $sessionId } # Make an API call (e.g., get list of VMs) $vmsUrl = \u0026#34;https://$vcServer/rest/vcenter/vm\u0026#34; $vms = Invoke-RestMethod -Uri $vmsUrl -Method Get -Headers $headers # Display results $vms.value | Format-Table ","link":"//localhost:1313/post/vsphere-api/","section":"post","tags":["VMware","vSphere","Automation"],"title":"vSphere REST API - Intro"},{"body":"Intro With vCenter File-level Backups you can restore your entire vCenter configuration. If your running vCenter has some misconfigurations (eg. a false proxy setting) and isn't working, you can use existing file-level backups and clear those misconfigurations before restoring.\nBasic Process The Basic vCenter File-level Restore Process looks like the following:\nDeploy a fresh vCenter via ISO (VCSA UI Installer) Shutdown the defunct vCenter (IP Conflicts) Same Network Configuration Same build number Same Size (Tiny, Small, Medium Large) When Stage 1 is completed you can start the Stage 2 Process to Restore your vCenter Configuration In case of a misconfiguration (which has also been backed up) you have the option to edit all configurations files Restore vCenter with configurations edits Stage 1 - Deploying a fresh (blank) appliance Before deploying a fresh vCenter, have a look at you VCSA Backup folder. In the root of each backupfolder there is a file 'backup-metadata.json' - within there grab the details before deploying a fresh VCSA:\n\u0026quot;Build\u0026quot; (Use the exact same buildnumber) After the successful deployment you with be presented with the Stage 2 Process. ('https://vcenterfqdn:5480/stage2')\nStage 1.5 - Clear misconfigurations Grab the whole backupfolder from your desired date and safe it somewhere. As a better unterstanding of the folder Naming convention:\n[M/S]_[VCSA version]_[Date YYYYMMDD]_[Time HHMMSS]_[Unique identifier] M: Manual backup / S: Scheduled backup\nfile structure:\n[SFTP/FTP/HTTP Server] â””â”€â”€ vCenter â””â”€â”€ [VCSA FQDN] â””â”€â”€ [M/S]_[VCSA version]_[Date YYYYMMDD]_[Time HHMMSS]_[Unique identifier] â”œâ”€â”€ backup.mf â”œâ”€â”€ backup_manifest.json â”œâ”€â”€ VADump.vad â”œâ”€â”€ config_files â””â”€â”€ [Additional backup files] It's important to note that the specific location of the backup files depends on the protocol and location you've configured for your VCSA backups, which can include FTPS, HTTPS, SFTP, FTP, NFS, SMB, or HTTP.\nEdit Configurations Files The 'config_files' holds all the configuration files. You can safely edit those files \u0026quot;offline\u0026quot; from your workstation.\nExample - Edit HTTP Proxy Unpack the archive:\ntar -xvzf config_files.tar.gz Edit the Proxy File:\nvim /config_files/etc/sysconfig/proxy Make your adjustments as needed. This is just an example. Within the config Files you could also edit Network Configs etc.\nThen create the archive and gzip it:\ntar -czvf archive_name.tar.gz config_files Now upload the newly created archive to your SFTP/FTP/HTTP Server which the freshly deployed VCSA can access the folder.\n###Â Encrypted Backups\nIf you enabled encryption in the file-level backups, those can be decrypted with openssl.\nExample: Config File Folder 'config_files.tar.gz.enc'\nDecrypt the archive (provide the configured encryption key/passphrase as configured in the vcsa backup)\nopenssl aes-256-cbc -a -salt -pbkdf2 -in config_files.tar.gz.enc -out config_files.tar.gz Unpack the archive as described above, make adjusments.\nThen encrypt the files again:\nopenssl aes-256-cbc -a -salt -pbkdf2 -in config_files.tar.gz -out config_files.tar.gz.enc Stage 2 - File-level Restore Now go back to 'https://vcenterfqdn:5480/stage2' and Start the Stage 2 Process. Provide the folder with the new edits.\nTipp: If you are unable to change the folder (Server, Location etc) at stage 2 - open 'https://vcenterfqdn:5480/stage2' in a private/incognito Windows in your Browser. Then you should be able to edit Stage 2 Restore Parameters.\n","link":"//localhost:1313/post/vcenter-file-level-restore/","section":"post","tags":["VMware","vCenter"],"title":"vCenter File Level Restore - Edit Configuration"},{"body":"Tanzu Portfolio explained Lately, we've been having lots conversations internally and with customers about Tanzu. There appears to be some confusion about Tanzu's portfolio and which Tanzu products are best suited for specific use cases.\nUpdate November 2024 With the VMware aquisition by Broadcom, the are some changes in the Naming and Product Bundles.\nThe new naming for vSphere with Tanzu is IaaS Control Plane. The Kubernetes Runtime itself is part of VMware Cloud Foundation (VCF). Add-on's, Management Tools are now part of the Tanzu Business Unit (BU). Tanzu Products include:\nTanzu Platform Tanzu Platform for Kubernetes Tanzu Platform for Cloud Foundry Tanzu Application Catalog Tanzu Spring Tanzu Salt Tanzu Data Services Tanzu RabbitMQ Tanzu for MySQL Postgres Redis If you look for Tanzu Application Platform, Tanzu Mission Control etc. Their are now Standalone Components:\nTanzu Mission Control Tanzu Kubernetes Grid (TKG) Tanzu Kubernetes Grid Integrated Edition (TKGI) Tanzu Application Platform Tanzu Build Service Tanzu Buildpacks Cluster Essentials for Tanzu Application Configuration Service for Tanzu Additional Information\nAnnouncement Tanzu Platform Docs Tanzu Docs IaaS Control Plane and vSphere Kubernetes Service (VKS) New Naming:\nvSphere with Tanzu = IaaS Control Plane Tanzu Kubernetes Grid Cluster (TKGS) = vSphere Kubernetes Service vSphere clusters enabled with vSphere IaaS control plane are called Supervisors. Supervisor enable you to deploy Virtual Machines and Kubernetes Clusters (via vSphere Kubernetes Service). Both are managed by a Kubernetes API.\nAdditional Information:\nVMware Docs Tanzu Platform for Kubernetes Tanzu Platform for Kubernetes is a SaaS Solution that provides lots of functionality. The Main Goal of Tanzu Platform for Kubernetes is to give the Developers a really easy experience. They should write Software, not Kubernetes Manifests.\nAdditional Information:\nVMware Docs Intro Video Tanzu Platform hub Older Information:\nKubernetes Runtimes To actually run Kubernetes Clusters with Tanzu, the following Products are available:\nTanzu Kubernetes Grid vSphere with Tanzu Tanzu Kubernetes Grid (TKG) TKG is a solution for routing Kubernetes clusters to different cloud providers. First, a management cluster (based on Docker and kind) is created from a Bootstrap Workstation, which itself then provides its web interface and additional CLI tools to create a Kubernetes cluster.\nThe following providers are supported:\nvSphere AWS Azure The management cluster will provision the VMs on the target and install \u0026amp; configure kubernetes within those VMs.\nWhen vSphere is used as a Target there is no integration of TKG with vSphere UI. Just a bunch of VMs will be provisioned.\nIf you want to deploy your Kubernetes Cluster and Integration with vCenter and other vSphere Services. Look at vSphere with Tanzu. It has also been built with the vSphere Administrator in mind.\nIf you want to deploy your Kubernetes Cluster to Azure or Amazon Webservices, use Tanzu Kubernetes Grid.\nTKGm TKGm is the multi-cloud version of TKG. It can be deployed ontop of VMware Cloud on AWS (VMConAWS) and other VMware Hyperscaler Solutions.\nTKGI TKG Integrated edition has deep integration with NSX-T. It uses BOSH and Ops Manager to manage operations. It was an offering mainly for Telcos at Pivotal (Enterprise PKS). Although Integrated in the name, it's not as tighly integrated was vSphere with Tanzu.\nvSphere with Tanzu aka Workload Management In newer version of vCenter there is the option of \u0026quot;Workload Management\u0026quot; within the vSphere Burger Menu. Workload Management is the same as vSphere with Tanzu. With vSphere with Tanzu obviously only works with vSphere. Kubernetes Ressources are provisioned on a vSphere Cluster.\nThe first step is to create a Supervisor Cluster, which consists of three VMs in vSphere. Kubernetes itself is then provisioned via the TKG Service running on the Supervisor Cluster.\nIf the supervisor cluster then is created, additional vSphere Namespaces need to be provisioned (vSphere Namespace isn't the same as a Kubernetes Namespace!) Within that Namespace then a Tanzu Kubernetes Cluster can be created with the TKG Service. The Tanzu Kubernetes Cluster (TKC) are upstream aligned, fully conformant Kubernetes Cluster. It is also possible to provision VMs withing a vSphere Namespace. The Idea is, that a vSphere Administrator isolates Workloads through vSphere Namespaces that then can be used by DevOps/Platform Operators to provision their Kubernetes Clusters.\nUser workloads then are running inside a Tanzu Kubernetes Cluster.\nvSphere with Tanzu minimum Ressources\nvSphere Cluster with DRS 3 Supervisor VMs NSX-T or vSphere Distributed Switches vDS must use NSX-ALB or HA-Proxy as an external Load Balancer - NSX-ALB is recommended. Tanzu Mission Control Tanzu Mission Control is a Tool to operate your Tanzu Kubernetes Clusters. Besides Tanzu Kubernetes Cluster it can also managed Azure Kubernetes Service (AKS) and Elasic Kubernetes Service (EKS) from Amazon. TMC was a SaaS only Service but now also always a self-hosted version with some limitations .\nTMC self-hosted Requirements\nvSphere with Tanzu Supervisor Services: Contour Harbor Image Registry DevSecOps Tools Where in my opinion Tanzu really shines are the tools for day 2 operations and the whole DevSecOps lifecycle.\nTanzu Application Platform (TAP) VMware Tanzu Application Platform (TAP) is a comprehensive application development platform designed to streamline the building, deployment, and management of applications on Kubernetes. It provides developers with a paved path to production, enabling them to rapidly develop software while ensuring security and compliance across various environments, whether on public clouds or on-premises Kubernetes clusters\nTAP comprises several key components that enhance its functionality:\nTanzu Build Service (TBS): Automates the creation of production-ready container images using Cloud Native Buildpacks, ensuring efficient image building and deployment.\nApplication Live View: A monitoring tool that provides insights into running applications, assisting developers and operators in troubleshooting issues effectively.\nAPI Portal: Facilitates API discovery for consumers, enabling easier integration of services within applications.\nTanzu Application Platform GUI: Built on the Backstage project, this graphical interface allows developers to view dependencies and manage applications centrally\nTanzu Application Catalog Tanzu Application Catalog came from the aquisition of Bitnami. TAC is the enterprise version of Bitnami Application Catalog. Bitnami was very well known for their prepackaded virtual machine Images (Drupal, Wordpress, Gitlab to name a few). Bitnami also provided prepackaked Applications on docker or kubernetes.\nLook at the VMware Marketplace for Solutions: https://marketplace.cloud.vmware.com/\n","link":"//localhost:1313/post/tanzu-overview/","section":"post","tags":["VMware","VMware Tanzu","Kubernetes"],"title":"VMware Tanzu - Overview"},{"body":"","link":"//localhost:1313/tags/hardware/","section":"tags","tags":null,"title":"Hardware"},{"body":"","link":"//localhost:1313/tags/hpe/","section":"tags","tags":null,"title":"HPE"},{"body":"Problem\nRecently I got my hands on some old HPE 3PAR 1.92TB SAS SSDs (Samsung PM1633a). I though they would be perfect for my SDDC home lab - sadly they did not work as intended with my HPE DL360 server. The SmartArray P420i was recognizing the disks but was giving the error \u0026quot;This physical drive does not support RAID and is not exposed to OS. It cannot be used for configuration on this controller\u0026quot;. But hopefully there is a solution to the problem we are facing:\nBasically the drives in a 3PAR Storage System use 520Bytes blocks as a low-level formatting. We need 512 Bytes for standard Operating Systems like Linux (520 Bytes does enable T10 DIF CRC error checking - the extra 8 Bytes are designated for data integrity/protection [1])\nSolution\nTL;DR:\nconnect to SAS Drives to a HBA or RAID-Controller in HBA Mode Install a OS (Windows or Linux) on a seperate, already working drive - or just use a live version install sg3_utils package [2] run ð™¨ð™œ_ð™¨ð™˜ð™–ð™£ and ð’”ð’ˆ_ð’‡ð’ð’“ð’Žð’‚ð’• --ð’‡ð’ð’“ð’Žð’‚ð’• --ð’”ð’Šð’›ð’† 512 ð‘·ð‘«ð’™ to reformat the disks to 512 Byte blocks The first step is to install the disk in a system with only a simple SAS HBA - most raid adapters will give you errors when you want to expose the disk to the OS. We need SCSI Access from a OS to do some commands. I used a HPE P420i in HBA Mode (Note: to be able to set your RAID Controller to HBA Mode, no logical drives should be present)\nNext install a OS on a already working drive, I used Rocky Linux - a live version also works fine. To be able to format the drives we need sg3_utils. Simply install the package with dnf:\nsudo dnf install sg3_utils Then with the help of sg3_utils package we run some commands. First we need to list all SCSI drives and get their ID (PDx)\nWindows\nsg_scan Then format one drive at a time (in this case PD1)\nsg_format --format --size 512 PD1 Linux\nsg_scan -i Then format one drive at a time (in this case sg3)\nsg_format --format --size 512 /dev/sg3 Now the disks are formatted with 512 byte blocks and you should be able to use the disk in a JBOD or RAID.\nReferences\n[1] https://www.openfabrics.org/images/2018workshop/presentations/307_TOved_T10-DIFOffload.pdf\n[2] https://sg.danny.cz/sg/sg3_utils.html\n","link":"//localhost:1313/post/reformatting-drives/","section":"post","tags":["Hardware","HPE"],"title":"Reformatting enterprise storage drives to 512 bytes"},{"body":"","link":"//localhost:1313/post/tanzu-mission-control-self-managed/","section":"post","tags":null,"title":"Tanzu Mission Control Self Managed"},{"body":"","link":"//localhost:1313/post/tanzu-cluster-creation/","section":"post","tags":["VMware","vSphere","VMware Tanzu","Kubernetes"],"title":"Tanzu with vSphere and Antrea: Cluster Creation"},{"body":"","link":"//localhost:1313/categories/it/","section":"categories","tags":null,"title":"IT"},{"body":"Intro This blog post is about getting started with VMware Tanzu Kubernetes Grid (TKG). TKG is used to deploy Tanzu Kubernetes Cluster to various Cloud Providers including vSphere, AWS and Azure. First, a management cluster (based on Docker and kind) is created from a Bootstrap Workstation, which itself then provides its web interface and additional CLI tools to create a Kubernetes cluster.\nTKG has no integration in vSphere UI. If you wan't to integrate Tanzu with vCenter/vSphere UI - use vSphere with Tanzu instead.\nInstall Tanzu Kubernetes Grid Prerequieries\nVMware vSphere 8 (7 U3 should work, in this tutorial we are using vSphere 8) Bootstrap VM or Workstation with Docker Access to VMware Customer Connect TKG Bootstrap Machine on Fedora See this blog post to create a Bootstrap Machine based on Fedora\nCreate Management Cluster Create the Management Cluster on your Machine:\nCreate a Tanzu Management Cluster to boostrap TKG\ntanzu management-cluster create --ui ","link":"//localhost:1313/post/tkg-getting-started/","section":"post","tags":["Kubernetes","VMware","VMware Tanzu","VMware Tanzu Kubernetes Grid"],"title":"VMware Tanzu Kubernetes Grid -  getting started"},{"body":"Cillium Gateway API The Cilium Gateway API is a Kubernetes API that provides a more powerful and flexible way to manage traffic routing than the traditional Ingress API of (vanilla) Kubernetes. It is a set of resources that model service networking in Kubernetes, and is designed to be role-oriented, portable, expressive, and extensible. The Cilium Service Mesh Gateway API Controller requires the ability to create LoadBalancer Kubernetes services.\n","link":"//localhost:1313/post/cillium-getting-started/","section":"post","tags":["Kubernetes Cillium","CNI"],"title":"Cillium Gateway API - Getting Started"},{"body":"","link":"//localhost:1313/tags/cni/","section":"tags","tags":null,"title":"CNI"},{"body":"","link":"//localhost:1313/tags/kubernetes-cillium/","section":"tags","tags":null,"title":"Kubernetes Cillium"},{"body":"","link":"//localhost:1313/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"","link":"//localhost:1313/tags/fedora/","section":"tags","tags":null,"title":"Fedora"},{"body":"TKG Bootstrap Machine on Fedora Tanzu Kubernetes Grid needs a workstation to bootstrap your Kubernetes Cluster. This is a short guide for a Fedora based bootstrap Machine:\nRequirements\nAbility to run Docker Access to download Tanzu CLI (via Customer Connect Portal) dedicated admin user in the wheel-group Install Docker\nDocker on modern RHEL systems can be confusing because the latest versions of RHEL replace Docker with Podman (dnf install docker will install podman). I should work with podman, but we explicity are going to install Docker Container Engine.\nLogin to your system (via ssh) with the admin user.\nAdd Repo and Install Docker CE\nsudo dnf -y install dnf-plugins-core sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Start \u0026amp; enable Docker\nsudo systemctl start docker \u0026amp;\u0026amp; sudo systemctl enable docker Set Docker Socket context to current user. This step is crucial, to allow the current user to access the docker socket as tanzu cli should run with the current user contexts not sudo.\nsudo chown $USER:docker /var/run/docker.sock You can also install docker-desktop if you want to have a GUI.\nInstall Tanzu CLI and Plugin\ncat \u0026lt;\u0026lt; EOF | sudo tee /etc/yum.repos.d/tanzu-cli.repo [tanzu-cli] name=Tanzu CLI baseurl=https://storage.googleapis.com/tanzu-cli-os-packages/rpm/tanzu-cli enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.vmware.com/tools/keys/VMWARE-PACKAGING-GPG-RSA-KEY.pub EOF sudo dnf install -y tanzu-cli tanzu plugin install --group vmware-tkg/default:v2.4.1 Install VMware kubectl\nGo to VMware Customer Connect and Download VMWare kubectl. https://customerconnect.vmware.com/downloads/get-download?downloadGroup=TKG-241)\nUnzip the package and make it executable\nchmod ugo+x kubectl-linux-v1.27.5+vmware.1 Install\nsudo install kubectl-linux-v1.27.5+vmware.1 /usr/local/bin/kubectl Test the setup If everything worked you should be able to run the following commands without any issues:\nList your Tanzu CLI plugins\ntanzu plugin list Create a Tanzu Management Cluster to boostrap TKG\ntanzu management-cluster create --ui Troubleshooting Error \u0026quot;Docker prerequiries not met\u0026quot;\nThis is usually the case when the current user does not have access to docker. In the current User Session run:\ndocker ps -a If presented with \u0026quot;Got permission denied\u0026quot; try adding current user to the wheel (local admin group) group\nsudo usermod -aG wheel $USER Then try again to create your tanzu management cluster.\n","link":"//localhost:1313/post/tanzu-kubernetes-grid-workstation-setup/","section":"post","tags":["VMware Tanzu","Tanzu Kubernetes Grid","Fedora","Docker"],"title":"Tanzu Kubernetes Grid - Linux Workstation Setup"},{"body":"Intro VMware vSAN is a Hyperconverged Infrastructure Solution from VMware. It basically turns your servers direct attached disks into a storage cluster.\nMinimum Bandwidth Requirements vSAN ESA A minimum bandwidth of 25 Gbps is required, 10Gbit works but is only supported for the AF0 Config.\nFor larger clusters or environments with high-capacity NVMe disks, even 25 Gbps may be insufficient. If you can do 100Gbps, the Performance gain will be massive.\nvSAN MAX Requires 100 Gbps\nSwitches that support higher port speeds are designed with higher Network Processor Unit (NPU) buffers. An NPU shared switch buffer of at least 16 MB is recommended for 10 GbE network connectivity. An NPU buffer of at least 32 MB is recommended for more demanding 25 GbE network connectivity.\nNetwork Configuration All hosts in the vSAN cluster must be connected to a vSAN Layer 2 or Layer 3 network LACP can be used, active/passive connection is recommended due to operational complexity. Latency Requirements Single Site vSAN Cluster Requires less than 1 ms RTT (Round Trip Time) inter-node latency For vSAN streched Clusters RTT must be below 5ms for the inter-site communication and less than 1 ms RTT within each site vSAN Strechted Cluster - Witness 2 Mbps per 1000 components (Maximum of 100 Mbps with 45k components) bandwidth between nodes and vSAN Witness Hosts General Considerations Use a dedicated VLAN on the Physical Network Switches for vSAN Traffic. If possible use Quality of Service Features to priorize vSAN traffic. Use two physical Uplinks for vSAN Traffic, do not mix with VM Workload traffic. Use a dedicated virtual Distributed Switch for vSphere Backend Services (vSAN, vMotion, FT) Use Network I/O Control vSAN over RoCE With vSphere 7.0 U2 vSAN with RDMA is supported. RDMA typically has lower CPU utilization and less I/O latency.\nIf your vSAN cluster will include adapters that support RoCE (RDMA over Converged Ethernet) for vSAN storage connectivity, the supporting network must support a â€œlosslessâ€ transport. A â€œlosslessâ€ network is defined as one where no frames are dropped because of network congestion.\nSelect switches that support Data Center Bridging (DCB). The Data Center Bridging feature supports the elimination of packet loss due to buffer or queue overflow. The Data Center Bridging must support bandwidth allocation based on priority settings, known as Class of Service (CoS). Priority Flow Control (PFC) is required on the switches to provide RoCE traffic a higher priority than other network traffic. All the nodes in the cluster connected to the common vSAN datastore must be configured with RoCE-supported adapter cards of the same vendor and the same model. This is strongly encouraged as a best practice to remove any possibility of slight variances in adapters from different vendors or different models disrupting I/O on the vSAN datastore.\nThe Ethernet ports on the RoCE-supported adapters are reserved for vSAN traffic only. The physical network supporting vSAN is configured as a â€œlosslessâ€ network. Look for RoCEv2 support with your Network Switch Vendor. Supported RDMA Network adapters can be found here:\nhttps://www.vmware.com/resources/compatibility/search.php?deviceCategory=rdmanic\u0026details=1\u0026vsan_type=rdmanic\u0026page=1\u0026display_interval=10\u0026sortColumn=Partner\u0026sortOrder=Asc\n","link":"//localhost:1313/post/vmware-vsan-network-requirements/","section":"post","tags":["VMware","vSAN","vSAN over RoCE"],"title":"VMware vSAN Network Considerations"},{"body":"","link":"//localhost:1313/tags/vsan-over-roce/","section":"tags","tags":null,"title":"VSAN Over RoCE"},{"body":"","link":"//localhost:1313/tags/monitoring/","section":"tags","tags":null,"title":"Monitoring"},{"body":"","link":"//localhost:1313/tags/tig-stack/","section":"tags","tags":null,"title":"TIG Stack"},{"body":"vSphere Monitoring with TIG (Telegraf, InfluxDB, Grafana) This blog post describes a solution for monitoring SDDC infrastructure using Telegraf, InfluxDB, and Grafana. This solution is based on Docker and displays graphs and metrics via Grafana. All metrics are described in the telegraf.conf file.\nTL;DR: https://github.com/varmox/vsphere-monitoring.git\nPrerequiries RHEL based Linux Docker \u0026amp; Docker Compose (or Podman with Docker Compose. In this tutorial we are using docker \u0026amp; docker compose) installed 3 internal IPs for telegraf, grafana and influxdb containers. Access to vSphere API (read-only is sufficient) Environment Variables\nIn this tutorial we're using the following varibales, change them according to your setup:\nsubnet 172.29.30.0/23 Filesystem structure:\n/srv/tig\nâ”œâ”€â”€ docker-compose.yml\nâ””â”€â”€ telegraf.conf\nCreate Telegraf Config Edit your telegraf.conf file with a editor of choice and paste overwrite your telegraf.conf File. This config skips tls certificate checking so can be used for self-signed certificates.\n[agent] ## Default data collection interval for all inputs interval = \u0026#34;10s\u0026#34; ## Rounds collection interval to \u0026#39;interval\u0026#39; ## ie, if interval=\u0026#34;10s\u0026#34; then always collect on :00, :10, :20, etc. round_interval = true ## Telegraf will send metrics to outputs in batches of at most ## metric_batch_size metrics. ## This controls the size of writes that Telegraf sends to output plugins. metric_batch_size = 1000 ## Maximum number of unwritten metrics per output. Increasing this value ## allows for longer periods of output downtime without dropping metrics at the ## cost of higher maximum memory usage. metric_buffer_limit = 10000 ## Collection jitter is used to jitter the collection by a random amount. ## Each plugin will sleep for a random time within jitter before collecting. ## This can be used to avoid many plugins querying things like sysfs at the ## same time, which can have a measurable effect on the system. collection_jitter = \u0026#34;0s\u0026#34; ## Default flushing interval for all outputs. Maximum flush_interval will be ## flush_interval + flush_jitter flush_interval = \u0026#34;10s\u0026#34; ## Jitter the flush interval by a random amount. This is primarily to avoid ## large write spikes for users running a large number of telegraf instances. ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s flush_jitter = \u0026#34;0s\u0026#34; ## By default or when set to \u0026#34;0s\u0026#34;, precision will be set to the same ## timestamp order as the collection interval, with the maximum being 1s. ## ie, when interval = \u0026#34;10s\u0026#34;, precision will be \u0026#34;1s\u0026#34; ## when interval = \u0026#34;250ms\u0026#34;, precision will be \u0026#34;1ms\u0026#34; ## Precision will NOT be used for service inputs. It is up to each individual ## service input to set the timestamp at the appropriate precision. ## Valid time units are \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;Âµs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;. precision = \u0026#34;\u0026#34; ## Log at debug level. # debug = false ## Log only error level messages. # quiet = false ## Log target controls the destination for logs and can be one of \u0026#34;file\u0026#34;, ## \u0026#34;stderr\u0026#34; or, on Windows, \u0026#34;eventlog\u0026#34;. When set to \u0026#34;file\u0026#34;, the output file ## is determined by the \u0026#34;logfile\u0026#34; setting. # logtarget = \u0026#34;file\u0026#34; ## Name of the file to be logged to when using the \u0026#34;file\u0026#34; logtarget. If set to ## the empty string then logs are written to stderr. # logfile = \u0026#34;\u0026#34; ## The logfile will be rotated after the time interval specified. When set ## to 0 no time based rotation is performed. Logs are rotated only when ## written to, if there is no log activity rotation may be delayed. # logfile_rotation_interval = \u0026#34;0d\u0026#34; ## The logfile will be rotated when it becomes larger than the specified ## size. When set to 0 no size based rotation is performed. # logfile_rotation_max_size = \u0026#34;0MB\u0026#34; ## Maximum number of rotated archives to keep, any older logs are deleted. ## If set to -1, no archives are removed. # logfile_rotation_max_archives = 5 ## Pick a timezone to use when logging or type \u0026#39;local\u0026#39; for local time. ## Example: America/Chicago # log_with_timezone = \u0026#34;\u0026#34; ## Override default hostname, if empty use os.Hostname() hostname = \u0026#34;\u0026#34; ## If set to true, do no set the \u0026#34;host\u0026#34; tag in the telegraf agent. omit_hostname = false [[outputs.influxdb_v2]] ## The URLs of the InfluxDB cluster nodes. ## ## Multiple URLs can be specified for a single cluster, only ONE of the ## urls will be written to each interval. ## ex: urls = [\u0026#34;https://us-west-2-1.aws.cloud2.influxdata.com\u0026#34;] urls = [\u0026#34;http://172.29.31.3:8086\u0026#34;] ## Token for authentication. token = \u0026#34;sFYmFqizqj8oJyJxuMc7l4PEztLIPOWfe0Aae_QwQiJXA-obkKo_AHCEgXRwMCEXXyYsq6mqRayan3Ylh_Dy0g==\u0026#34; ## Organization is the name of the organization you wish to write to; must exist. organization = \u0026#34;sddc\u0026#34; ## Destination bucket to write into. bucket = \u0026#34;vsphere\u0026#34; ## The value of this tag will be used to determine the bucket. If this ## tag is not set the \u0026#39;bucket\u0026#39; option is used as the default. # bucket_tag = \u0026#34;\u0026#34; ## If true, the bucket tag will not be added to the metric. # exclude_bucket_tag = false ## Timeout for HTTP messages. # timeout = \u0026#34;5s\u0026#34; ## Additional HTTP headers # http_headers = {\u0026#34;X-Special-Header\u0026#34; = \u0026#34;Special-Value\u0026#34;} ## HTTP Proxy override, if unset values the standard proxy environment ## variables are consulted to determine which proxy, if any, should be used. # http_proxy = \u0026#34;http://corporate.proxy:3128\u0026#34; ## HTTP User-Agent # user_agent = \u0026#34;telegraf\u0026#34; ## Content-Encoding for write request body, can be set to \u0026#34;gzip\u0026#34; to ## compress body or \u0026#34;identity\u0026#34; to apply no encoding. # content_encoding = \u0026#34;gzip\u0026#34; ## Enable or disable uint support for writing uints influxdb 2.0. # influx_uint_support = false ## Optional TLS Config for use on HTTP connections. # tls_ca = \u0026#34;/etc/telegraf/ca.pem\u0026#34; # tls_cert = \u0026#34;/etc/telegraf/cert.pem\u0026#34; # tls_key = \u0026#34;/etc/telegraf/key.pem\u0026#34; ## Use TLS but skip chain \u0026amp; host verification # insecure_skip_verify = false # Read metrics from VMware vCenter [[inputs.vsphere]] ## List of vCenter URLs to be monitored. These three lines must be uncommented ## and edited for the plugin to work. vcenters = [ \u0026#34;https://vcenter-fqdn/sdk\u0026#34; ] username = \u0026#34;serviceaccount@sso.vsphere.local\u0026#34; password = \u0026#34;secret\u0026#34; ## VMs ## Typical VM metrics (if omitted or empty, all metrics are collected) # vm_include = [ \u0026#34;/*/vm/**\u0026#34;] # Inventory path to VMs to collect (by default all are collected) # vm_exclude = [] # Inventory paths to exclude vm_metric_include = [ \u0026#34;cpu.demand.average\u0026#34;, \u0026#34;cpu.idle.summation\u0026#34;, \u0026#34;cpu.latency.average\u0026#34;, \u0026#34;cpu.readiness.average\u0026#34;, \u0026#34;cpu.ready.summation\u0026#34;, \u0026#34;cpu.run.summation\u0026#34;, \u0026#34;cpu.usagemhz.average\u0026#34;, \u0026#34;cpu.used.summation\u0026#34;, \u0026#34;cpu.wait.summation\u0026#34;, \u0026#34;mem.active.average\u0026#34;, \u0026#34;mem.granted.average\u0026#34;, \u0026#34;mem.latency.average\u0026#34;, \u0026#34;mem.swapin.average\u0026#34;, \u0026#34;mem.swapinRate.average\u0026#34;, \u0026#34;mem.swapout.average\u0026#34;, \u0026#34;mem.swapoutRate.average\u0026#34;, \u0026#34;mem.usage.average\u0026#34;, \u0026#34;mem.vmmemctl.average\u0026#34;, \u0026#34;net.bytesRx.average\u0026#34;, \u0026#34;net.bytesTx.average\u0026#34;, \u0026#34;net.droppedRx.summation\u0026#34;, \u0026#34;net.droppedTx.summation\u0026#34;, \u0026#34;net.usage.average\u0026#34;, \u0026#34;power.power.average\u0026#34;, \u0026#34;virtualDisk.numberReadAveraged.average\u0026#34;, \u0026#34;virtualDisk.numberWriteAveraged.average\u0026#34;, \u0026#34;virtualDisk.read.average\u0026#34;, \u0026#34;virtualDisk.readOIO.latest\u0026#34;, \u0026#34;virtualDisk.throughput.usage.average\u0026#34;, \u0026#34;virtualDisk.totalReadLatency.average\u0026#34;, \u0026#34;virtualDisk.totalWriteLatency.average\u0026#34;, \u0026#34;virtualDisk.write.average\u0026#34;, \u0026#34;virtualDisk.writeOIO.latest\u0026#34;, \u0026#34;sys.uptime.latest\u0026#34;, ] # vm_metric_exclude = [] ## Nothing is excluded by default # vm_instances = true ## true by default ## Hosts ## Typical host metrics (if omitted or empty, all metrics are collected) # host_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to hosts to collect (by default all are collected) # host_exclude [] # Inventory paths to exclude host_metric_include = [ \u0026#34;cpu.coreUtilization.average\u0026#34;, \u0026#34;cpu.costop.summation\u0026#34;, \u0026#34;cpu.demand.average\u0026#34;, \u0026#34;cpu.idle.summation\u0026#34;, \u0026#34;cpu.latency.average\u0026#34;, \u0026#34;cpu.readiness.average\u0026#34;, \u0026#34;cpu.ready.summation\u0026#34;, \u0026#34;cpu.swapwait.summation\u0026#34;, \u0026#34;cpu.usage.average\u0026#34;, \u0026#34;cpu.usagemhz.average\u0026#34;, \u0026#34;cpu.used.summation\u0026#34;, \u0026#34;cpu.utilization.average\u0026#34;, \u0026#34;cpu.wait.summation\u0026#34;, \u0026#34;disk.deviceReadLatency.average\u0026#34;, \u0026#34;disk.deviceWriteLatency.average\u0026#34;, \u0026#34;disk.kernelReadLatency.average\u0026#34;, \u0026#34;disk.kernelWriteLatency.average\u0026#34;, \u0026#34;disk.numberReadAveraged.average\u0026#34;, \u0026#34;disk.numberWriteAveraged.average\u0026#34;, \u0026#34;disk.read.average\u0026#34;, \u0026#34;disk.totalReadLatency.average\u0026#34;, \u0026#34;disk.totalWriteLatency.average\u0026#34;, \u0026#34;disk.write.average\u0026#34;, \u0026#34;mem.active.average\u0026#34;, \u0026#34;mem.latency.average\u0026#34;, \u0026#34;mem.state.latest\u0026#34;, \u0026#34;mem.swapin.average\u0026#34;, \u0026#34;mem.swapinRate.average\u0026#34;, \u0026#34;mem.swapout.average\u0026#34;, \u0026#34;mem.swapoutRate.average\u0026#34;, \u0026#34;mem.totalCapacity.average\u0026#34;, \u0026#34;mem.usage.average\u0026#34;, \u0026#34;mem.vmmemctl.average\u0026#34;, \u0026#34;net.bytesRx.average\u0026#34;, \u0026#34;net.bytesTx.average\u0026#34;, \u0026#34;net.droppedRx.summation\u0026#34;, \u0026#34;net.droppedTx.summation\u0026#34;, \u0026#34;net.errorsRx.summation\u0026#34;, \u0026#34;net.errorsTx.summation\u0026#34;, \u0026#34;net.usage.average\u0026#34;, \u0026#34;power.power.average\u0026#34;, \u0026#34;storageAdapter.numberReadAveraged.average\u0026#34;, \u0026#34;storageAdapter.numberWriteAveraged.average\u0026#34;, \u0026#34;storageAdapter.read.average\u0026#34;, \u0026#34;storageAdapter.write.average\u0026#34;, \u0026#34;sys.uptime.latest\u0026#34;, ] ## Collect IP addresses? Valid values are \u0026#34;ipv4\u0026#34; and \u0026#34;ipv6\u0026#34; # ip_addresses = [\u0026#34;ipv6\u0026#34;, \u0026#34;ipv4\u0026#34; ] # host_metric_exclude = [] ## Nothing excluded by default # host_instances = true ## true by default ## Clusters # cluster_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to clusters to collect (by default all are collected) # cluster_exclude = [] # Inventory paths to exclude # cluster_metric_include = [] ## if omitted or empty, all metrics are collected # cluster_metric_exclude = [] ## Nothing excluded by default # cluster_instances = false ## false by default ## Datastores # datastore_include = [ \u0026#34;/*/datastore/**\u0026#34;] # Inventory path to datastores to collect (by default all are collected) # datastore_exclude = [] # Inventory paths to exclude # datastore_metric_include = [] ## if omitted or empty, all metrics are collected # datastore_metric_exclude = [] ## Nothing excluded by default # datastore_instances = false ## false by default ## Datacenters # datacenter_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to clusters to collect (by default all are collected) # datacenter_exclude = [] # Inventory paths to exclude datacenter_metric_include = [] ## if omitted or empty, all metrics are collected datacenter_metric_exclude = [ \u0026#34;*\u0026#34; ] ## Datacenters are not collected by default. # datacenter_instances = false ## false by default ## Plugin Settings ## separator character to use for measurement and field names (default: \u0026#34;_\u0026#34;) # separator = \u0026#34;_\u0026#34; ## number of objects to retrieve per query for realtime resources (vms and hosts) ## set to 64 for vCenter 5.5 and 6.0 (default: 256) # max_query_objects = 256 ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores) ## set to 64 for vCenter 5.5 and 6.0 (default: 256) # max_query_metrics = 256 ## number of go routines to use for collection and discovery of objects and metrics # collect_concurrency = 1 # discover_concurrency = 1 ## the interval before (re)discovering objects subject to metrics collection (default: 300s) # object_discovery_interval = \u0026#34;300s\u0026#34; ## timeout applies to any of the api request made to vcenter # timeout = \u0026#34;60s\u0026#34; ## When set to true, all samples are sent as integers. This makes the output ## data types backwards compatible with Telegraf 1.9 or lower. Normally all ## samples from vCenter, with the exception of percentages, are integer ## values, but under some conditions, some averaging takes place internally in ## the plugin. Setting this flag to \u0026#34;false\u0026#34; will send values as floats to ## preserve the full precision when averaging takes place. # use_int_samples = true ## Custom attributes from vCenter can be very useful for queries in order to slice the ## metrics along different dimension and for forming ad-hoc relationships. They are disabled ## by default, since they can add a considerable amount of tags to the resulting metrics. To ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include ## to select the attributes you want to include. ## By default, since they can add a considerable amount of tags to the resulting metrics. To ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include ## to select the attributes you want to include. # custom_attribute_include = [] # custom_attribute_exclude = [\u0026#34;*\u0026#34;] ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported ## with a significant delay (\u0026gt;30min). If this happens, try increasing this number. Please note that increasing ## it too much may cause performance issues. # metric_lookback = 3 ## Optional SSL Config # ssl_ca = \u0026#34;/path/to/cafile\u0026#34; # ssl_cert = \u0026#34;/path/to/certfile\u0026#34; # ssl_key = \u0026#34;/path/to/keyfile\u0026#34; ## Use SSL but skip chain \u0026amp; host verification insecure_skip_verify = true ## The Historical Interval value must match EXACTLY the interval in the daily # \u0026#34;Interval Duration\u0026#34; found on the VCenter server under Configure \u0026gt; General \u0026gt; Statistics \u0026gt; Statistic intervals # historical_interval = \u0026#34;5m\u0026#34; Container Setup Use the following docker compose file to start your containers. This solution uses ipvlan to assign each container an IPv4 Address. You don't have to assign the IPs to your OSes NIC.\nChange the passwords and secrets to your needs.\nversion: \u0026#39;3.6\u0026#39; networks: ipvlan0: driver: ipvlan driver_opts: parent: ens192 services: telegraf: image: telegraf container_name: telegraf restart: always volumes: - ./telegraf.conf:/etc/telegraf/telegraf.conf:ro depends_on: - influxdb links: - influxdb ports: - \u0026#39;8125:8125\u0026#39; networks: ipvlan0: ipv4_address: 172.29.31.2 influxdb: image: influxdb:2.6-alpine container_name: influxdb restart: always environment: - INFLUXDB_DB=influx - INFLUXDB_ADMIN_USER=admin - INFLUXDB_ADMIN_PASSWORD=admin networks: ipvlan0: ipv4_address: 172.29.31.3 ports: - \u0026#39;8086:8086\u0026#39; volumes: - influxdb_data:/var/lib/influxdb grafana: image: grafana/grafana container_name: grafana-server restart: always depends_on: - influxdb environment: - GF_SECURITY_ADMIN_USER=admin - GF_SECURITY_ADMIN_PASSWORD=admin - GF_INSTALL_PLUGINS= links: - influxdb networks: ipvlan0: ipv4_address: 172.29.31.4 ports: - \u0026#39;3000:3000\u0026#39; volumes: - grafana_data:/var/lib/grafana volumes: grafana_data: {} influxdb_data: {} Start Containers\nIn the directory where the docker compose file is located:\ndocker compose up -d Check Containers\ndocker ps -a The containers should all have a running state.\nCreate TIG Config Head to your influxDB container ip with port 8086. In my case it is '172.29.31.3:8086' With help of the webinterface create your buckets according to the naming in your \u0026quot;telegraf.conf\u0026quot; file.\nImport Grafana Dashboards Access your grafana dashboard at your IP with port 3000 (in my case https://172.29.31.4:3000)\nThe grafana dashboards can be access via github: https://github.com/jorgedlcruz/vmware-grafana/tree/master (Cudos to Jorge de la Cruz)\nImport the grafana dashboards:\nClick Dashboards in the left-side menu. Click New and select Import in the dropdown menu. Paste dashboard JSON text directly into the text area https://jorgedelacruz.uk/\n","link":"//localhost:1313/post/vsphere-monitoring/","section":"post","tags":["VMware","vSphere","Monitoring","TIG Stack"],"title":"vSphere Monitoring with Grafana"},{"body":"","link":"//localhost:1313/tags/it-security/","section":"tags","tags":null,"title":"IT-Security"},{"body":"","link":"//localhost:1313/tags/opinion/","section":"tags","tags":null,"title":"Opinion"},{"body":"Why current Enterprise IT-Security is s shipwreck.\nThe state of enterprise IT-Security is pretty bad and I don't think the current strategy to better it is any good. And here's the why:\nstatus quo\nBut firstly, what are the security concerns of businesses regarding IT-Security?\nIf we take the CIA triad for help:\nRansomware that disrups availability and integrity Data Leaks that violences confidentiality malware/virus that disrups integrity .. and more\ncurrent ways \u0026quot;to fix\u0026quot; the issue\nIf we look at current solutions that enterprise commonly use to \u0026quot;fix it\u0026quot;:\nsomekind of antivirus systems (on servers, clients, firewalls) IPD/IDS Systems (nothing other than an antivirus running on a network device) MFA an other Authentication Serivces, Conditional access etc - mostly based on Active Directory Why I think current ways to better IT-Security is b*llshit\nWe simply to not fix the root cause of the problems. We use somekind of software to secure unsecure software and think the \u0026quot;secure\u0026quot; sofware is initself secure. But there is a fundamental issue, every software has bugs. With each additional line of code written, there are potential bugs in that exact code. So more code just means more bugs. Bugs mean potential security bugs and weaknesses.\nYou cannot shift the responsibility to the user (aka do not open that email attachement) - A Backoffice persons jobs is to open emails with attachements, how should they make sure every email attachement is save? The fundamental problem is that an office application can open doors to ransomware and encrypt your hole (microsoft based) Infrastructure\nThe deadly combination of Winodws + Office + Active Directory\nCat an mouse game\n","link":"//localhost:1313/post/state-of-enterprise-itsecurity/","section":"post","tags":["IT-Security","Opinion"],"title":"State of Enterprise IT-Security"},{"body":"Intro This post gives a few examples of Tanzu Kubernetes Cluster Manifests and how to deploy them.\nPrerequiries Successfull Installation of vSphere with Tanzu Supervisor vSphere Namespace created Already existing VM Classes, Storages Policies and Tanzu Kubernetes Releases assigned to the Namespace. Login via kubectl You will find the IP to your Kubernetes API on the Namespace Option \u0026quot;Link to CLI Tools\u0026quot;\nkubectl vsphere login --server=10.40.80.20 kubectl config get-contexts kubectl config use-context \u0026lt;\u0026gt; Get Parameters of your vSphere Namespace VM Class Storagge Policy Tanzu Releases kubectl get vmclass kubectl get storageclass kubectl get tanzukubernetesrelease Cluster with 3 Control and 6 Worker Nodes apiVersion: run.tanzu.vmware.com/v1alpha3 kind: TanzuKubernetesCluster metadata: name: tkgs-cluster namespace: tkgs-cluster-ns spec: topology: controlPlane: replicas: 3 vmClass: best-effort-large storageClass: workload-management-storage-policy tkr: reference: name: v1.23.8---vmware.2-tkg.2-zshippable nodePools: - replicas: 6 name: worker vmClass: best-effort-large storageClass: workload-management-storage-policy Save this file as a yaml file on your workstation.\nApply Cluster Manifest kubectl apply -f clusterspecs.yaml kubectl get tanzukubernetescluster Check the Status of the Cluster. \u0026quot;READY\u0026quot; should be true after some minutes.\nEdge Cluster apiVersion: run.tanzu.vmware.com/v1alpha3 kind: TanzuKubernetesCluster metadata: name: tkgs-cluster namespace: tkgs-cluster-ns spec: topology: controlPlane: replicas: 3 vmClass: best-effort-small storageClass: workload-management-storage-policy tkr: reference: name: v1.23.8---vmware.2-tkg.2-zshippable nodePools: - replicas: 3 name: worker vmClass: best-effort-medium storageClass: workload-management-storage-policy Minimal Cluser for PoC Use this cluster only for PoC/testing. Only three VMs are created in total. Later, you'll use Kubernetes Taints to enable the Control node to run user workloads. This is useful when testing workloads with a minimal VM footprint when deploying in environments with few resources available.\nConcept of Kubernetes Taints In Kubernetes, taints are a way to mark a node with a special attribute that affects which pods can be scheduled onto that node. Taints are used to repel or attract pods based on certain criteria, such as node characteristics or hardware specifications.\napiVersion: run.tanzu.vmware.com/v1alpha3 kind: TanzuKubernetesCluster metadata: name: tkgs-cluster namespace: tkgs-cluster-ns spec: topology: controlPlane: replicas: 3 vmClass: best-effort-medium storageClass: workload-management-storage-policy tkr: reference: name: v1.23.8---vmware.2-tkg.2-zshippable Do not use this in a production environment After a successfull deployment of the cluster run the following commands to allow the Control Nodes to run User Workloads:\nkubectl taint nodes --all node-role.kubernetes.io/master- ","link":"//localhost:1313/post/tanzu-kubernetes-cluster-examples/","section":"post","tags":["VMware Tanzu","Tanzu Kubernetes Clusters","yaml"],"title":"Tanzu Kubernetes Cluster Example Deployments"},{"body":"","link":"//localhost:1313/tags/tanzu-kubernetes-clusters/","section":"tags","tags":null,"title":"Tanzu Kubernetes Clusters"},{"body":"","link":"//localhost:1313/tags/yaml/","section":"tags","tags":null,"title":"Yaml"},{"body":"Quick Tip: vSphere with Tanzu an HA-Proxy This is a short article with tips and tricks I experienced when deploying vSphere with Tanzu and HA-Proxy as a Load Balancer.\nDeploying Ha-Proxy OVA Network\nAlwasys use three NICs when deploying in a production environment. It is very important that these IP addresses do not overlap with the address ranges of your workloads and load balancers, or with the gateway itself.\nAt first it can be somewhat confusing what networks are needed and which services will be deployed in each network.\nManagement: Your Management Network (must be only accessible for Admins, like vCenter) Workload: This is the workload IP address for the HA Proxy server. This subnet will be used to for virtual servers created by HA-Proxy. Frontend: This is the frontend IP address for the HA Proxy server. From this network clients will access resources. For a simple PoC you can safely use two NIC. Management is seperate and Workload/Frontend will be shared.\nCertificate\nJust leave it blank it will be generated for you.\nGetting the Certificate From your Workstation run the following command to get the Certificates from HA-Proxy needed for the Workload Management Feature Setup:\nscp root@haproxyMGMTIP:/etc/haproxy/ca.crt ca.crt \u0026amp;\u0026amp; cat ca.crt ","link":"//localhost:1313/post/tanzu-haproxy-tips/","section":"post","tags":["VMware","vSphere","VMware Tanzu"],"title":"Quicktip: Tanzu \u0026 HA-Proxy"},{"body":"","link":"//localhost:1313/tags/cheat-sheet/","section":"tags","tags":null,"title":"Cheat Sheet"},{"body":"Kubernetes Cheat Sheet\nCheat Sheet for kubectl, kubeadm and general k8s related commands.\nkubectl Interacting with clusters kubectl command description Comment kubectl config view View Kubectl Configuration Display the current Kubectl configuration kubectl config get-contexts List available contexts See the available Kubernetes clusters and their associated contexts kubectl config current-context Display the current context Show the active Kubernetes cluster and context kubectl config use-context \u0026lt;context_name\u0026gt; Switch to another context Change the active context to the specified one kubectl config set-context \u0026lt;context_name\u0026gt; --namespace=\u0026lt;namespace_name\u0026gt; Set default namespace for a context Specify the default namespace for a specific context kubectl config unset current-context Unset the current context Remove the association with the current context kubectl config set-cluster \u0026lt;cluster_name\u0026gt; --server=\u0026lt;api_server_url\u0026gt; Add a new cluster to the configuration Define a new Kubernetes cluster with its API server URL kubectl config set-credentials \u0026lt;user_name\u0026gt; --token=\u0026lt;access_token\u0026gt; Add user credentials to the configuration Specify user credentials, typically an access token kubectl config set-context \u0026lt;context_name\u0026gt; --cluster=\u0026lt;cluster_name\u0026gt; --user=\u0026lt;user_name\u0026gt; Create a new context Combine a cluster and user to create a context kubectl config delete-context \u0026lt;context_name\u0026gt; Delete a context Remove a specific context from the configuration kubectl config delete-cluster \u0026lt;cluster_name\u0026gt; Delete a cluster Remove a cluster from the configuration kubectl config delete-user \u0026lt;user_name\u0026gt; Delete user credentials Remove user credentials from the configuration kubectl config use-context \u0026lt;context_name\u0026gt; Switch to another context Change the active context to the specified one kubectl config rename-context \u0026lt;old_name\u0026gt; \u0026lt;new_name\u0026gt; Rename a context Change the name of an existing context kubectl config set preferences.colors true Enable colorized output Enhance readability with colorized command output General Troubleshooting kubectl command description Comment kubectl get events Check events for resources View events for debugging and troubleshooting kubectl describe node \u0026lt;node_name\u0026gt; Run diagnostics on a specific node Replace \u0026lt;node_name\u0026gt; with the actual node name kubectl get componentstatuses Check the health of cluster components Verify the status of essential components in the cluster kubectl get pods --all-namespaces List all pods in all namespaces Useful for identifying pods in any namespace kubectl describe pod \u0026lt;pod_name\u0026gt; Get detailed information about a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl logs \u0026lt;pod_name\u0026gt; Retrieve the logs from a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl top nodes Display resource usage statistics for nodes View CPU and memory usage for all nodes kubectl top pods Display resource usage statistics for pods View CPU and memory usage for all pods Nodes kubectl command description Comment kubectl get nodes List all nodes in the cluster Display a list of nodes and their status kubectl describe node \u0026lt;node_name\u0026gt; Get detailed information about a node Replace \u0026lt;node_name\u0026gt; with the actual node name kubectl get nodes -o wide Display additional details about nodes View IP addresses and other information about nodes kubectl get nodes --show-labels Show labels assigned to nodes Display labels associated with each node kubectl get nodes -o json Display node information in JSON format View detailed node information in JSON kubectl cordon \u0026lt;node_name\u0026gt; Mark a node as unschedulable Prevent new pods from being scheduled on the specified node kubectl uncordon \u0026lt;node_name\u0026gt; Mark a node as schedulable Allow new pods to be scheduled on the specified node kubectl drain \u0026lt;node_name\u0026gt; Safely evict all pods from a node Evacuate a node for maintenance kubectl top nodes Display resource usage statistics for nodes View CPU and memory usage for all nodes kubectl get nodes --sort-by=.status.capacity.cpu Sort nodes by CPU capacity Sort nodes based on CPU capacity in ascending order kubectl label nodes \u0026lt;node_name\u0026gt; \u0026lt;label_key\u0026gt;=\u0026lt;label_value\u0026gt; Label a node Assign a label to a specific node kubectl taint nodes \u0026lt;node_name\u0026gt; \u0026lt;taint_key\u0026gt;=\u0026lt;taint_value\u0026gt;: Taint a node Apply a taint to a node for node affinity or tolerations kubectl describe nodes Get detailed information about all nodes View detailed information about all nodes in the cluster kubectl get componentstatuses Check the health of cluster components Verify the status of essential components in the cluster kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.phase Display custom columns for nodes Define custom output columns for nodes Pods kubectl command description Comment kubectl get pods List all pods in the current namespace Display a list of pods and their status kubectl get pods -n List pods in a specific namespace Replace \u0026lt;namespace\u0026gt; with the desired namespace kubectl describe pod \u0026lt;pod_name\u0026gt; Get detailed information about a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl logs \u0026lt;pod_name\u0026gt; Retrieve the logs from a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl exec -it \u0026lt;pod_name\u0026gt; -- Execute a command in a running pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name and \u0026lt;command\u0026gt; with the desired command kubectl port-forward \u0026lt;pod_name\u0026gt; \u0026lt;local_port\u0026gt;:\u0026lt;pod_port\u0026gt; Forward a pod's port to the local machine Replace \u0026lt;pod_name\u0026gt;, \u0026lt;local_port\u0026gt;, and \u0026lt;pod_port\u0026gt; with the actual values kubectl delete pod \u0026lt;pod_name\u0026gt; Delete a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl delete pods --all Delete all pods in the current namespace Be cautious when using this command kubectl apply -f \u0026lt;pod_manifest.yaml\u0026gt; Create or update a pod using a manifest file Replace \u0026lt;pod_manifest.yaml\u0026gt; with the path to the pod's YAML manifest kubectl get pod \u0026lt;pod_name\u0026gt; -o yaml Get the YAML definition of a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl exec -it \u0026lt;pod_name\u0026gt; -- /bin/bash Open a shell in a running pod for debugging Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl describe pod \u0026lt;pod_name\u0026gt; | grep Image Get the container image version running in a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name Storage kubectl command description Comment kubectl get storageclass List available storage classes Check the available storage classes in the cluster kubectl describe storageclass \u0026lt;storageclass_name\u0026gt; Get detailed information about a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name kubectl get persistentvolumes Display information about persistent volumes View details of persistent volumes in the cluster kubectl describe persistentvolume \u0026lt;pv_name\u0026gt; Get detailed information about a persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl get persistentvolumeclaims List persistent volume claims View information about persistent volume claims in the current namespace kubectl describe persistentvolumeclaim \u0026lt;pvc_name\u0026gt; Get detailed information about a persistent volume claim Replace \u0026lt;pvc_name\u0026gt; with the actual persistent volume claim name kubectl get pv,pvc List both persistent volumes and claims Display a combined list of persistent volumes and claims kubectl apply -f \u0026lt;storage_manifest.yaml\u0026gt; Create or update storage using a manifest file Replace \u0026lt;storage_manifest.yaml\u0026gt; with the path to the storage YAML manifest kubectl delete persistentvolume \u0026lt;pv_name\u0026gt; Delete a specific persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl delete persistentvolumeclaims --all Delete all persistent volume claims in the current namespace Be cautious when using this command kubectl get storageclass -o yaml Get the YAML definition of a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name kubectl get persistentvolume -o yaml Get the YAML definition of a persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl get persistentvolumeclaim -o yaml Get the YAML definition of a persistent volume claim Replace \u0026lt;pvc_name\u0026gt; with the actual persistent volume claim name kubectl exec -it \u0026lt;pod_name\u0026gt; -- df -h Check storage usage inside a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name and df -h with the desired command kubectl describe storageclass \u0026lt;storageclass_name\u0026gt; | grep Provisioner Get the provisioner information for a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name Services kubectl command description Comment kubectl get services List all services in the current namespace Display a list of services and their types kubectl get services -o wide Display additional details about services View IP addresses and ports of services kubectl describe service \u0026lt;service_name\u0026gt; Get detailed information about a service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl get service \u0026lt;service_name\u0026gt; -o yaml Get the YAML definition of a service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl expose deployment \u0026lt;deployment_name\u0026gt; --port=\u0026lt;port_number\u0026gt; --target-port=\u0026lt;target_port\u0026gt; --name=\u0026lt;service_name\u0026gt; --type=\u0026lt;service_type\u0026gt; Expose a deployment as a service Replace \u0026lt;deployment_name\u0026gt;, \u0026lt;port_number\u0026gt;, \u0026lt;target_port\u0026gt;, \u0026lt;service_name\u0026gt;, and \u0026lt;service_type\u0026gt; with the actual values kubectl apply -f \u0026lt;service_manifest.yaml\u0026gt; Create or update a service using a manifest file Replace \u0026lt;service_manifest.yaml\u0026gt; with the path to the service YAML manifest kubectl delete service \u0026lt;service_name\u0026gt; Delete a specific service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl get services --sort-by=.spec.ports[0].nodePort Sort services by NodePort Sort services based on NodePort value in ascending order kubectl get services -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP,EXTERNAL-IP:.status.loadBalancer.ingress[0].ip,PORT(S).:.spec.ports[*].nodePort Display custom columns for services Define custom output columns for services kubectl get services -n List services in a specific namespace Replace \u0026lt;namespace\u0026gt; with the desired namespace kubectl get endpoints \u0026lt;service_name\u0026gt; Display endpoints for a service View the IP addresses and ports of pods backing a service kubectl describe ingress \u0026lt;ingress_name\u0026gt; Get detailed information about an ingress Replace \u0026lt;ingress_name\u0026gt; with the actual ingress name kubectl get svc \u0026lt;service_name\u0026gt; -o=jsonpath='{.spec.ports[0].nodePort}' Get the NodePort of a service using JSONPath Replace \u0026lt;service_name\u0026gt; with the actual service name Operations Run kubectl as non-root user non-root@cp: Ìƒ$ mkdir -p $HOME/.kube non-root@cp: Ìƒ$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config non-root@cp: Ìƒ$ sudo chown $(id -u):$(id -g) $HOME/.kube/config non-root@cp: Ìƒ$ less .kube/config\n","link":"//localhost:1313/post/kubernetes/k8-cheat-sheet/","section":"post","tags":["Cheat Sheet","Kubernetes"],"title":"K8 Cheat Sheet"},{"body":"","link":"//localhost:1313/tags/huawei/","section":"tags","tags":null,"title":"Huawei"},{"body":"Huawei Versatile Routing Platform (VRP) Overview\nThis post gives an overview about the network operating system for huawei network devices - VRP.\nGeneral\nTask Command Comment Enter System View \u0026lt; Huawei \u0026gt; sys Enter Interface View [R1] int GigabitEthernet0/0/1 Enter Protocol View [R1] ospf 1 Display Running Config in current View [R1- GigabitEthernet0/0/3] display this Display current configuration of the System [R1] dis current-config System Configuration\nTask Command Comment Set hostname [Huawei] sysname R1 Interface Configuration\nTask Command Comment Set IP Address of Interface [R1-GigabitEthernet0/0/1] ip address 172.29.40.13 24 Switching\nTask Command Comment VLAN taggin per Port --- --- First create a vlanif int vlanif ","link":"//localhost:1313/post/networking/huawei-vrp/","section":"post","tags":["Networking","Huawei"],"title":"Huawei VRP"},{"body":"","link":"//localhost:1313/tags/networking/","section":"tags","tags":null,"title":"Networking"},{"body":"A blog about virtualization, networking, kubernetes and general IT related topics. Opinions are my own.\nWhy vBÃ¼nzli? The meaning of BÃ¼nzli\nWhile it is difficult to translate precisely, even for Swiss people, it is generally accepted that BÃ¼nzli describes a person who is very conformist and is always prepared to ask others to abide by the rules\nor as other internet fellows describe it in schwizerdÃ¼tsch:\n..BÃ¼nzli isch nid nume e \u0026quot;schteit of mÃ¤ind\u0026quot; sondern wÃ¼rklech e LÃ¤bensart...\nSee also\n#takeitwithagrainofsalt\n\u0026gt;_ about me System Engineer based in central Switzerland\n","link":"//localhost:1313/about/","section":"","tags":null,"title":"About this blog"},{"body":"","link":"//localhost:1313/archives/","section":"","tags":null,"title":""},{"body":"","link":"//localhost:1313/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"//localhost:1313/tags/data-services-manager/","section":"tags","tags":null,"title":"Data Services Manager"},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager"],"title":"Data Services Manager - K8s "},{"body":"","link":"//localhost:1313/tags/dsm/","section":"tags","tags":null,"title":"DSM"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-upgrade/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Data Services Manager - Update"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-upgrade/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Data Services Manager - Update to 9"},{"body":"","link":"//localhost:1313/categories/dsm/","section":"categories","tags":null,"title":"DSM"},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"","link":"//localhost:1313/tags/postgresql/","section":"tags","tags":null,"title":"PostgreSQL"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-upgrade/","section":"post","tags":["DSM","Data Services Manager"],"title":"Data Services Manager - Update to 9"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/dsm-9-upgrade/","section":"post","tags":["DSM","Data Services Manager","VCF 9"],"title":"Data Services Manager - Update to 9"},{"body":"","link":"//localhost:1313/tags/vcf-9/","section":"tags","tags":null,"title":"VCF 9"},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nasdasd\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nasd\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nasd\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Install the operator Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backuplocations and your Infrastructure Policies to the values-override.yaml.\nhelm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backuplocations and your Infrastructure Policies to the values-override.yaml.\nhelm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backuplocations and your Infrastructure Policies to the values-override.yaml.\nhelm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall DSM Consumption Operator (Version 2.x) Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f ","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] [2] (DSM K8s Operator)[https://techdocs.broadcom.com/us/en/vmware-cis/dsm/data-services-manager/9-0/working-with-databases-in-vmware-data-services-manager/enabling-self-service-consumption-of-vmware-data-services-manager/step-2-install-the-dsm-consumption-operator.html]\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] [2] DSM K8s Operator[https://techdocs.broadcom.com/us/en/vmware-cis/dsm/data-services-manager/9-0/working-with-databases-in-vmware-data-services-manager/enabling-self-service-consumption-of-vmware-data-services-manager/step-2-install-the-dsm-consumption-operator.html]\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] (DSM K8s Operator)\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] [2]DSM K8s Operator\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1]\n[2]DSM K8s Operator\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1]\n[2] DSM K8s Operator\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] DSM Intro\n[2] DSM K8s Operator\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create Example:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it: ```bash kubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Ressources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM through UI (DSM UI or VCF Automation) or via K8s Operator.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\na\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\na\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisiones Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\na\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\na\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a kubernetes cluster and some apps need a PostgreSQL Database. In general it is a good idea to have your persitent data outside of your k8s cluster (s3, external DB etc). You could have postgres running on the same k8s cluster as your other apps but that will be more of a Build-your-own solution which will have it drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and option pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and setup PostgreSQL within those three VMs (note: it will leverage kubernetes for that.\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\ndd your Backuplocations and your Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Location can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backup Locations and Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Locations can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1 two things have changed to regards the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backup Locations and Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Locations can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1, two things have changed regarding the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\na\nRessources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backup Locations and Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Locations can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1, two things have changed regarding the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\na\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nResources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backup Locations and Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Locations can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1, two things have changed regarding the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nResources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/vks-troubleshooting/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Vks Troubleshooting"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/vks-vmotion/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Vks Vmotion"},{"body":"Insert Lead paragraph here.\n","link":"//localhost:1313/post/vcd-postgres-update/","section":"post","tags":["Tag_name1","Tag_name2"],"title":"Vcd Postgres Update"},{"body":"","link":"//localhost:1313/tags/architecture/","section":"tags","tags":null,"title":"Architecture"},{"body":"","link":"//localhost:1313/tags/design/","section":"tags","tags":null,"title":"Design"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides:\nlogcal design physical design conceptual design ","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defined HOW the architecture looks like. Its the detailed design, the how:\nspecific server models uplink design (switchports, pNICs etc) vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) The physical design\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defines HOW the system is built. A few examples for the physical design:\nspecific server models and their hardware layout, BIOS/UEFI Config Which Switchports are used vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) If you compare it with a house, the physical architecture is the CAD Drawing:\nOutput: LLD (Low Level Design), rack diagrams, IP sheets, runbooks.\nConceptual Design the WHY\nThe conceptual design is the opposite of the physical design, is all about the why - the \u0026quot;owners perspective\u0026quot;. No vendor names, no specs. Pure intent and requirements.\n\u0026quot;We need high availability for critical workloads\u0026quot; \u0026quot;Development environments must be isolated from production\u0026quot; \u0026quot;We need DR capabilities with RTO \u0026lt; 4h\u0026quot; \u0026quot;Infrastructure must scale to support 500 VMs over 3 years\u0026quot;\nLogical Design The logical design glues the physical and conceptual design together. The logical design translates conceptual fuzziness into a coherent architectural blueprint â€“ it's the architect's perspective made tangible. It's where design decisions get weighed: benefits against risks, trade-offs made explicit, alternatives considered and dismissed with reasoning. Without it, the gap between intent and implementation becomes dangerous. A conceptual design alone leaves too many valid technical solutions on the table with no basis for choosing between them. A physical design alone is a set of instructions without justification â€“ there's no traceability back to business requirements, and no one can explain why things were built the way they were. That's what makes the logical design arguably the most critical layer of any architecture. It sits at the intersection of business and engineering, speaking to both audiences simultaneously. The owner sees their requirements reflected in structure and decision rationale. The engineer sees a clear, justified blueprint to build from. It's the one artifact that both parties can read, understand, and hold each other accountable to.\nVendor-aware but not yet hardware-specific.\nvSphere cluster topology: how many clusters, what separation (management / edge / compute) vSAN vs. shared storage decision NSX segment and tier layout: T0/T1 design, overlay vs. underlay separation Network zones: management, vMotion, vSAN, VM traffic â€“ VLANs defined but no physical ports yet vCenter hierarchy: datacenter objects, host folders, resource pools, tagging strategy DRS / HA policies per cluster DR strategy: Stretched cluster vs. SRM vs. Zerto Output: Logical diagrams, VLAN tables, cluster design docs â€“ the classic VMware HLD (High Level Design)\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defines HOW the system is built. A few examples for the physical design:\nspecific server models and their hardware layout, BIOS/UEFI Config Which Switchports are used vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) If you compare it with a house, the physical architecture is the CAD Drawing:\nOutput: LLD (Low Level Design), rack diagrams, IP sheets, runbooks.\nConceptual Design the WHY\nThe conceptual design is the opposite of the physical design, is all about the why - the \u0026quot;owners perspective\u0026quot;. No vendor names, no specs. Pure intent and requirements.\n\u0026quot;We need high availability for critical workloads\u0026quot; \u0026quot;Development environments must be isolated from production\u0026quot; \u0026quot;We need DR capabilities with RTO \u0026lt; 4h\u0026quot; \u0026quot;Infrastructure must scale to support 500 VMs over 3 years\u0026quot;\nLogical Design The logical design glues the physical and conceptual design together. The logical design translates conceptual fuzziness into a coherent architectural blueprint â€“ it's the architect's perspective made tangible. It's where design decisions get weighed: benefits against risks, trade-offs made explicit, alternatives considered and dismissed with reasoning. Without it, the gap between intent and implementation becomes dangerous. A conceptual design alone leaves too many valid technical solutions on the table with no basis for choosing between them. A physical design alone is a set of instructions without justification â€“ there's no traceability back to business requirements, and no one can explain why things were built the way they were.\nThat's what makes the logical design arguably the most critical layer of any architecture. It sits at the intersection of business and engineering, speaking to both audiences simultaneously. The owner sees their requirements reflected in structure and decision rationale. The engineer sees a clear, justified blueprint to build from. It's the one artifact that both parties can read, understand, and hold each other accountable to.\nIts Vendor-aware but not yet hardware-specific.\nvSphere cluster topology: how many clusters, what separation (management / edge / compute) vSAN vs. shared storage decision NSX segment and tier layout: T0/T1 design, overlay vs. underlay separation Network zones: management, vMotion, vSAN, VM traffic â€“ VLANs defined but no physical ports yet vCenter hierarchy: datacenter objects, host folders, resource pools, tagging strategy DRS / HA policies per cluster DR strategy: Stretched cluster vs. SRM vs. Zerto Output: Logical diagrams, VLAN tables, cluster design docs â€“ the classic VMware HLD (High Level Design)\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defines HOW the system is built. A few examples for the physical design:\nspecific server models and their hardware layout, BIOS/UEFI Config Which Switchports are used vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) If you compare it with a house, the physical architecture is the CAD Drawing:\nOutput: LLD (Low Level Design), rack diagrams, IP sheets, runbooks.\nConceptual Design the WHY\nThe conceptual design is the opposite of the physical design, is all about the why - the \u0026quot;owners perspective\u0026quot;. No vendor names, no specs. Pure intent and requirements.\n\u0026quot;We need high availability for critical workloads\u0026quot; \u0026quot;Development environments must be isolated from production\u0026quot; \u0026quot;We need DR capabilities with RTO \u0026lt; 4h\u0026quot; \u0026quot;Infrastructure must scale to support 500 VMs over 3 years\u0026quot;\nLogical Design The logical design glues the physical and conceptual design together. The logical design turns vague ideas into a clear architectural blueprint â€“ this is the architect's perspective.\nIt's where design decisions are made explicit: what are the benefits, what are the risks, why was one approach chosen over another.\nWithout a logical design, things break down in both directions. A conceptual design alone leaves too many possible technical solutions open â€“ there's no basis for choosing between them. A physical design alone is just a set of instructions with no justification behind them â€“ no one can trace back why things were built that way or connect them to any business requirement. That's why the logical design is arguably the most important part of any architecture. It bridges the gap between business and engineering. The owner can read it and see their requirements reflected. The engineer can read it and understand exactly what to build and why. It's the one layer that both sides can understand and relate to.\nIts Vendor-aware but not yet hardware-specific.\nvSphere cluster topology: how many clusters, what separation (management / edge / compute) vSAN vs. shared storage decision NSX segment and tier layout: T0/T1 design, overlay vs. underlay separation Network zones: management, vMotion, vSAN, VM traffic â€“ VLANs defined but no physical ports yet vCenter hierarchy: datacenter objects, host folders, resource pools, tagging strategy DRS / HA policies per cluster DR strategy: Stretched cluster vs. SRM vs. Zerto Output: Logical diagrams, VLAN tables, cluster design docs â€“ the classic VMware HLD (High Level Design)\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defines HOW the system is built. A few examples for the physical design:\nspecific server models and their hardware layout, BIOS/UEFI Config Which Switchports are used vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) If you compare it with a house, the physical architecture is the CAD Drawing:\nOutput: LLD (Low Level Design), rack diagrams, IP sheets, runbooks.\nConceptual Design the WHY\nThe conceptual design is the opposite of the physical design, is all about the why - the \u0026quot;owners perspective\u0026quot;. No vendor names, no specs. Pure intent and requirements.\n\u0026quot;We need high availability for critical workloads\u0026quot; \u0026quot;Development environments must be isolated from production\u0026quot; \u0026quot;We need DR capabilities with RTO \u0026lt; 4h\u0026quot; \u0026quot;Infrastructure must scale to support 500 VMs over 3 years\u0026quot;\nLogical Design The logical design glues the physical and conceptual design together. The logical design turns vague ideas into a clear architectural blueprint â€“ this is the architect's perspective.\nIt's where design decisions are made explicit: what are the benefits, what are the risks, why was one approach chosen over another.\nWithout a logical design, things break down in both directions. A conceptual design alone leaves too many possible technical solutions open â€“ there's no basis for choosing between them. A physical design alone is just a set of instructions with no justification behind them â€“ no one can trace back why things were built that way or connect them to any business requirement. That's why the logical design is arguably the most important part of any architecture. It bridges the gap between business and engineering. The owner can read it and see their requirements reflected. The engineer can read it and understand exactly what to build and why. It's the one layer that both sides can understand and relate to.\nIts Vendor-aware but not yet hardware-specific.\nvSphere cluster topology: how many clusters, what separation (management / edge / compute) vSAN vs. shared storage decision NSX segment and tier layout: T0/T1 design, overlay vs. underlay separation Network zones: management, vMotion, vSAN, VM traffic â€“ VLANs defined but no physical ports yet vCenter hierarchy: datacenter objects, host folders, resource pools, tagging strategy DRS / HA policies per cluster DR strategy: Stretched cluster vs. SRM vs. Zerto Output: Logical diagrams, VLAN tables, cluster design docs â€“ the classic VMware HLD (High Level Design)\nImportant Categories ","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"Insert Lead paragraph here.\nIn Technical Architecture the three-tier oder conceptual model comes often into play. You probably stumbled into a few key words when reading VMware Docs or Reference Guides. Words like:\nlogcal design physical design conceptual design The three Layers Physical Design the HOW\nThe physical design is probably the easiest layer where us engineers can relate most. It defines HOW the system is built. A few examples for the physical design:\nspecific server models and their hardware layout, BIOS/UEFI Config Which Switchports are used vSAN Disk Layout Physical Rack Layout etc. configuration details (like HA, DRS, ESX Advanced Settings) If you compare it with a house, the physical architecture is the CAD Drawing:\nOutput: LLD (Low Level Design), rack diagrams, IP sheets, runbooks.\nConceptual Design the WHY\nThe conceptual design is the opposite of the physical design, is all about the why - the \u0026quot;owners perspective\u0026quot;. No vendor names, no specs. Pure intent and requirements.\n\u0026quot;We need high availability for critical workloads\u0026quot; \u0026quot;Development environments must be isolated from production\u0026quot; \u0026quot;We need DR capabilities with RTO \u0026lt; 4h\u0026quot; \u0026quot;Infrastructure must scale to support 500 VMs over 3 years\u0026quot;\nLogical Design The logical design glues the physical and conceptual design together. The logical design turns vague ideas into a clear architectural blueprint â€“ this is the architect's perspective.\nIt's where design decisions are made explicit: what are the benefits, what are the risks, why was one approach chosen over another.\nWithout a logical design, things break down in both directions. A conceptual design alone leaves too many possible technical solutions open â€“ there's no basis for choosing between them. A physical design alone is just a set of instructions with no justification behind them â€“ no one can trace back why things were built that way or connect them to any business requirement. That's why the logical design is arguably the most important part of any architecture. It bridges the gap between business and engineering. The owner can read it and see their requirements reflected. The engineer can read it and understand exactly what to build and why. It's the one layer that both sides can understand and relate to.\nIts Vendor-aware but not yet hardware-specific.\nvSphere cluster topology: how many clusters, what separation (management / edge / compute) vSAN vs. shared storage decision NSX segment and tier layout: T0/T1 design, overlay vs. underlay separation Network zones: management, vMotion, vSAN, VM traffic â€“ VLANs defined but no physical ports yet vCenter hierarchy: datacenter objects, host folders, resource pools, tagging strategy DRS / HA policies per cluster DR strategy: Stretched cluster vs. SRM vs. Zerto Output: Logical diagrams, VLAN tables, cluster design docs â€“ the classic VMware HLD (High Level Design)\nImportant Categories Now that we have the three layers whats very important and what should always be documented in a project are the following categories:\nRequirements (Functional and non-functional) Contraints Assumptions Risks When working on a project design decisions should\n","link":"//localhost:1313/post/design-methodology/","section":"post","tags":["Architecture","Design"],"title":"Design Methodology"},{"body":"VMware Data Services Manager - K8s Operator VMware DSM provisions Data Services like Postgres, MySQL and MSSQL. You can consume DSM in three ways:\nDSM UI VCF Automation K8s Operator DSM Installation is straightforward, just grab the OVA and install it. DSM will install a Plugin in vCenter and the DSM UI will be accessible with the IP you configured on the OVA setup. Nothing special needed, no NSX, no vSAN.\nScenario 1 - Postgres HA Cluster We have the scenario that we have a Kubernetes cluster and some apps need a PostgreSQL database. In general, it is a good idea to have your persistent data outside of your k8s cluster (S3, external DB, etc.). You could have Postgres running on the same k8s cluster as your other apps, but that will be more of a build-your-own solution which will have its drawbacks.\nDSM UI If you never used DSM the UI will give you a great overview. Lets provision a highly available PostgreSQL Cluster.\nSelect the Topology (Cluster = 3 VMs)\nSelect the Infrastructure Policy (vSphere Cluster, Portgroup, Storage Policy)\nConfigure Maintenance Windows and optional pg_hba parameters.\nDSM will now provision three VMs via vCenter and set up PostgreSQL within those three VMs (note: it will leverage Kubernetes for that).\nDSM K8s Operator Now we will provision a PostgreSQL Cluster via K8s Operator thats running inside our Kubernetes Cluster.\nInstall K8s Consumption Operator - DSM 2.x Login to vSphere Kubernetes Cluster (VKS)\nkubectl vsphere login --server=https://172.29.30.2 --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name cl-lab-vbuenzli-kcl-01 --tanzu-kubernetes-cluster-namespace cl-lab-vbuenzli --vsphere-username \u0026#34;vbuenzli@sddc.lab\u0026#34; create k8s namespace for dsm operator\nkubectl create namespace dsm-consumption-operator-system We are directly using the default registry, such as Broadcom Artifactory, you don't need authentication. We create a registry secret using the following command, ignoring the username and password values:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=ignore \\ --docker-username=ignore \\ --docker-password=ignore If you are using your own internal registry, where the consumption operator image exists, you need to provide those credentials here:\nkubectl -n dsm-consumption-operator-system create secret docker-registry registry-creds \\ --docker-server=\u0026lt;DOCKER_REGISTRY\u0026gt; \\ --docker-username=\u0026lt;REGISTRY_USERNAME\u0026gt; \\ --docker-password=\u0026lt;REGISTRY_PASSWORD\u0026gt; Create an authentication secret that includes all the information needed to connect to the VMware Data Services Manager provider:\nkubectl -n dsm-consumption-operator-system create secret generic dsm-auth-creds \\ --from-file=root_ca=dsm-root-ca \\ --from-literal=dsm_user=admin@dsm \\ --from-literal=dsm_password=\u0026#39;VMware1!\u0026#39; \\ --from-literal=dsm_endpoint=https://172.29.30.51 Pull the helm chart from registry and unpack it in a directory and deploy to cluster.\nAdd your Backup Locations and Infrastructure Policies to the values-override.yaml. Infrastructure and Backup Locations can be created and viewed in the DSM UI.\nvalues-override.yaml\nimagePullSecret: registry-creds replicas: 1 image: name: projects.packages.broadcom.com/dsm-consumption-operator/consumption-operator tag: 2.2.0 dsm: authSecretName: dsm-auth-creds # allowedInfrastructurePolicies is a mandatory field that needs to be filled with allowed infrastructure policies for the given consumption cluster allowedInfrastructurePolicies: - labvce3-dsm-ip01 # allowedBackupLocations is a mandatory field that holds a list of backup locations that can be used by database clusters created in this consumption cluster allowedBackupLocations: - vsan-datastore - artesca-lab # list of infraPolicies and backupLocations that need to be applied to all namespaces that match the given selector applyToNamespaces: selector: matchAnnotations: {} infrastructurePolicies: [] backupLocations: [] # adminNamespace specifies where administrative DSM system objects # should be stored in the consumption cluster. When not set, administrative # objects will not be available in the consumption cluster adminNamespace: \u0026#34;dsm-consumption-operator-system\u0026#34; # consumptionClusterName is an optional name that you can provide to identify the Kubernetes cluster where the operator is deployed consumptionClusterName: \u0026#34;infra01\u0026#34; # psp field allows you to deploy the operator on pod security policies-enabled Kubernetes cluster (ONLY for k8s version \u0026lt; 1.25). # Set psp.required to true and provide the ClusterRole corresponding to the restricted policy. psp: required: false role: \u0026#34;\u0026#34; helm pull oci://projects.packages.broadcom.com/dsm-consumption-operator/dsm-consumption-operator --version 2.2.0 -d consumption/ --untar helm upgrade --install dsm-consumption-operator consumption/dsm-consumption-operator -f values-override.yaml --namespace dsm-consumption-operator-system Check Logs:\nkubectl logs -n dsm-consumption-operator-system deployments/dsm-consumption-operator-controller-manager -f Install K8s Consumption Operator - DSM 9.0.1 In DSM 9.0.1, two things have changed regarding the k8s operator. See the Vendor Docs [2]\nFor VMware Data Services Manager version 9.0.1 and later, add a dataServicePolicyNamespaceLabels field to the values.yaml file. For VMware Data Services Manager version 9.0.1 and later, you create data service policies in the VMware Data Services Manager gateway. Create PostgreSQL Cluster via K8s Consumption Operator Create PostgresCluster Manifest:\n--- apiVersion: v1 kind: Namespace metadata: name: dev-team --- apiVersion: infrastructure.dataservices.vmware.com/v1alpha1 kind: InfrastructurePolicyBinding metadata: name: labvce3-dsm-ip01 namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: BackupLocationBinding metadata: name: artesca-lab namespace: dev-team --- apiVersion: databases.dataservices.vmware.com/v1alpha1 kind: PostgresCluster metadata: name: pg-dev-cluster namespace: dev-team spec: replicas: 3 version: \u0026#34;14\u0026#34; vmClass: name: medium storageSpace: 20Gi infrastructurePolicy: name: labvce3-dsm-ip01 storagePolicyName: sp-dsm-e3 backupConfig: backupRetentionDays: 91 schedules: - name: full-weekly type: full schedule: \u0026#34;0 0 * * 0\u0026#34; - name: incremental-daily type: incremental schedule: \u0026#34;30 10 * * *\u0026#34; backupLocation: name: artesca-lab Apply it:\nkubectl apply -f user-namespace-example.yaml export K8S_NAMESPACE=dev-team View available infrastructure policies by running the following command:\nkubectl get infrastructurepolicybinding -n $K8S_NAMESPACE You can also check the status field of each infrapolicybinding to find out the values of vmClass, storagePolicy, and so on.\nCheck status of ongoing deployment:\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.conditions}\u0026#39; | jq Wait for:\n{ \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2025-06-26T06:24:40Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;reason\u0026#34;: \u0026#34;ConfigApplied\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CustomConfigStatus\u0026#34; } Retrieve Connection Information\nkubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection}\u0026#39; | jq { \u0026#34;dbname\u0026#34;: \u0026#34;pg-dev-cluster\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;10.177.150.86\u0026#34;, \u0026#34;passwordRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pg-pg-dev-cluster\u0026#34; }, \u0026#34;port\u0026#34;: 5432, \u0026#34;username\u0026#34;: \u0026#34;pgadmin\u0026#34; } Retrieve Connection Vars:\nPASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Connect to the DB:\nPGPASSWORD=$PASSWORD psql -h $HOST -U $USER $DBNAME Test Deployment Patch existing secret # Define variables K8S_NAMESPACE=\u0026#34;dev-team\u0026#34; CLUSTER_NAME=\u0026#34;pg-dev-cluster\u0026#34; SECRET_NAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE $CLUSTER_NAME -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) # The secret you want to patch # Fetch the connection info as a JSON object CONNECTION_INFO=$(kubectl get postgresclusters.databases.dataservices.vmware.com \\ -n \u0026#34;$K8S_NAMESPACE\u0026#34; \u0026#34;$CLUSTER_NAME\u0026#34; \\ -o jsonpath=\u0026#39;{.status.connection}\u0026#39;) # Check if the command succeeded if [ -z \u0026#34;$CONNECTION_INFO\u0026#34; ]; then echo \u0026#34;Error: Could not retrieve connection info for cluster \u0026#39;$CLUSTER_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;.\u0026#34; exit 1 fi # Extract values using jq HOST=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.host\u0026#39;) PORT=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.port\u0026#39;) DBNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.dbname\u0026#39;) USERNAME=$(echo \u0026#34;$CONNECTION_INFO\u0026#34; | jq -r \u0026#39;.username\u0026#39;) # Construct the JSON patch payload using stringData PATCH_PAYLOAD=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;stringData\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;$HOST\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;$PORT\u0026#34;, \u0026#34;dbname\u0026#34;: \u0026#34;$DBNAME\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;$USERNAME\u0026#34; } } EOF ) # Apply the patch to the secret echo \u0026#34;Patching secret \u0026#39;$SECRET_NAME\u0026#39; in namespace \u0026#39;$K8S_NAMESPACE\u0026#39;...\u0026#34; kubectl patch secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; --type=merge -p \u0026#34;$PATCH_PAYLOAD\u0026#34; printf \u0026#34;Patch complete. \\n Secret: \\n $(kubectl get secret \u0026#34;$SECRET_NAME\u0026#34; -n \u0026#34;$K8S_NAMESPACE\u0026#34; -o yaml ) \\n\u0026#34; # create new secret kubectl create secret generic \u0026#34;pg-demo\u0026#34; --from-literal=host=$HOST --from-literal=port=$PORT --from-literal=dbname=$DBNAME --from-literal=username=$USERNAME -n \u0026#34;$K8S_NAMESPACE\u0026#34; --from-literal=password=$PASSWORD Test Application Retrieve Connection Vars:\nexport PG_PASSWORD=$(kubectl -n $K8S_NAMESPACE get secrets/$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.passwordRef.name}\u0026#39;) --template={{.data.password}} | base64 -d) export PG_USER=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.username}\u0026#39;) export PG_HOST=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.host}\u0026#39;) export PG_DBNAME=$(kubectl get postgresclusters.databases.dataservices.vmware.com -n $K8S_NAMESPACE pg-dev-cluster -o jsonpath=\u0026#39;{.status.connection.dbname}\u0026#39;) Label the namespace to allow workload without matching security context:\nkubectl label namespace $K8S_NAMESPACE \\ pod-security.kubernetes.io/audit=privileged \\ pod-security.kubernetes.io/warn=privileged \\ pod-security.kubernetes.io/enforce=privileged \\ --overwrite Test App Manifest (dsm-test-deployment.yaml)\napiVersion: apps/v1 kind: Deployment metadata: name: pgtestapp spec: replicas: 1 selector: matchLabels: app: pgtestapp template: metadata: labels: app: pgtestapp spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-pg-dev-cluster key: password --- apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoidHJpYm9jayIsInBhc3N3b3JkIjoiZ2hwX1hpVmYxZFVGYUdCVDRzeHo5NHRUZTVDeGNIRjh5djNMUG5YOSIsImVtYWlsIjoiY2ljZEBzb3VsdGVjLmNoIiwiYXV0aCI6ImRISnBZbTlqYXpwbmFIQmZXR2xXWmpGa1ZVWmhSMEpVTkhONGVqazBkRlJsTlVONFkwaEdPSGwyTTB4UWJsZzUifX19 kind: Secret metadata: creationTimestamp: null name: regcred type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Service metadata: name: pgtestapp spec: ports: - port: 8080 selector: app: pgtestapp --- apiVersion: apps/v1 kind: Deployment metadata: name: pg-demo spec: replicas: 1 selector: matchLabels: app: pg-demo template: metadata: labels: app: pg-demo spec: imagePullSecrets: - name: regcred containers: - name: pgtestapp image: ghcr.io/kastenhq/pgtest:v0.0.1 imagePullPolicy: Always # command: # - \u0026#34;/bin/sh\u0026#34; # - \u0026#34;-c\u0026#34; # - | # echo \u0026#34;PG_HOST: $PG_HOST\u0026#34; # echo \u0026#34;PG_DBNAME: $PG_DBNAME\u0026#34; # echo \u0026#34;PG_USER: $PG_USER\u0026#34; # echo \u0026#34;PG_PASSWORD: $PG_PASSWORD\u0026#34; # tail -f /dev/null ports: - containerPort: 8080 env: - name: PG_HOST valueFrom: secretKeyRef: name: pg-demo key: host - name: PG_DBNAME valueFrom: secretKeyRef: name: pg-demo key: dbname - name: PG_USER valueFrom: secretKeyRef: name: pg-demo key: username - name: PG_PASSWORD valueFrom: secretKeyRef: name: pg-demo key: password Deploy the Application:\nkubectl apply -f dsm-test-deployment.yaml -n $K8S_NAMESPACE Docker Image used for Testing: https://github.com/kastenhq/pgtest\nTerraform If you want to use terraform with DSM have a look at:\nhttps://blogs.vmware.com/cloud-foundation/2025/05/13/database-as-code-with-data-services-manager-and-terraform/ https://github.com/xfirestyle2k/VCF-Data-Services-Terraform You can use the kubernetes terraform provider with DSM!\nSummary Now we have successfully deployed a PostgreSQL Cluster with the DSM K8s Operator (from our Kubernetes Guest Cluster) and deployed a sample application that used the dsm-provisioned database.\nIn the next blog we are going to have a look at the third option on how to consume db's via DSM, VCF Automation (VCFA)\nResources [1] DSM Intro - VMware Blog from my friend Thomas\n[2] DSM K8s Operator - Vendor Doc\n[3] DSM MSSQL - First Look\n","link":"//localhost:1313/post/dsm-9-k8s/","section":"post","tags":["DSM","Data Services Manager","PostgreSQL"],"title":"Data Services Manager - K8s "}]