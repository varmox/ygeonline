[{"body":"","link":"/","section":"","tags":null,"title":""},{"body":"","link":"/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"/tags/vcenter/","section":"tags","tags":null,"title":"vCenter"},{"body":"","link":"/tags/vcf/","section":"tags","tags":null,"title":"VCF"},{"body":"","link":"/categories/vmware/","section":"categories","tags":null,"title":"VMware"},{"body":"vCenter Troubleshooting Guide Log Files Networking Manual Network Config DNS *** Flush DNS ***\n1 2systemctl¬†restart¬†systemd‚Äìresolved.service 3 4systemctl restart dnsmasq HTTP Proxy Certificate Management Renew all vCenter Machine Certificates vLCM *** Cannot sync depot ***\n1 2/var/log/vmware/vmware-updatemgr/vum-server/vmware-vum-server.log ","link":"/post/vcenter-troubleshooting/","section":"post","tags":["vCenter","VCF"],"title":"VMware vCenter Troubleshooting Guide"},{"body":"","link":"/categories/vcf/","section":"categories","tags":null,"title":"VCF"},{"body":"","link":"/tags/vmware/","section":"tags","tags":null,"title":"VMware"},{"body":"The new VCF 5.2 Update delivers quite a few interesting updates. This blog post covers expecially the updates and improvements for Tanzu.\nPart 1 covers new Feature within Tanzu on vSphere 8 Update 3.\nvSphere with Tanzu Update -\u0026gt; vSphere IaaS Control Plane vSphere with Tanzu is pass√© - the new naming is vSphere IaaS Control Plane.\nThe naming already suggest - not only Kubernetes Cluster can be deployed, also VMs (this was possible a long time but with the new naming the focus is not only on k8s). Via the vm-operator you can deploy VMs alongside Tanzu Kubernetes Cluster. The IaaS Control Plane is really interesting as you can deploy VMs with Code. A YAML File will describe your VM. For that the VM Class now support much more configuration, it handled like a normal VM provisioned through vSphere UI.\nIt is even possible to deploy Windows Workloads via VM-Operator with sysprep! (blog to follow)\nDeployment of Windows Server within a ArgoCD Pipeline is possible (if you would want that ;))\nNow also the Backup via vSphere Storage APIs for Data Protection (VADP) for VMs deployed via vm-operator are supportet (as \u0026quot;normal\u0026quot; VMs) - within Veeam you can just backup the whole vSphere Namespace (with is basically a advanced vSphere Ressource Pool)\nLCI - Local consumption Interface With the new Local Consumption Interface within vSphere UI you can deploy Tanzu Kubernetes Clusters or VMs - both via IaaS Control Plane with a GUI!\nSurely some of you already knew the Cloud Consumption Interface (CCI) within Aria - Local Cloud Consumption Interface is a built in UI within vSphere UI (no need to deploy Aria)\nLCI Installation LCI is a supervisor Service. Supervisor Services are deployed directly to the Supervisor instead of deploying to a TKGS Cluster.\nhttps://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-with-tanzu-services-workloads/GUID-4843E6C6-747E-43B1-AC55-8F02299CC10E.html https://vsphere-tmm.github.io/Supervisor-Services/#consumption-interface Download Link of LCI YAML: https://vmwaresaas.jfrog.io/ui/native/supervisor-services/cci-supervisor-service/v1.0.0/cci-supervisor-service.yml\nvSAN streched Cluster support for Tanzu Kubernetes Grid Service (TKGs) With vSphere 8 Update 3 TKGS now supports vSAN streched Clusters. Interesting is that Kubernetes Control Plane Nodes and the Supervisor Control Plane Nodes should be kept at one site within a vSAN streched Clusters. This is due to etcd. etcd requires more than half of the replicas to be available at any time.\nThe 3 Supervisor Control Plane VMs should be placed in the same site All the Control Plane VMs of any given TKGs cluster should be placed in the same site Worker Nodes can be streched acroos two sites Tanzu Kubernetes Cluster Autoscaling As the name already suggest: Autoscaling of Tanzu Kubernetes Workers Nodes (VMs)! Only requirements is to have TKR Relase 1.25\nLinks [1] https://blogs.vmware.com/cloud-foundation/2024/06/25/vmware-cloud-foundation-launch/\n[2] https://core.vmware.com/resource/whats-new-vsphere-update-3-vsphere-iaas-control-plane\n[3] https://docs.vmware.com/en/VMware-vSphere/8.0/rn/vmware-vsphere-with-tanzu-80-release-notes/index.html\n","link":"/post/vcf52/","section":"post","tags":["VMware","VCF","VMware Tanzu"],"title":"VMware Cloud Foundation 5.2 - Part 1"},{"body":"","link":"/tags/vmware-tanzu/","section":"tags","tags":null,"title":"VMware Tanzu"},{"body":"","link":"/tags/devsecops/","section":"tags","tags":null,"title":"DevSecOps"},{"body":"Introduction Due to the rise of containerized applications the new word DevOps or DevSecOps.\nSo what is DevOps?\nBasically the developer should also build \u0026amp; operate the infrastructure on which is code runs on.\nAnd DevSecOps?\nWe need Security, so adapt DevOps Practises also for Security. Devs should also be resonsible to securing their infrastructure.\nWhat are the goals?\nThe main Goal for DevOps Practises is to shorten time of the SDLC (Systems Develoopment Lifecyle). Basically be faster to ship code into production.\nProblems DevSecOps is not a person DevSecOps covers a whole lot of topics. Tooo much to handle for a single person.\nI've noticed a theme with security teams: they take a while to adapt. In the days before the Internet, security was not a priority because it was never a serious concern. These teams were established only when internet adoption began to spread, and they were incorporated into the early waterfall processes because it was how infrastructure teams and development teams collaborated.\nDevelopers aren't Security or Infrastructure Experts One of\n","link":"/post/devsecops/","section":"post","tags":["DevSecOps"],"title":"DevSecOps"},{"body":"","link":"/categories/my-2-cents/","section":"categories","tags":null,"title":"my 2 cents"},{"body":"","link":"/tags/hardware/","section":"tags","tags":null,"title":"Hardware"},{"body":"","link":"/tags/hpe/","section":"tags","tags":null,"title":"HPE"},{"body":"Problem\nRecently I got my hands on some old HPE 3PAR 1.92TB SAS SSDs (Samsung PM1633a). I though they would be perfect for my SDDC home lab - sadly they did not work as intended with my HPE DL360 server. The SmartArray P420i was recognizing the disks but was giving the error \u0026quot;This physical drive does not support RAID and is not exposed to OS. It cannot be used for configuration on this controller\u0026quot;. But hopefully there is a solution to the problem we are facing:\nBasically the drives in a 3PAR Storage System use 520Bytes blocks as a low-level formatting. We need 512 Bytes for standard Operating Systems like Linux (520 Bytes does enable T10 DIF CRC error checking - the extra 8 Bytes are designated for data integrity/protection [1])\nSolution\nTL;DR:\nconnect to SAS Drives to a HBA or RAID-Controller in HBA Mode Install a OS (Windows or Linux) on a seperate, already working drive - or just use a live version install sg3_utils package [2] run ùô®ùôú_ùô®ùôòùôñùô£ and ùíîùíà_ùíáùíêùíìùíéùíÇùíï --ùíáùíêùíìùíéùíÇùíï --ùíîùíäùíõùíÜ 512 ùë∑ùë´ùíô to reformat the disks to 512 Byte blocks The first step is to install the disk in a system with only a simple SAS HBA - most raid adapters will give you errors when you want to expose the disk to the OS. We need SCSI Access from a OS to do some commands. I used a HPE P420i in HBA Mode (Note: to be able to set your RAID Controller to HBA Mode, no logical drives should be present)\nNext install a OS on a already working drive, I used Rocky Linux - a live version also works fine. To be able to format the drives we need sg3_utils. Simply install the package with dnf:\n1sudo dnf install sg3_utils Then with the help of sg3_utils package we run some commands. First we need to list all SCSI drives and get their ID (PDx)\nWindows\n1sg_scan Then format one drive at a time (in this case PD1)\n1sg_format --format --size 512 PD1 Linux\n1sg_scan -i Then format one drive at a time (in this case sg3)\n1sg_format --format --size 512 /dev/sg3 Now the disks are formatted with 512 byte blocks and you should be able to use the disk in a JBOD or RAID.\nReferences\n[1] https://www.openfabrics.org/images/2018workshop/presentations/307_TOved_T10-DIFOffload.pdf\n[2] https://sg.danny.cz/sg/sg3_utils.html\n","link":"/post/reformatting-drives/","section":"post","tags":["Hardware","HPE"],"title":"Reformatting enterprise storage drives to 512 bytes"},{"body":"","link":"/categories/technology/","section":"categories","tags":null,"title":"Technology"},{"body":"","link":"/post/tanzu-mission-control-self-managed/","section":"post","tags":null,"title":"Tanzu Mission Control Self Managed"},{"body":"","link":"/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"","link":"/post/tanzu-cluster-creation/","section":"post","tags":["VMware","vSphere","VMware Tanzu","Kubernetes"],"title":"Tanzu with vSphere and Antrea: Cluster Creation"},{"body":"","link":"/tags/vsphere/","section":"tags","tags":null,"title":"vSphere"},{"body":"","link":"/categories/it/","section":"categories","tags":null,"title":"IT"},{"body":"","link":"/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"","link":"/tags/vmware-tanzu-kubernetes-grid/","section":"tags","tags":null,"title":"VMware Tanzu Kubernetes Grid"},{"body":"Intro This blog post is about getting started with VMware Tanzu Kubernetes Grid (TKG). TKG is used to deploy Tanzu Kubernetes Cluster to various Cloud Providers including vSphere, AWS and Azure. First, a management cluster (based on Docker and kind) is created from a Bootstrap Workstation, which itself then provides its web interface and additional CLI tools to create a Kubernetes cluster.\nTKG has no integration in vSphere UI. If you wan't to integrate Tanzu with vCenter/vSphere UI - use vSphere with Tanzu instead.\nInstall Tanzu Kubernetes Grid Prerequieries\nVMware vSphere 8 (7 U3 should work, in this tutorial we are using vSphere 8) Bootstrap VM or Workstation with Docker Access to VMware Customer Connect TKG Bootstrap Machine on Fedora See this blog post to create a Bootstrap Machine based on Fedora\nCreate Management Cluster Create the Management Cluster on your Machine:\nCreate a Tanzu Management Cluster to boostrap TKG\n1tanzu management-cluster create --ui ","link":"/post/tkg-getting-started/","section":"post","tags":["Kubernetes","VMware","VMware Tanzu","VMware Tanzu Kubernetes Grid"],"title":"VMware Tanzu Kubernetes Grid -  getting started"},{"body":"Cillium Gateway API The Cilium Gateway API is a Kubernetes API that provides a more powerful and flexible way to manage traffic routing than the traditional Ingress API of (vanilla) Kubernetes. It is a set of resources that model service networking in Kubernetes, and is designed to be role-oriented, portable, expressive, and extensible. The Cilium Service Mesh Gateway API Controller requires the ability to create LoadBalancer Kubernetes services.\n","link":"/post/cillium-getting-started/","section":"post","tags":["Kubernetes Cillium","CNI"],"title":"Cillium Gateway API - Getting Started"},{"body":"","link":"/tags/cni/","section":"tags","tags":null,"title":"CNI"},{"body":"","link":"/tags/kubernetes-cillium/","section":"tags","tags":null,"title":"Kubernetes Cillium"},{"body":"","link":"/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"","link":"/tags/fedora/","section":"tags","tags":null,"title":"Fedora"},{"body":"","link":"/tags/tanzu-kubernetes-grid/","section":"tags","tags":null,"title":"Tanzu Kubernetes Grid"},{"body":"TKG Bootstrap Machine on Fedora Tanzu Kubernetes Grid needs a workstation to bootstrap your Kubernetes Cluster. This is a short guide for a Fedora based bootstrap Machine:\nRequirements\nAbility to run Docker Access to download Tanzu CLI (via Customer Connect Portal) dedicated admin user in the wheel-group Install Docker\nDocker on modern RHEL systems can be confusing because the latest versions of RHEL replace Docker with Podman (dnf install docker will install podman). I should work with podman, but we explicity are going to install Docker Container Engine.\nLogin to your system (via ssh) with the admin user.\nAdd Repo and Install Docker CE\n1sudo dnf -y install dnf-plugins-core 2sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo 3sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Start \u0026amp; enable Docker\n1sudo systemctl start docker \u0026amp;\u0026amp; sudo systemctl enable docker Set Docker Socket context to current user. This step is crucial, to allow the current user to access the docker socket as tanzu cli should run with the current user contexts not sudo.\n1sudo chown $USER:docker /var/run/docker.sock You can also install docker-desktop if you want to have a GUI.\nInstall Tanzu CLI and Plugin\n1cat \u0026lt;\u0026lt; EOF | sudo tee /etc/yum.repos.d/tanzu-cli.repo 2[tanzu-cli] 3name=Tanzu CLI 4baseurl=https://storage.googleapis.com/tanzu-cli-os-packages/rpm/tanzu-cli 5enabled=1 6gpgcheck=1 7repo_gpgcheck=1 8gpgkey=https://packages.vmware.com/tools/keys/VMWARE-PACKAGING-GPG-RSA-KEY.pub 9EOF 10 11sudo dnf install -y tanzu-cli 1tanzu plugin install --group vmware-tkg/default:v2.4.1 Install VMware kubectl\nGo to VMware Customer Connect and Download VMWare kubectl. https://customerconnect.vmware.com/downloads/get-download?downloadGroup=TKG-241)\nUnzip the package and make it executable\n1chmod ugo+x kubectl-linux-v1.27.5+vmware.1 Install\n1sudo install kubectl-linux-v1.27.5+vmware.1 /usr/local/bin/kubectl Test the setup If everything worked you should be able to run the following commands without any issues:\nList your Tanzu CLI plugins\n1tanzu plugin list Create a Tanzu Management Cluster to boostrap TKG\n1tanzu management-cluster create --ui Troubleshooting Error \u0026quot;Docker prerequiries not met\u0026quot;\nThis is usually the case when the current user does not have access to docker. In the current User Session run:\n1docker ps -a If presented with \u0026quot;Got permission denied\u0026quot; try adding current user to the wheel (local admin group) group\n1sudo usermod -aG wheel $USER Then try again to create your tanzu management cluster.\n","link":"/post/tanzu-kubernetes-grid-workstation-setup/","section":"post","tags":["VMware Tanzu","Tanzu Kubernetes Grid","Fedora","Docker"],"title":"Tanzu Kubernetes Grid - Linux Workstation Setup"},{"body":"Intro VMware vSAN is a Hyperconverged Infrastructure Solution from VMware. It basically turns your servers direct attached disks into a storage cluster.\nGeneral Requirements 10 GbE network switches are the minimum requirement for all-flash vSAN Clusters 25 GBE network switches are recommended Switches that support higher port speeds are designed with higher Network Processor Unit (NPU) buffers. An NPU shared switch buffer of at least 16 MB is recommended for 10 GbE network connectivity. An NPU buffer of at least 32 MB is recommended for more demanding 25 GbE network connectivity. With vSAN ESA 25GbE is a must. (In ESA-AF-0 vSAN Ready Nodes the network bandwidth has been mentioned as 10 GbE - so probalby 10GbE will work for smaller ESA Deployments)\nGeneral Considerations Use a dedicated VLAN on the Physical Network Switches for vSAN Traffic. If possible use Quality of Service Features to priorize vSAN traffic. Use two physical Uplinks for vSAN Traffic, do not mix with VM Workload traffic. Use a dedicated virtual Distributed Switch for vSphere Backend Services (vSAN, vMotion, FT) Use Network I/O Control vSAN over RoCE With vSphere 7.0 U2 vSAN with RDMA is supported. RDMA typically has lower CPU utilization and less I/O latency.\nIf your vSAN cluster will include adapters that support RoCE (RDMA over Converged Ethernet) for vSAN storage connectivity, the supporting network must support a ‚Äúlossless‚Äù transport. A ‚Äúlossless‚Äù network is defined as one where no frames are dropped because of network congestion.\nSelect switches that support Data Center Bridging (DCB). The Data Center Bridging feature supports the elimination of packet loss due to buffer or queue overflow. The Data Center Bridging must support bandwidth allocation based on priority settings, known as Class of Service (CoS). Priority Flow Control (PFC) is required on the switches to provide RoCE traffic a higher priority than other network traffic. All the nodes in the cluster connected to the common vSAN datastore must be configured with RoCE-supported adapter cards of the same vendor and the same model. This is strongly encouraged as a best practice to remove any possibility of slight variances in adapters from different vendors or different models disrupting I/O on the vSAN datastore.\nThe Ethernet ports on the RoCE-supported adapters are reserved for vSAN traffic only. The physical network supporting vSAN is configured as a ‚Äúlossless‚Äù network. Look for RoCEv2 support with your Network Switch Vendor. Supported RDMA Network adapters can be found here:\nhttps://www.vmware.com/resources/compatibility/search.php?deviceCategory=rdmanic\u0026amp;details=1\u0026amp;vsan_type=rdmanic\u0026amp;page=1\u0026amp;display_interval=10\u0026amp;sortColumn=Partner\u0026amp;sortOrder=Asc\n","link":"/post/vmware-vsan-network-requirements/","section":"post","tags":["VMware","vSAN","vSAN over RoCE"],"title":"VMware vSAN Network Considerations"},{"body":"","link":"/tags/vsan/","section":"tags","tags":null,"title":"vSAN"},{"body":"","link":"/tags/vsan-over-roce/","section":"tags","tags":null,"title":"vSAN over RoCE"},{"body":"","link":"/tags/monitoring/","section":"tags","tags":null,"title":"Monitoring"},{"body":"","link":"/tags/tig-stack/","section":"tags","tags":null,"title":"TIG Stack"},{"body":"vSphere Monitoring with TIG (Telegraf, InfluxDB, Grafana) This blog post describes a solution for monitoring SDDC infrastructure using Telegraf, InfluxDB, and Grafana. This solution is based on Docker and displays graphs and metrics via Grafana. All metrics are described in the telegraf.conf file.\nTL;DR: https://github.com/varmox/vsphere-monitoring.git\nPrerequiries RHEL based Linux Docker \u0026amp; Docker Compose (or Podman with Docker Compose. In this tutorial we are using docker \u0026amp; docker compose) installed 3 internal IPs for telegraf, grafana and influxdb containers. Access to vSphere API (read-only is sufficient) Environment Variables\nIn this tutorial we're using the following varibales, change them according to your setup:\nsubnet 172.29.30.0/23 Filesystem structure:\n/srv/tig\n‚îú‚îÄ‚îÄ docker-compose.yml\n‚îî‚îÄ‚îÄ telegraf.conf\nCreate Telegraf Config Edit your telegraf.conf file with a editor of choice and paste overwrite your telegraf.conf File. This config skips tls certificate checking so can be used for self-signed certificates.\n1[agent] 2 ## Default data collection interval for all inputs 3 interval = \u0026#34;10s\u0026#34; 4 ## Rounds collection interval to \u0026#39;interval\u0026#39; 5 ## ie, if interval=\u0026#34;10s\u0026#34; then always collect on :00, :10, :20, etc. 6 round_interval = true 7 8 ## Telegraf will send metrics to outputs in batches of at most 9 ## metric_batch_size metrics. 10 ## This controls the size of writes that Telegraf sends to output plugins. 11 metric_batch_size = 1000 12 13 ## Maximum number of unwritten metrics per output. Increasing this value 14 ## allows for longer periods of output downtime without dropping metrics at the 15 ## cost of higher maximum memory usage. 16 metric_buffer_limit = 10000 17 18 ## Collection jitter is used to jitter the collection by a random amount. 19 ## Each plugin will sleep for a random time within jitter before collecting. 20 ## This can be used to avoid many plugins querying things like sysfs at the 21 ## same time, which can have a measurable effect on the system. 22 collection_jitter = \u0026#34;0s\u0026#34; 23 24 ## Default flushing interval for all outputs. Maximum flush_interval will be 25 ## flush_interval + flush_jitter 26 flush_interval = \u0026#34;10s\u0026#34; 27 ## Jitter the flush interval by a random amount. This is primarily to avoid 28 ## large write spikes for users running a large number of telegraf instances. 29 ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s 30 flush_jitter = \u0026#34;0s\u0026#34; 31 32 ## By default or when set to \u0026#34;0s\u0026#34;, precision will be set to the same 33 ## timestamp order as the collection interval, with the maximum being 1s. 34 ## ie, when interval = \u0026#34;10s\u0026#34;, precision will be \u0026#34;1s\u0026#34; 35 ## when interval = \u0026#34;250ms\u0026#34;, precision will be \u0026#34;1ms\u0026#34; 36 ## Precision will NOT be used for service inputs. It is up to each individual 37 ## service input to set the timestamp at the appropriate precision. 38 ## Valid time units are \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;¬µs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;. 39 precision = \u0026#34;\u0026#34; 40 41 ## Log at debug level. 42 # debug = false 43 ## Log only error level messages. 44 # quiet = false 45 46 ## Log target controls the destination for logs and can be one of \u0026#34;file\u0026#34;, 47 ## \u0026#34;stderr\u0026#34; or, on Windows, \u0026#34;eventlog\u0026#34;. When set to \u0026#34;file\u0026#34;, the output file 48 ## is determined by the \u0026#34;logfile\u0026#34; setting. 49 # logtarget = \u0026#34;file\u0026#34; 50 51 ## Name of the file to be logged to when using the \u0026#34;file\u0026#34; logtarget. If set to 52 ## the empty string then logs are written to stderr. 53 # logfile = \u0026#34;\u0026#34; 54 55 ## The logfile will be rotated after the time interval specified. When set 56 ## to 0 no time based rotation is performed. Logs are rotated only when 57 ## written to, if there is no log activity rotation may be delayed. 58 # logfile_rotation_interval = \u0026#34;0d\u0026#34; 59 60 ## The logfile will be rotated when it becomes larger than the specified 61 ## size. When set to 0 no size based rotation is performed. 62 # logfile_rotation_max_size = \u0026#34;0MB\u0026#34; 63 64 ## Maximum number of rotated archives to keep, any older logs are deleted. 65 ## If set to -1, no archives are removed. 66 # logfile_rotation_max_archives = 5 67 68 ## Pick a timezone to use when logging or type \u0026#39;local\u0026#39; for local time. 69 ## Example: America/Chicago 70 # log_with_timezone = \u0026#34;\u0026#34; 71 72 ## Override default hostname, if empty use os.Hostname() 73 hostname = \u0026#34;\u0026#34; 74 ## If set to true, do no set the \u0026#34;host\u0026#34; tag in the telegraf agent. 75 omit_hostname = false 76[[outputs.influxdb_v2]] 77 ## The URLs of the InfluxDB cluster nodes. 78 ## 79 ## Multiple URLs can be specified for a single cluster, only ONE of the 80 ## urls will be written to each interval. 81 ## ex: urls = [\u0026#34;https://us-west-2-1.aws.cloud2.influxdata.com\u0026#34;] 82 urls = [\u0026#34;http://172.29.31.3:8086\u0026#34;] 83 84 ## Token for authentication. 85 token = \u0026#34;sFYmFqizqj8oJyJxuMc7l4PEztLIPOWfe0Aae_QwQiJXA-obkKo_AHCEgXRwMCEXXyYsq6mqRayan3Ylh_Dy0g==\u0026#34; 86 87 ## Organization is the name of the organization you wish to write to; must exist. 88 organization = \u0026#34;sddc\u0026#34; 89 90 ## Destination bucket to write into. 91 bucket = \u0026#34;vsphere\u0026#34; 92 93 ## The value of this tag will be used to determine the bucket. If this 94 ## tag is not set the \u0026#39;bucket\u0026#39; option is used as the default. 95 # bucket_tag = \u0026#34;\u0026#34; 96 97 ## If true, the bucket tag will not be added to the metric. 98 # exclude_bucket_tag = false 99 100 ## Timeout for HTTP messages. 101 # timeout = \u0026#34;5s\u0026#34; 102 103 ## Additional HTTP headers 104 # http_headers = {\u0026#34;X-Special-Header\u0026#34; = \u0026#34;Special-Value\u0026#34;} 105 106 ## HTTP Proxy override, if unset values the standard proxy environment 107 ## variables are consulted to determine which proxy, if any, should be used. 108 # http_proxy = \u0026#34;http://corporate.proxy:3128\u0026#34; 109 110 ## HTTP User-Agent 111 # user_agent = \u0026#34;telegraf\u0026#34; 112 113 ## Content-Encoding for write request body, can be set to \u0026#34;gzip\u0026#34; to 114 ## compress body or \u0026#34;identity\u0026#34; to apply no encoding. 115 # content_encoding = \u0026#34;gzip\u0026#34; 116 117 ## Enable or disable uint support for writing uints influxdb 2.0. 118 # influx_uint_support = false 119 120 ## Optional TLS Config for use on HTTP connections. 121 # tls_ca = \u0026#34;/etc/telegraf/ca.pem\u0026#34; 122 # tls_cert = \u0026#34;/etc/telegraf/cert.pem\u0026#34; 123 # tls_key = \u0026#34;/etc/telegraf/key.pem\u0026#34; 124 ## Use TLS but skip chain \u0026amp; host verification 125 # insecure_skip_verify = false 126# Read metrics from VMware vCenter 127[[inputs.vsphere]] 128 ## List of vCenter URLs to be monitored. These three lines must be uncommented 129 ## and edited for the plugin to work. 130 vcenters = [ \u0026#34;https://vcenter-fqdn/sdk\u0026#34; ] 131 username = \u0026#34;serviceaccount@sso.vsphere.local\u0026#34; 132 password = \u0026#34;secret\u0026#34; 133 134 ## VMs 135 ## Typical VM metrics (if omitted or empty, all metrics are collected) 136 # vm_include = [ \u0026#34;/*/vm/**\u0026#34;] # Inventory path to VMs to collect (by default all are collected) 137 # vm_exclude = [] # Inventory paths to exclude 138 vm_metric_include = [ 139 \u0026#34;cpu.demand.average\u0026#34;, 140 \u0026#34;cpu.idle.summation\u0026#34;, 141 \u0026#34;cpu.latency.average\u0026#34;, 142 \u0026#34;cpu.readiness.average\u0026#34;, 143 \u0026#34;cpu.ready.summation\u0026#34;, 144 \u0026#34;cpu.run.summation\u0026#34;, 145 \u0026#34;cpu.usagemhz.average\u0026#34;, 146 \u0026#34;cpu.used.summation\u0026#34;, 147 \u0026#34;cpu.wait.summation\u0026#34;, 148 \u0026#34;mem.active.average\u0026#34;, 149 \u0026#34;mem.granted.average\u0026#34;, 150 \u0026#34;mem.latency.average\u0026#34;, 151 \u0026#34;mem.swapin.average\u0026#34;, 152 \u0026#34;mem.swapinRate.average\u0026#34;, 153 \u0026#34;mem.swapout.average\u0026#34;, 154 \u0026#34;mem.swapoutRate.average\u0026#34;, 155 \u0026#34;mem.usage.average\u0026#34;, 156 \u0026#34;mem.vmmemctl.average\u0026#34;, 157 \u0026#34;net.bytesRx.average\u0026#34;, 158 \u0026#34;net.bytesTx.average\u0026#34;, 159 \u0026#34;net.droppedRx.summation\u0026#34;, 160 \u0026#34;net.droppedTx.summation\u0026#34;, 161 \u0026#34;net.usage.average\u0026#34;, 162 \u0026#34;power.power.average\u0026#34;, 163 \u0026#34;virtualDisk.numberReadAveraged.average\u0026#34;, 164 \u0026#34;virtualDisk.numberWriteAveraged.average\u0026#34;, 165 \u0026#34;virtualDisk.read.average\u0026#34;, 166 \u0026#34;virtualDisk.readOIO.latest\u0026#34;, 167 \u0026#34;virtualDisk.throughput.usage.average\u0026#34;, 168 \u0026#34;virtualDisk.totalReadLatency.average\u0026#34;, 169 \u0026#34;virtualDisk.totalWriteLatency.average\u0026#34;, 170 \u0026#34;virtualDisk.write.average\u0026#34;, 171 \u0026#34;virtualDisk.writeOIO.latest\u0026#34;, 172 \u0026#34;sys.uptime.latest\u0026#34;, 173 ] 174 # vm_metric_exclude = [] ## Nothing is excluded by default 175 # vm_instances = true ## true by default 176 177 ## Hosts 178 ## Typical host metrics (if omitted or empty, all metrics are collected) 179 # host_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to hosts to collect (by default all are collected) 180 # host_exclude [] # Inventory paths to exclude 181 host_metric_include = [ 182 \u0026#34;cpu.coreUtilization.average\u0026#34;, 183 \u0026#34;cpu.costop.summation\u0026#34;, 184 \u0026#34;cpu.demand.average\u0026#34;, 185 \u0026#34;cpu.idle.summation\u0026#34;, 186 \u0026#34;cpu.latency.average\u0026#34;, 187 \u0026#34;cpu.readiness.average\u0026#34;, 188 \u0026#34;cpu.ready.summation\u0026#34;, 189 \u0026#34;cpu.swapwait.summation\u0026#34;, 190 \u0026#34;cpu.usage.average\u0026#34;, 191 \u0026#34;cpu.usagemhz.average\u0026#34;, 192 \u0026#34;cpu.used.summation\u0026#34;, 193 \u0026#34;cpu.utilization.average\u0026#34;, 194 \u0026#34;cpu.wait.summation\u0026#34;, 195 \u0026#34;disk.deviceReadLatency.average\u0026#34;, 196 \u0026#34;disk.deviceWriteLatency.average\u0026#34;, 197 \u0026#34;disk.kernelReadLatency.average\u0026#34;, 198 \u0026#34;disk.kernelWriteLatency.average\u0026#34;, 199 \u0026#34;disk.numberReadAveraged.average\u0026#34;, 200 \u0026#34;disk.numberWriteAveraged.average\u0026#34;, 201 \u0026#34;disk.read.average\u0026#34;, 202 \u0026#34;disk.totalReadLatency.average\u0026#34;, 203 \u0026#34;disk.totalWriteLatency.average\u0026#34;, 204 \u0026#34;disk.write.average\u0026#34;, 205 \u0026#34;mem.active.average\u0026#34;, 206 \u0026#34;mem.latency.average\u0026#34;, 207 \u0026#34;mem.state.latest\u0026#34;, 208 \u0026#34;mem.swapin.average\u0026#34;, 209 \u0026#34;mem.swapinRate.average\u0026#34;, 210 \u0026#34;mem.swapout.average\u0026#34;, 211 \u0026#34;mem.swapoutRate.average\u0026#34;, 212 \u0026#34;mem.totalCapacity.average\u0026#34;, 213 \u0026#34;mem.usage.average\u0026#34;, 214 \u0026#34;mem.vmmemctl.average\u0026#34;, 215 \u0026#34;net.bytesRx.average\u0026#34;, 216 \u0026#34;net.bytesTx.average\u0026#34;, 217 \u0026#34;net.droppedRx.summation\u0026#34;, 218 \u0026#34;net.droppedTx.summation\u0026#34;, 219 \u0026#34;net.errorsRx.summation\u0026#34;, 220 \u0026#34;net.errorsTx.summation\u0026#34;, 221 \u0026#34;net.usage.average\u0026#34;, 222 \u0026#34;power.power.average\u0026#34;, 223 \u0026#34;storageAdapter.numberReadAveraged.average\u0026#34;, 224 \u0026#34;storageAdapter.numberWriteAveraged.average\u0026#34;, 225 \u0026#34;storageAdapter.read.average\u0026#34;, 226 \u0026#34;storageAdapter.write.average\u0026#34;, 227 \u0026#34;sys.uptime.latest\u0026#34;, 228 ] 229 ## Collect IP addresses? Valid values are \u0026#34;ipv4\u0026#34; and \u0026#34;ipv6\u0026#34; 230 # ip_addresses = [\u0026#34;ipv6\u0026#34;, \u0026#34;ipv4\u0026#34; ] 231 232 # host_metric_exclude = [] ## Nothing excluded by default 233 # host_instances = true ## true by default 234 235 236 ## Clusters 237 # cluster_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to clusters to collect (by default all are collected) 238 # cluster_exclude = [] # Inventory paths to exclude 239 # cluster_metric_include = [] ## if omitted or empty, all metrics are collected 240 # cluster_metric_exclude = [] ## Nothing excluded by default 241 # cluster_instances = false ## false by default 242 243 ## Datastores 244 # datastore_include = [ \u0026#34;/*/datastore/**\u0026#34;] # Inventory path to datastores to collect (by default all are collected) 245 # datastore_exclude = [] # Inventory paths to exclude 246 # datastore_metric_include = [] ## if omitted or empty, all metrics are collected 247 # datastore_metric_exclude = [] ## Nothing excluded by default 248 # datastore_instances = false ## false by default 249 250 ## Datacenters 251 # datacenter_include = [ \u0026#34;/*/host/**\u0026#34;] # Inventory path to clusters to collect (by default all are collected) 252 # datacenter_exclude = [] # Inventory paths to exclude 253 datacenter_metric_include = [] ## if omitted or empty, all metrics are collected 254 datacenter_metric_exclude = [ \u0026#34;*\u0026#34; ] ## Datacenters are not collected by default. 255 # datacenter_instances = false ## false by default 256 257 ## Plugin Settings 258 ## separator character to use for measurement and field names (default: \u0026#34;_\u0026#34;) 259 # separator = \u0026#34;_\u0026#34; 260 261 ## number of objects to retrieve per query for realtime resources (vms and hosts) 262 ## set to 64 for vCenter 5.5 and 6.0 (default: 256) 263 # max_query_objects = 256 264 265 ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores) 266 ## set to 64 for vCenter 5.5 and 6.0 (default: 256) 267 # max_query_metrics = 256 268 269 ## number of go routines to use for collection and discovery of objects and metrics 270 # collect_concurrency = 1 271 # discover_concurrency = 1 272 273 ## the interval before (re)discovering objects subject to metrics collection (default: 300s) 274 # object_discovery_interval = \u0026#34;300s\u0026#34; 275 276 ## timeout applies to any of the api request made to vcenter 277 # timeout = \u0026#34;60s\u0026#34; 278 279 ## When set to true, all samples are sent as integers. This makes the output 280 ## data types backwards compatible with Telegraf 1.9 or lower. Normally all 281 ## samples from vCenter, with the exception of percentages, are integer 282 ## values, but under some conditions, some averaging takes place internally in 283 ## the plugin. Setting this flag to \u0026#34;false\u0026#34; will send values as floats to 284 ## preserve the full precision when averaging takes place. 285 # use_int_samples = true 286 287 ## Custom attributes from vCenter can be very useful for queries in order to slice the 288 ## metrics along different dimension and for forming ad-hoc relationships. They are disabled 289 ## by default, since they can add a considerable amount of tags to the resulting metrics. To 290 ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include 291 ## to select the attributes you want to include. 292 ## By default, since they can add a considerable amount of tags to the resulting metrics. To 293 ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include 294 ## to select the attributes you want to include. 295 # custom_attribute_include = [] 296 # custom_attribute_exclude = [\u0026#34;*\u0026#34;] 297 298 ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In 299 ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported 300 ## with a significant delay (\u0026gt;30min). If this happens, try increasing this number. Please note that increasing 301 ## it too much may cause performance issues. 302 # metric_lookback = 3 303 304 ## Optional SSL Config 305 # ssl_ca = \u0026#34;/path/to/cafile\u0026#34; 306 # ssl_cert = \u0026#34;/path/to/certfile\u0026#34; 307 # ssl_key = \u0026#34;/path/to/keyfile\u0026#34; 308 ## Use SSL but skip chain \u0026amp; host verification 309 insecure_skip_verify = true 310 311 ## The Historical Interval value must match EXACTLY the interval in the daily 312 # \u0026#34;Interval Duration\u0026#34; found on the VCenter server under Configure \u0026gt; General \u0026gt; Statistics \u0026gt; Statistic intervals 313 # historical_interval = \u0026#34;5m\u0026#34; Container Setup Use the following docker compose file to start your containers. This solution uses ipvlan to assign each container an IPv4 Address. You don't have to assign the IPs to your OSes NIC.\nChange the passwords and secrets to your needs.\n1version: \u0026#39;3.6\u0026#39; 2 3networks: 4 ipvlan0: 5 driver: ipvlan 6 driver_opts: 7 parent: ens192 8 9 10services: 11 telegraf: 12 image: telegraf 13 container_name: telegraf 14 restart: always 15 volumes: 16 - ./telegraf.conf:/etc/telegraf/telegraf.conf:ro 17 depends_on: 18 - influxdb 19 links: 20 - influxdb 21 ports: 22 - \u0026#39;8125:8125\u0026#39; 23 networks: 24 ipvlan0: 25 ipv4_address: 172.29.31.2 26 27 influxdb: 28 image: influxdb:2.6-alpine 29 container_name: influxdb 30 restart: always 31 environment: 32 - INFLUXDB_DB=influx 33 - INFLUXDB_ADMIN_USER=admin 34 - INFLUXDB_ADMIN_PASSWORD=admin 35 networks: 36 ipvlan0: 37 ipv4_address: 172.29.31.3 38 39 ports: 40 - \u0026#39;8086:8086\u0026#39; 41 volumes: 42 - influxdb_data:/var/lib/influxdb 43 44 grafana: 45 image: grafana/grafana 46 container_name: grafana-server 47 restart: always 48 depends_on: 49 - influxdb 50 environment: 51 - GF_SECURITY_ADMIN_USER=admin 52 - GF_SECURITY_ADMIN_PASSWORD=admin 53 - GF_INSTALL_PLUGINS= 54 links: 55 - influxdb 56 networks: 57 ipvlan0: 58 ipv4_address: 172.29.31.4 59 ports: 60 - \u0026#39;3000:3000\u0026#39; 61 volumes: 62 - grafana_data:/var/lib/grafana 63 64volumes: 65 grafana_data: {} 66 influxdb_data: {} Start Containers\nIn the directory where the docker compose file is located:\n1docker compose up -d Check Containers\n1docker ps -a The containers should all have a running state.\nCreate TIG Config Head to your influxDB container ip with port 8086. In my case it is '172.29.31.3:8086' With help of the webinterface create your buckets according to the naming in your \u0026quot;telegraf.conf\u0026quot; file.\nImport Grafana Dashboards Access your grafana dashboard at your IP with port 3000 (in my case https://172.29.31.4:3000)\nThe grafana dashboards can be access via github: https://github.com/jorgedlcruz/vmware-grafana/tree/master (Cudos to Jorge de la Cruz)\nImport the grafana dashboards:\nClick Dashboards in the left-side menu. Click New and select Import in the dropdown menu. Paste dashboard JSON text directly into the text area https://jorgedelacruz.uk/\n","link":"/post/vsphere-monitoring/","section":"post","tags":["VMware","vSphere","Monitoring","TIG Stack"],"title":"vSphere Monitoring with Grafana"},{"body":"","link":"/tags/it-security/","section":"tags","tags":null,"title":"IT-Security"},{"body":"","link":"/tags/opinion/","section":"tags","tags":null,"title":"Opinion"},{"body":"Why current Enterprise IT-Security is s shipwreck.\nThe state of enterprise IT-Security is pretty bad and I don't think the current strategy to better it is any good. And here's the why:\nstatus quo\nBut firstly, what are the security concerns of businesses regarding IT-Security?\nIf we take the CIA triad for help:\nRansomware that disrups availability and integrity Data Leaks that violences confidentiality malware/virus that disrups integrity .. and more\ncurrent ways \u0026quot;to fix\u0026quot; the issue\nIf we look at current solutions that enterprise commonly use to \u0026quot;fix it\u0026quot;:\nsomekind of antivirus systems (on servers, clients, firewalls) IPD/IDS Systems (nothing other than an antivirus running on a network device) MFA an other Authentication Serivces, Conditional access etc - mostly based on Active Directory Why I think current ways to better IT-Security is b*llshit\nWe simply to not fix the root cause of the problems. We use somekind of software to secure unsecure software and think the \u0026quot;secure\u0026quot; sofware is initself secure. But there is a fundamental issue, every software has bugs. With each additional line of code written, there are potential bugs in that exact code. So more code just means more bugs. Bugs mean potential security bugs and weaknesses.\nYou cannot shift the responsibility to the user (aka do not open that email attachement) - A Backoffice persons jobs is to open emails with attachements, how should they make sure every email attachement is save? The fundamental problem is that an office application can open doors to ransomware and encrypt your hole (microsoft based) Infrastructure\nThe deadly combination of Winodws + Office + Active Directory\nCat an mouse game\n","link":"/post/state-of-enterprise-itsecurity/","section":"post","tags":["IT-Security","Opinion"],"title":"State of Enterprise IT-Security"},{"body":"Intro This post gives a few examples of Tanzu Kubernetes Cluster Manifests and how to deploy them.\nPrerequiries Successfull Installation of vSphere with Tanzu Supervisor vSphere Namespace created Already existing VM Classes, Storages Policies and Tanzu Kubernetes Releases assigned to the Namespace. Login via kubectl You will find the IP to your Kubernetes API on the Namespace Option \u0026quot;Link to CLI Tools\u0026quot;\n1kubectl vsphere login --server=10.40.80.20 1kubectl config get-contexts 1kubectl config use-context \u0026lt;\u0026gt; Get Parameters of your vSphere Namespace VM Class Storagge Policy Tanzu Releases 1kubectl get vmclass 1kubectl get storageclass 1kubectl get tanzukubernetesrelease Cluster with 3 Control and 6 Worker Nodes 1apiVersion: run.tanzu.vmware.com/v1alpha3 2kind: TanzuKubernetesCluster 3metadata: 4 name: tkgs-cluster 5 namespace: tkgs-cluster-ns 6spec: 7 topology: 8 controlPlane: 9 replicas: 3 10 vmClass: best-effort-large 11 storageClass: workload-management-storage-policy 12 tkr: 13 reference: 14 name: v1.23.8---vmware.2-tkg.2-zshippable 15 nodePools: 16 - replicas: 6 17 name: worker 18 vmClass: best-effort-large 19 storageClass: workload-management-storage-policy Save this file as a yaml file on your workstation.\nApply Cluster Manifest 1kubectl apply -f clusterspecs.yaml 1kubectl get tanzukubernetescluster Check the Status of the Cluster. \u0026quot;READY\u0026quot; should be true after some minutes.\nEdge Cluster 1apiVersion: run.tanzu.vmware.com/v1alpha3 2kind: TanzuKubernetesCluster 3metadata: 4 name: tkgs-cluster 5 namespace: tkgs-cluster-ns 6spec: 7 topology: 8 controlPlane: 9 replicas: 3 10 vmClass: best-effort-small 11 storageClass: workload-management-storage-policy 12 tkr: 13 reference: 14 name: v1.23.8---vmware.2-tkg.2-zshippable 15 nodePools: 16 - replicas: 3 17 name: worker 18 vmClass: best-effort-medium 19 storageClass: workload-management-storage-policy Minimal Cluser for PoC Use this cluster only for PoC/testing. Only three VMs are created in total. Later, you'll use Kubernetes Taints to enable the Control node to run user workloads. This is useful when testing workloads with a minimal VM footprint when deploying in environments with few resources available.\nConcept of Kubernetes Taints In Kubernetes, taints are a way to mark a node with a special attribute that affects which pods can be scheduled onto that node. Taints are used to repel or attract pods based on certain criteria, such as node characteristics or hardware specifications.\n1apiVersion: run.tanzu.vmware.com/v1alpha3 2kind: TanzuKubernetesCluster 3metadata: 4 name: tkgs-cluster 5 namespace: tkgs-cluster-ns 6spec: 7 topology: 8 controlPlane: 9 replicas: 3 10 vmClass: best-effort-medium 11 storageClass: workload-management-storage-policy 12 tkr: 13 reference: 14 name: v1.23.8---vmware.2-tkg.2-zshippable 15 Do not use this in a production environment After a successfull deployment of the cluster run the following commands to allow the Control Nodes to run User Workloads:\n1kubectl taint nodes --all node-role.kubernetes.io/master- ","link":"/post/tanzu-kubernetes-cluster-examples/","section":"post","tags":["VMware Tanzu","Tanzu Kubernetes Clusters","yaml"],"title":"Tanzu Kubernetes Cluster Example Deployments"},{"body":"","link":"/tags/tanzu-kubernetes-clusters/","section":"tags","tags":null,"title":"Tanzu Kubernetes Clusters"},{"body":"","link":"/tags/yaml/","section":"tags","tags":null,"title":"yaml"},{"body":"Quick Tip: vSphere with Tanzu an HA-Proxy This is a short article with tips and tricks I experienced when deploying vSphere with Tanzu and HA-Proxy as a Load Balancer.\nDeploying Ha-Proxy OVA Network\nAlwasys use three NICs when deploying in a production environment. It is very important that these IP addresses do not overlap with the address ranges of your workloads and load balancers, or with the gateway itself.\nAt first it can be somewhat confusing what networks are needed and which services will be deployed in each network.\nManagement: Your Management Network (must be only accessible for Admins, like vCenter) Workload: This is the workload IP address for the HA Proxy server. This subnet will be used to for virtual servers created by HA-Proxy. Frontend: This is the frontend IP address for the HA Proxy server. From this network clients will access resources. For a simple PoC you can safely use two NIC. Management is seperate and Workload/Frontend will be shared.\nCertificate\nJust leave it blank it will be generated for you.\nGetting the Certificate From your Workstation run the following command to get the Certificates from HA-Proxy needed for the Workload Management Feature Setup:\n1scp root@haproxyMGMTIP:/etc/haproxy/ca.crt ca.crt \u0026amp;\u0026amp; cat ca.crt ","link":"/post/tanzu-haproxy-tips/","section":"post","tags":["VMware","vSphere","VMware Tanzu"],"title":"Quicktip: Tanzu \u0026 HA-Proxy"},{"body":"Tanzu Portfolio explained Lately, we've been having lots conversations internally and with customers about Tanzu. There appears to be some confusion about Tanzu's portfolio and which Tanzu products are best suited for specific use cases.\nTanzu consists is basically packages open-source tools\nKubernetes Runtimes To actually run Kubernetes Clusters with Tanzu, the following Products are available:\nTanzu Kubernetes Grid vSphere with Tanzu Tanzu Kubernetes Grid (TKG) TKG is a solution for routing Kubernetes clusters to different cloud providers. First, a management cluster (based on Docker and kind) is created from a Bootstrap Workstation, which itself then provides its web interface and additional CLI tools to create a Kubernetes cluster.\nThe following providers are supported:\nvSphere AWS Azure The management cluster will provision the VMs on the target and install \u0026amp; configure kubernetes within those VMs.\nWhen vSphere is used as a Target there is no integration of TKG with vSphere UI. Just a bunch of VMs will be provisioned.\nIf you want to deploy your Kubernetes Cluster and Integration with vCenter and other vSphere Services. Look at vSphere with Tanzu. It has also been built with the vSphere Administrator in mind.\nIf you want to deploy your Kubernetes Cluster to Azure or Amazon Webservices, use Tanzu Kubernetes Grid.\nTKGm TKGm is the multi-cloud version of TKG. It can be deployed ontop of VMware Cloud on AWS (VMConAWS) and other VMware Hyperscaler Solutions.\nTKGI TKG Integrated edition has deep integration with NSX-T. It uses BOSH and Ops Manager to manage operations. It was an offering mainly for Telcos at Pivotal (Enterprise PKS). Although Integrated in the name, it's not as tighly integrated was vSphere with Tanzu.\nvSphere with Tanzu aka Workload Management In newer version of vCenter there is the option of \u0026quot;Workload Management\u0026quot; within the vSphere Burger Menu. Workload Management is the same as vSphere with Tanzu. With vSphere with Tanzu obviously only works with vSphere. Kubernetes Ressources are provisioned on a vSphere Cluster.\nThe first step is to create a Supervisor Cluster, which consists of three VMs in vSphere. Kubernetes itself is then provisioned via the TKG Service running on the Supervisor Cluster.\nIf the supervisor cluster then is created, additional vSphere Namespaces need to be provisioned (vSphere Namespace isn't the same as a Kubernetes Namespace!) Within that Namespace then a Tanzu Kubernetes Cluster can be created with the TKG Service. The Tanzu Kubernetes Cluster (TKC) are upstream aligned, fully conformant Kubernetes Cluster. It is also possible to provision VMs withing a vSphere Namespace. The Idea is, that a vSphere Administrator isolates Workloads through vSphere Namespaces that then can be used by DevOps/Platform Operators to provision their Kubernetes Clusters.\nUser workloads then are running inside a Tanzu Kubernetes Cluster.\nvSphere with Tanzu minimum Ressources\nvSphere Cluster with DRS 3 Supervisor VMs NSX-T or vSphere Distributed Switches vDS must use NSX-ALB or HA-Proxy as an external Load Balancer - NSX-ALB is recommended. Tanzu Mission Control Tanzu Mission Control is a Tool to operate your Tanzu Kubernetes Clusters. Besides Tanzu Kubernetes Cluster it can also managed Azure Kubernetes Service (AKS) and Elasic Kubernetes Service (EKS) from Amazon. TMC was a SaaS only Service but now also always a self-hosted version with some limitations .\nTMC self-hosted Requirements\nvSphere with Tanzu Supervisor Services: Contour Harbor Image Registry DevSecOps Tools Where in my opinion Tanzu really shines are the tools for day 2 operations and the whole DevSecOps lifecycle.\nTanzu Application Platform (TAP) Tanzu Application Catalog Tanzu Application Catalog came from the aquisition of Bitnami. TAC is the enterprise version of Bitnami Application Catalog. Bitnami was very well known for their prepackaded virtual machine Images (Drupal, Wordpress, Gitlab to name a few). Bitnami also provided prepackaked Applications on docker or kubernetes.\nLook at the VMware Marketplace for Solutions: https://marketplace.cloud.vmware.com/\n","link":"/post/tanzu-overview/","section":"post","tags":["VMware","VMware Tanzu","Kubernetes"],"title":"VMware Tanzu - Overview"},{"body":"","link":"/tags/cheat-sheet/","section":"tags","tags":null,"title":"Cheat Sheet"},{"body":"Kubernetes Cheat Sheet\nCheat Sheet for kubectl, kubeadm and general k8s related commands.\nkubectl Interacting with clusters kubectl command description Comment kubectl config view View Kubectl Configuration Display the current Kubectl configuration kubectl config get-contexts List available contexts See the available Kubernetes clusters and their associated contexts kubectl config current-context Display the current context Show the active Kubernetes cluster and context kubectl config use-context \u0026lt;context_name\u0026gt; Switch to another context Change the active context to the specified one kubectl config set-context \u0026lt;context_name\u0026gt; --namespace=\u0026lt;namespace_name\u0026gt; Set default namespace for a context Specify the default namespace for a specific context kubectl config unset current-context Unset the current context Remove the association with the current context kubectl config set-cluster \u0026lt;cluster_name\u0026gt; --server=\u0026lt;api_server_url\u0026gt; Add a new cluster to the configuration Define a new Kubernetes cluster with its API server URL kubectl config set-credentials \u0026lt;user_name\u0026gt; --token=\u0026lt;access_token\u0026gt; Add user credentials to the configuration Specify user credentials, typically an access token kubectl config set-context \u0026lt;context_name\u0026gt; --cluster=\u0026lt;cluster_name\u0026gt; --user=\u0026lt;user_name\u0026gt; Create a new context Combine a cluster and user to create a context kubectl config delete-context \u0026lt;context_name\u0026gt; Delete a context Remove a specific context from the configuration kubectl config delete-cluster \u0026lt;cluster_name\u0026gt; Delete a cluster Remove a cluster from the configuration kubectl config delete-user \u0026lt;user_name\u0026gt; Delete user credentials Remove user credentials from the configuration kubectl config use-context \u0026lt;context_name\u0026gt; Switch to another context Change the active context to the specified one kubectl config rename-context \u0026lt;old_name\u0026gt; \u0026lt;new_name\u0026gt; Rename a context Change the name of an existing context kubectl config set preferences.colors true Enable colorized output Enhance readability with colorized command output General Troubleshooting kubectl command description Comment kubectl get events Check events for resources View events for debugging and troubleshooting kubectl describe node \u0026lt;node_name\u0026gt; Run diagnostics on a specific node Replace \u0026lt;node_name\u0026gt; with the actual node name kubectl get componentstatuses Check the health of cluster components Verify the status of essential components in the cluster kubectl get pods --all-namespaces List all pods in all namespaces Useful for identifying pods in any namespace kubectl describe pod \u0026lt;pod_name\u0026gt; Get detailed information about a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl logs \u0026lt;pod_name\u0026gt; Retrieve the logs from a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl top nodes Display resource usage statistics for nodes View CPU and memory usage for all nodes kubectl top pods Display resource usage statistics for pods View CPU and memory usage for all pods Nodes kubectl command description Comment kubectl get nodes List all nodes in the cluster Display a list of nodes and their status kubectl describe node \u0026lt;node_name\u0026gt; Get detailed information about a node Replace \u0026lt;node_name\u0026gt; with the actual node name kubectl get nodes -o wide Display additional details about nodes View IP addresses and other information about nodes kubectl get nodes --show-labels Show labels assigned to nodes Display labels associated with each node kubectl get nodes -o json Display node information in JSON format View detailed node information in JSON kubectl cordon \u0026lt;node_name\u0026gt; Mark a node as unschedulable Prevent new pods from being scheduled on the specified node kubectl uncordon \u0026lt;node_name\u0026gt; Mark a node as schedulable Allow new pods to be scheduled on the specified node kubectl drain \u0026lt;node_name\u0026gt; Safely evict all pods from a node Evacuate a node for maintenance kubectl top nodes Display resource usage statistics for nodes View CPU and memory usage for all nodes kubectl get nodes --sort-by=.status.capacity.cpu Sort nodes by CPU capacity Sort nodes based on CPU capacity in ascending order kubectl label nodes \u0026lt;node_name\u0026gt; \u0026lt;label_key\u0026gt;=\u0026lt;label_value\u0026gt; Label a node Assign a label to a specific node kubectl taint nodes \u0026lt;node_name\u0026gt; \u0026lt;taint_key\u0026gt;=\u0026lt;taint_value\u0026gt;: Taint a node Apply a taint to a node for node affinity or tolerations kubectl describe nodes Get detailed information about all nodes View detailed information about all nodes in the cluster kubectl get componentstatuses Check the health of cluster components Verify the status of essential components in the cluster kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.phase Display custom columns for nodes Define custom output columns for nodes Pods kubectl command description Comment kubectl get pods List all pods in the current namespace Display a list of pods and their status kubectl get pods -n List pods in a specific namespace Replace \u0026lt;namespace\u0026gt; with the desired namespace kubectl describe pod \u0026lt;pod_name\u0026gt; Get detailed information about a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl logs \u0026lt;pod_name\u0026gt; Retrieve the logs from a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl exec -it \u0026lt;pod_name\u0026gt; -- Execute a command in a running pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name and \u0026lt;command\u0026gt; with the desired command kubectl port-forward \u0026lt;pod_name\u0026gt; \u0026lt;local_port\u0026gt;:\u0026lt;pod_port\u0026gt; Forward a pod's port to the local machine Replace \u0026lt;pod_name\u0026gt;, \u0026lt;local_port\u0026gt;, and \u0026lt;pod_port\u0026gt; with the actual values kubectl delete pod \u0026lt;pod_name\u0026gt; Delete a specific pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl delete pods --all Delete all pods in the current namespace Be cautious when using this command kubectl apply -f \u0026lt;pod_manifest.yaml\u0026gt; Create or update a pod using a manifest file Replace \u0026lt;pod_manifest.yaml\u0026gt; with the path to the pod's YAML manifest kubectl get pod \u0026lt;pod_name\u0026gt; -o yaml Get the YAML definition of a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl exec -it \u0026lt;pod_name\u0026gt; -- /bin/bash Open a shell in a running pod for debugging Replace \u0026lt;pod_name\u0026gt; with the actual pod name kubectl describe pod \u0026lt;pod_name\u0026gt; | grep Image Get the container image version running in a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name Storage kubectl command description Comment kubectl get storageclass List available storage classes Check the available storage classes in the cluster kubectl describe storageclass \u0026lt;storageclass_name\u0026gt; Get detailed information about a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name kubectl get persistentvolumes Display information about persistent volumes View details of persistent volumes in the cluster kubectl describe persistentvolume \u0026lt;pv_name\u0026gt; Get detailed information about a persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl get persistentvolumeclaims List persistent volume claims View information about persistent volume claims in the current namespace kubectl describe persistentvolumeclaim \u0026lt;pvc_name\u0026gt; Get detailed information about a persistent volume claim Replace \u0026lt;pvc_name\u0026gt; with the actual persistent volume claim name kubectl get pv,pvc List both persistent volumes and claims Display a combined list of persistent volumes and claims kubectl apply -f \u0026lt;storage_manifest.yaml\u0026gt; Create or update storage using a manifest file Replace \u0026lt;storage_manifest.yaml\u0026gt; with the path to the storage YAML manifest kubectl delete persistentvolume \u0026lt;pv_name\u0026gt; Delete a specific persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl delete persistentvolumeclaims --all Delete all persistent volume claims in the current namespace Be cautious when using this command kubectl get storageclass -o yaml Get the YAML definition of a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name kubectl get persistentvolume -o yaml Get the YAML definition of a persistent volume Replace \u0026lt;pv_name\u0026gt; with the actual persistent volume name kubectl get persistentvolumeclaim -o yaml Get the YAML definition of a persistent volume claim Replace \u0026lt;pvc_name\u0026gt; with the actual persistent volume claim name kubectl exec -it \u0026lt;pod_name\u0026gt; -- df -h Check storage usage inside a pod Replace \u0026lt;pod_name\u0026gt; with the actual pod name and df -h with the desired command kubectl describe storageclass \u0026lt;storageclass_name\u0026gt; | grep Provisioner Get the provisioner information for a storage class Replace \u0026lt;storageclass_name\u0026gt; with the actual storage class name Services kubectl command description Comment kubectl get services List all services in the current namespace Display a list of services and their types kubectl get services -o wide Display additional details about services View IP addresses and ports of services kubectl describe service \u0026lt;service_name\u0026gt; Get detailed information about a service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl get service \u0026lt;service_name\u0026gt; -o yaml Get the YAML definition of a service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl expose deployment \u0026lt;deployment_name\u0026gt; --port=\u0026lt;port_number\u0026gt; --target-port=\u0026lt;target_port\u0026gt; --name=\u0026lt;service_name\u0026gt; --type=\u0026lt;service_type\u0026gt; Expose a deployment as a service Replace \u0026lt;deployment_name\u0026gt;, \u0026lt;port_number\u0026gt;, \u0026lt;target_port\u0026gt;, \u0026lt;service_name\u0026gt;, and \u0026lt;service_type\u0026gt; with the actual values kubectl apply -f \u0026lt;service_manifest.yaml\u0026gt; Create or update a service using a manifest file Replace \u0026lt;service_manifest.yaml\u0026gt; with the path to the service YAML manifest kubectl delete service \u0026lt;service_name\u0026gt; Delete a specific service Replace \u0026lt;service_name\u0026gt; with the actual service name kubectl get services --sort-by=.spec.ports[0].nodePort Sort services by NodePort Sort services based on NodePort value in ascending order kubectl get services -o custom-columns=NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP,EXTERNAL-IP:.status.loadBalancer.ingress[0].ip,PORT(S).:.spec.ports[*].nodePort Display custom columns for services Define custom output columns for services kubectl get services -n List services in a specific namespace Replace \u0026lt;namespace\u0026gt; with the desired namespace kubectl get endpoints \u0026lt;service_name\u0026gt; Display endpoints for a service View the IP addresses and ports of pods backing a service kubectl describe ingress \u0026lt;ingress_name\u0026gt; Get detailed information about an ingress Replace \u0026lt;ingress_name\u0026gt; with the actual ingress name kubectl get svc \u0026lt;service_name\u0026gt; -o=jsonpath='{.spec.ports[0].nodePort}' Get the NodePort of a service using JSONPath Replace \u0026lt;service_name\u0026gt; with the actual service name Operations Run kubectl as non-root user non-root@cp: ÃÉ$ mkdir -p $HOME/.kube non-root@cp: ÃÉ$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config non-root@cp: ÃÉ$ sudo chown $(id -u):$(id -g) $HOME/.kube/config non-root@cp: ÃÉ$ less .kube/config\n","link":"/post/kubernetes/k8-cheat-sheet/","section":"post","tags":["Cheat Sheet","Kubernetes"],"title":"K8 Cheat Sheet"},{"body":"","link":"/tags/huawei/","section":"tags","tags":null,"title":"Huawei"},{"body":"Huawei Versatile Routing Platform (VRP) Overview\nThis post gives an overview about the network operating system for huawei network devices - VRP.\nGeneral\nTask Command Comment Enter System View \u0026lt; Huawei \u0026gt; sys Enter Interface View [R1] int GigabitEthernet0/0/1 Enter Protocol View [R1] ospf 1 Display Running Config in current View [R1- GigabitEthernet0/0/3] display this Display current configuration of the System [R1] dis current-config System Configuration\nTask Command Comment Set hostname [Huawei] sysname R1 Interface Configuration\nTask Command Comment Set IP Address of Interface [R1-GigabitEthernet0/0/1] ip address 172.29.40.13 24 Switching\nTask Command Comment VLAN taggin per Port --- --- First create a vlanif int vlanif ","link":"/post/networking/huawei-vrp/","section":"post","tags":["Networking","Huawei"],"title":"Huawei VRP"},{"body":"","link":"/tags/networking/","section":"tags","tags":null,"title":"Networking"},{"body":"A blog about networking, virtualization and general IT related stuff. Opinions are my own.\n\u0026gt;_ about me System Engineer based in central Switzerland\n","link":"/about/","section":"","tags":null,"title":"About this blog"},{"body":"","link":"/archives/","section":"","tags":null,"title":""},{"body":"","link":"/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"","link":"/series/","section":"series","tags":null,"title":"Series"}]